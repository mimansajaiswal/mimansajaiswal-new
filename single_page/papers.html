<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="">

<title>Research Publications</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-listing/list.min.js"></script>
<script src="../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-all-papers .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-categories','listing-title','listing-subtitle','listing-date','listing-description','listing-image','listing-outputHref','listing-display-date','listing-authors',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] }],
      
      searchColumns: ["listing-title","listing-date","listing-categories","listing-exthref","listing-description","listing-authors","listing-publisher","listing-display-date","listing-talk","listing-demo","listing-repo","listing-relatednote"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-all-papers'] = new List('listing-all-papers', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<style>html{ scroll-behavior: smooth; }</style>
<script>
    
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta property="og:title" content="Mimansa Jaiswal">
<meta property="og:description" content="I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.">
<meta property="og:image" content="https://mimansajaiswal.github.io/single_page/preview.png">
<meta property="og:site-name" content="Mimansa Jaiswal">
<meta name="twitter:title" content="Research Publications">
<meta name="twitter:description" content="I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.">
<meta name="twitter:image" content="https://mimansajaiswal.github.io/single_page/preview.png">
<meta name="twitter:creator" content="@MimansaJ">
<meta name="twitter:site" content="@MimansaJ">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../single_page/updates.html" rel="" target=""><i class="bi bi-newspaper" role="img">
</i> 
 <span class="menu-text">Updates</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../single_page/papers.html" rel="" target="" aria-current="page"><i class="bi bi-pen-fill" role="img">
</i> 
 <span class="menu-text">Research Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes/index.html" rel="" target=""><i class="bi bi-text-indent-left" role="img">
</i> 
 <span class="menu-text">Research Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../stream/index.html" rel="" target=""><i class="bi bi-cursor-text" role="img">
</i> 
 <span class="menu-text">Blurbs</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/MimansaJ" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text">Twitter</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text">Feed</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    <h5 class="quarto-listing-category-title">Categories</h5><div class="quarto-listing-category category-default"><div class="category" data-category="">All <span class="quarto-category-count">(14)</span></div><div class="category" data-category="Confounding Factors">Confounding Factors <span class="quarto-category-count">(2)</span></div><div class="category" data-category="Crowdsourcing">Crowdsourcing <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Data Annotation">Data Annotation <span class="quarto-category-count">(4)</span></div><div class="category" data-category="Data Augmentation">Data Augmentation <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Data Collection">Data Collection <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Debiasing">Debiasing <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Design">Design <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Dialogue">Dialogue <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Emotion Recognition">Emotion Recognition <span class="quarto-category-count">(11)</span></div><div class="category" data-category="Empirical Analysis">Empirical Analysis <span class="quarto-category-count">(5)</span></div><div class="category" data-category="Evaluation">Evaluation <span class="quarto-category-count">(3)</span></div><div class="category" data-category="Foundation Models">Foundation Models <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Generalization">Generalization <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Interpretation">Interpretation <span class="quarto-category-count">(4)</span></div><div class="category" data-category="Mental Health">Mental Health <span class="quarto-category-count">(5)</span></div><div class="category" data-category="Metric Design">Metric Design <span class="quarto-category-count">(4)</span></div><div class="category" data-category="Model Training">Model Training <span class="quarto-category-count">(5)</span></div><div class="category" data-category="Multimodal">Multimodal <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Schema">Schema <span class="quarto-category-count">(2)</span></div><div class="category" data-category="Speech and Audio">Speech and Audio <span class="quarto-category-count">(8)</span></div><div class="category" data-category="Text">Text <span class="quarto-category-count">(10)</span></div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7">Research Publications</h1>
<p class="author"></p>

</header>

<script>
for (const div of document.querySelectorAll('.category')) {
  div.classList.add('list-group-item', 'd-flex', 'justify-content-between', 'align-items-center');
}
for (const div of document.querySelectorAll('.quarto-listing-category.category-default')) {
  div.classList.add('list-group');
}

let counts = document.querySelectorAll('.quarto-category-count');
for (let i = 0; i < counts.length; i++) {
  let countText = counts[i].textContent;
  countText = countText.replace(/\(|\)/g, '');
  counts[i].textContent = countText;
  counts[i].classList.add('badge', 'bg-primary', 'rounded-pill');
}
</script>
<div class="mob-quick-nav">
<div class="callout callout-style-default callout-note callout-titled" title="Quick Navigation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Navigation
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="../index.html">Homepage</a>, <a href="mailto:mimansa@umich.edu">Email</a>, <a href="https://drive.google.com/file/d/114a9vX3dpjajZa9isMDg_d294-v-FcZ7/view">Resume</a>, <a href="../notes/index.html">Research Notes</a>, and, <a href="../single_page/papers.html">Publications</a></p>
</div>
</div>
</div>




<div class="quarto-listing quarto-listing-container-custom" id="listing-all-papers">
<br>
<div class="list quarto-listing-custom">
         <div class="papers-card quarto-post" data-index="0" data-categories="Emotion Recognition,Mental Health,Text,Speech and Audio,Metric Design" data-listing-date-sort="1693526400000">
            <div class="papers-main-metadata-display-date">
               Sep 2023
            </div>
            <div class="papers-main-metadata-publisher">
               Interspeech
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     Interspeech,
                        Sep 2023
                  </em>
               </p>
               <p class="papers-main-authors">
                  Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Mental Health'); return false;">
                           Mental Health
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Metric Design'); return false;">
                           Metric Design
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Emotion is expressed through language, vocal and facial expressions. Lack of emotional alignment between modalities is a symptom of mental disorders. We propose to quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional Mismatch (EMM). EMM patterns differ between symptomatic and euthymic moods. EMM statistics serve as an effective feature for mood recognition, reducing annotation cost while preserving mood identification.
                        </p>
                        <div class="papers-back-links">
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="1" data-categories="Text,Data Annotation,Design" data-listing-date-sort="1677628800000">
            <div class="papers-main-metadata-display-date">
               Mar 2023
            </div>
            <div class="papers-main-metadata-publisher">
               Submission
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     Submission,
                        Mar 2023
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Data Annotation'); return false;">
                           Data Annotation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Design'); return false;">
                           Design
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.
                        </p>
                        <div class="papers-back-links">
                           <a href="../notes/explanation_annotation_interfaces.html">Note</a>
                           <a href="https://human-in-loop-explanation-annotation.vercel.app/">Demo</a>
                           <a href="https://github.com/mimansajaiswal/hil-text-annotation">Repo</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="2" data-categories="Text,Evaluation,Metric Design,Schema,Interpretation,Data Annotation,Foundation Models" data-listing-date-sort="1677628800000">
            <div class="papers-main-metadata-display-date">
               Mar 2023
            </div>
            <div class="papers-main-metadata-publisher">
               Submission
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     Submission,
                        Mar 2023
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Evaluation'); return false;">
                           Evaluation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Metric Design'); return false;">
                           Metric Design
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Schema'); return false;">
                           Schema
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Interpretation'); return false;">
                           Interpretation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Data Annotation'); return false;">
                           Data Annotation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Foundation Models'); return false;">
                           Foundation Models
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Prompt-based language models introduce uncertainty to classification and require users to try multiple prompts with varying temperatures to find the best fit. However, this approach lacks the ability to capture implicit differences in prompts and provide adequate vocabulary. To address this, a text annotation framework is proposed to provide a structured approach to prompt definition and annotation. Better validation structures and structured prompts are necessary for using prompt-based systems at scale for labeling or retrieval.
                        </p>
                        <div class="papers-back-links">
                           <a href="../notes/capstone.html">Note</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="3" data-categories="Debiasing,Emotion Recognition,Text,Model Training,Empirical Analysis,Generalization,Evaluation,Metric Design,Interpretation" data-listing-date-sort="1664582400000">
            <div class="papers-main-metadata-display-date">
               Nov 2022
            </div>
            <div class="papers-main-metadata-publisher">
               arXiv
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Human-Centered Metric Design to Promote Generalizable and Debiased Emotion Recognition
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     arXiv,
                        Nov 2022
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Debiasing'); return false;">
                           Debiasing
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Model Training'); return false;">
                           Model Training
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Empirical Analysis'); return false;">
                           Empirical Analysis
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Generalization'); return false;">
                           Generalization
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Evaluation'); return false;">
                           Evaluation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Metric Design'); return false;">
                           Metric Design
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Interpretation'); return false;">
                           Interpretation
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Metrics for emotion recognition can be challenging due to their dependence on subjective human perception. This paper proposes a template formulation that derives human-centered, automatic, optimizable evaluation metrics for emotion recognition models. The template uses model explanations and sociolinguistic wordlists and can be applied to a sample or whole dataset. The proposed metrics include generalizability and debiasing improvement, and are tested on three models, datasets and sensitive variables. The metrics correlate with the models' performance and biased representations, and can be used to train models with increased generalizability, decreased bias, or both. The template is the first to provide quantifiable metrics for training and evaluating generalizability and bias in emotion recognition models.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/abs/2104.08792">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="4" data-categories="Emotion Recognition,Text,Speech and Audio,Model Training,Interpretation" data-listing-date-sort="1661990400000">
            <div class="papers-main-metadata-display-date">
               Sep 2022
            </div>
            <div class="papers-main-metadata-publisher">
               Interspeech
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Mind the Gap: On the Value of Silence Representations to Lexical-Based Speech Emotion Recognition
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     Interspeech,
                        Sep 2022
                  </em>
               </p>
               <p class="papers-main-authors">
                  Matthew Perez, Mimansa Jaiswal, Minxue Niu, Cristina Gorrostieta, Matthew Roddy, Kye Taylor, Reza Lotfian, John Kane, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Model Training'); return false;">
                           Model Training
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Interpretation'); return false;">
                           Interpretation
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Silence is crucial in speech perception, conveying emphasis and emotion. However, little research has been done on the effect of silence on linguistics and emotion recognition. We present a novel framework that fuses linguistic and silence representations for emotion recognition in naturalistic speech. Two methods to represent silence are investigated, with results showing improved performance. Modeling silence as a token in a transformer language model significantly improves performance on the MSP-Podcast dataset. Analyses show that silence emphasizes the attention of its surrounding words.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/perez22_interspeech.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="5" data-categories="Text,Evaluation,Metric Design,Schema,Interpretation,Data Annotation" data-listing-date-sort="1646092800000">
            <div class="papers-main-metadata-display-date">
               Mar 2022
            </div>
            <div class="papers-main-metadata-publisher">
               Submission
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     Submission,
                        Mar 2022
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Minxue Niu, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Evaluation'); return false;">
                           Evaluation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Metric Design'); return false;">
                           Metric Design
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Schema'); return false;">
                           Schema
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Interpretation'); return false;">
                           Interpretation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Data Annotation'); return false;">
                           Data Annotation
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Factors affecting explanation efficacy include the algorithm used and the end user. NLP papers focus on algorithms for generating explanations, but overlook other factors. This paper examines how saliency-based explanation methods for machine learning models change with controlled variables. We aim to provide a standardized list of variables to evaluate these explanations and show how SoTA algorithms can have different rankings when controlling for evaluation criteria.
                        </p>
                        <div class="papers-back-links">
                           <a href="../notes/controlled_evaluation_of_explanations.html">Note</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="6" data-categories="Data Augmentation,Emotion Recognition,Speech and Audio,Empirical Analysis" data-listing-date-sort="1593561600000">
            <div class="papers-main-metadata-display-date">
               2020
            </div>
            <div class="papers-main-metadata-publisher">
               ACL-SRW
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Noise-Based Augmentation Techniques for Emotion Datasets: What Do We Recommend?
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     ACL-SRW,
                        2020
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Data Augmentation'); return false;">
                           Data Augmentation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Empirical Analysis'); return false;">
                           Empirical Analysis
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Multiple noise-based data augmentation approaches have been proposed to counteract this challenge in other speech domains. But, unlike speech recognition and speaker verification, the underlying label of emotion data may change given the addition of noise. In this work, we propose a set of recommendations for noise-based augmentation of emotion datasets based on human and machine performance evaluation of generated realistic noisy samples using multiple categories of environmental and synthetic noise.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/abs/2104.08806">PDF</a>
                           <a href="https://slideslive.com/38928670/noisebased-augmentation-techniques-for-emotion-datasets-what-do-we-recommend">Talk</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="7" data-categories="Data Collection,Confounding Factors,Emotion Recognition,Speech and Audio" data-listing-date-sort="1588291200000">
            <div class="papers-main-metadata-display-date">
               May 2020
            </div>
            <div class="papers-main-metadata-publisher">
               LREC
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  MuSE: Multimodal Stressed Emotion Dataset
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     LREC,
                        May 2020
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Cristian-Paul Bara, Yuanhang Luo, Rada Mihalcea, Mihai Burzo, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Data Collection'); return false;">
                           Data Collection
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Confounding Factors'); return false;">
                           Confounding Factors
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.
                        </p>
                        <div class="papers-back-links">
                           <a href="http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.187.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="8" data-categories="Confounding Factors,Emotion Recognition,Speech and Audio,Text,Model Training" data-listing-date-sort="1580515200000">
            <div class="papers-main-metadata-display-date">
               Feb 2020
            </div>
            <div class="papers-main-metadata-publisher">
               AAAI and NeuRIPS-W
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Privacy Enhanced Multimodal Neural Representations for Emotion Recognition
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     AAAI and NeuRIPS-W,
                        Feb 2020
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Confounding Factors'); return false;">
                           Confounding Factors
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Model Training'); return false;">
                           Model Training
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/pdf/1910.13212.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="9" data-categories="Emotion Recognition,Text,Model Training,Speech and Audio,Empirical Analysis,Mental Health,Dialogue" data-listing-date-sort="1567296000000">
            <div class="papers-main-metadata-display-date">
               Sep 2019
            </div>
            <div class="papers-main-metadata-publisher">
               Interspeech
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  Identifying Mood Episodes Using Dialogue Features from Clinical Interviews
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     Interspeech,
                        Sep 2019
                  </em>
               </p>
               <p class="papers-main-authors">
                  Zakaria Aldeneh, Mimansa Jaiswal, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Model Training'); return false;">
                           Model Training
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Empirical Analysis'); return false;">
                           Empirical Analysis
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Mental Health'); return false;">
                           Mental Health
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Dialogue'); return false;">
                           Dialogue
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Mental health professionals assess symptom severity through semi-structured clinical interviews. During these interviews, they observe their patients’ spoken behaviors, including both what the patients say and how they say it. In this work, we move beyond acoustic and lexical information, investigating how higher-level interactive patterns also change during mood episodes.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/pdf/1910.05115.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="10" data-categories="Emotion Recognition,Data Annotation,Empirical Analysis,Crowdsourcing" data-listing-date-sort="1556668800000">
            <div class="papers-main-metadata-display-date">
               May 2019
            </div>
            <div class="papers-main-metadata-publisher">
               ICASSP
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  MuSE-ing on the Impact of Utterance Ordering on Crowdsourced Emotion Annotations
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     ICASSP,
                        May 2019
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Zakaria Aldeneh, Cristian-Paul Bara, Yuanhang Luo, Mihai Burzo, Rada Mihalcea, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Data Annotation'); return false;">
                           Data Annotation
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Empirical Analysis'); return false;">
                           Empirical Analysis
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Crowdsourcing'); return false;">
                           Crowdsourcing
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Emotion expression and perception are inherently subjective. There is generally not a single annotation that can be unambiguously declared “correct.” As a result, annotations are colored by the manner in which they were collected, i.e., with or without context.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/pdf/1903.11672.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="11" data-categories="Emotion Recognition,Model Training,Speech and Audio,Empirical Analysis,Mental Health" data-listing-date-sort="1535760000000">
            <div class="papers-main-metadata-display-date">
               Sep 2018
            </div>
            <div class="papers-main-metadata-publisher">
               Interspeech
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     Interspeech,
                        Sep 2018
                  </em>
               </p>
               <p class="papers-main-authors">
                  Soheil Khorram, Mimansa Jaiswal, John Gideon, Melvin McInnis, Emily Mower Provost
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Model Training'); return false;">
                           Model Training
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Empirical Analysis'); return false;">
                           Empirical Analysis
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Mental Health'); return false;">
                           Mental Health
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           This paper presents critical steps in developing this pipeline, including (a) a new in the wild emotion dataset, the PRIORI Emotion Dataset, (b) activation/valence emotion recognition baselines, and, (c) establish emotion as a meta-feature for mood state monitoring.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/pdf/1806.10658.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="12" data-categories="Emotion Recognition,Mental Health,Text" data-listing-date-sort="1512086400000">
            <div class="papers-main-metadata-display-date">
               Dec 2017
            </div>
            <div class="papers-main-metadata-publisher">
               FLAIRS
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  'Hang in there': Lexical and Visual Analysis to Identify Posts Warranting Empathetic Responses
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     FLAIRS,
                        Dec 2017
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Sairam Tabibu, Erik Cambria
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Mental Health'); return false;">
                           Mental Health
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           Saying "You deserved it!" to "I failed the test" is not a good idea. In this paper, we propose a method supported by hand-crafted features to judge if the discourse or statement requires an empathetic response.
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/pdf/1903.05210.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
         <div class="papers-card quarto-post" data-index="13" data-categories="Emotion Recognition,Mental Health,Multimodal,Text,Speech and Audio" data-listing-date-sort="1467331200000">
            <div class="papers-main-metadata-display-date">
               Jul 2017
            </div>
            <div class="papers-main-metadata-publisher">
               ICDM-W
            </div>
            <div class="papers-main-content">
               <p class="papers-main-title">
                  'The Truth and Nothing But The Truth': Multimodal Analysis for Deception Detection
               </p>
               <p class="papers-main-display-date-publisher-in-text"><em>In
                     ICDM-W,
                        Jul 2017
                  </em>
               </p>
               <p class="papers-main-authors">
                  Mimansa Jaiswal, Sairam Tabibu, Rajiv Bajpai
               </p>
                  <div class="listing-categories">
                           <div class="listing-category" onclick="window.quartoListingCategory('Emotion Recognition'); return false;">
                           Emotion Recognition
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Mental Health'); return false;">
                           Mental Health
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Multimodal'); return false;">
                           Multimodal
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Text'); return false;">
                           Text
                        </div>
                           <div class="listing-category" onclick="window.quartoListingCategory('Speech and Audio'); return false;">
                           Speech and Audio
                        </div>
                  </div>
                     <div class="papers-hover_content">
                        <p class="papers-back-description">
                           We propose a data-driven method (SVMs) for automatic deception detection in real-life trial data using visual (OpenFace) and verbal cues (Bag of Words).
                        </p>
                        <div class="papers-back-links">
                           <a href="https://arxiv.org/pdf/1903.04484.pdf">PDF</a>
                        </div>
                     </div>
            </div>
         </div>
</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>