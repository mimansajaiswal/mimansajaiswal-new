[
  {
    "objectID": "single_page/papers.html",
    "href": "single_page/papers.html",
    "title": "Research Publications",
    "section": "",
    "text": "Quick Navigation\n\n\n\nHomepage, Email, Resume, Research Notes, and, Publications\n\n\n\n\n\n\n\n\n\n\n         \n            \n               Sep 2023\n            \n            \n               Interspeech\n            \n            \n               \n                  Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder\n               \n               In\n                     Interspeech,\n                        Sep 2023\n                  \n               \n               \n                  Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Metric Design\n                        \n                  \n                     \n                        \n                           Emotion is expressed through language, vocal and facial expressions. Lack of emotional alignment between modalities is a symptom of mental disorders. We propose to quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional Mismatch (EMM). EMM patterns differ between symptomatic and euthymic moods. EMM statistics serve as an effective feature for mood recognition, reducing annotation cost while preserving mood identification.\n                        \n                        \n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Design\n                        \n                  \n                     \n                        \n                           Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                        \n                        \n                           Note\n                           Demo\n                           Repo\n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Foundation Models\n                        \n                  \n                     \n                        \n                           Prompt-based language models introduce uncertainty to classification and require users to try multiple prompts with varying temperatures to find the best fit. However, this approach lacks the ability to capture implicit differences in prompts and provide adequate vocabulary. To address this, a text annotation framework is proposed to provide a structured approach to prompt definition and annotation. Better validation structures and structured prompts are necessary for using prompt-based systems at scale for labeling or retrieval.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               Nov 2022\n            \n            \n               arXiv\n            \n            \n               \n                  Human-Centered Metric Design to Promote Generalizable and Debiased Emotion Recognition\n               \n               In\n                     arXiv,\n                        Nov 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Debiasing\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Generalization\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Metrics for emotion recognition can be challenging due to their dependence on subjective human perception. This paper proposes a template formulation that derives human-centered, automatic, optimizable evaluation metrics for emotion recognition models. The template uses model explanations and sociolinguistic wordlists and can be applied to a sample or whole dataset. The proposed metrics include generalizability and debiasing improvement, and are tested on three models, datasets and sensitive variables. The metrics correlate with the models' performance and biased representations, and can be used to train models with increased generalizability, decreased bias, or both. The template is the first to provide quantifiable metrics for training and evaluating generalizability and bias in emotion recognition models.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2022\n            \n            \n               Interspeech\n            \n            \n               \n                  Mind the Gap: On the Value of Silence Representations to Lexical-Based Speech Emotion Recognition\n               \n               In\n                     Interspeech,\n                        Sep 2022\n                  \n               \n               \n                  Matthew Perez, Mimansa Jaiswal, Minxue Niu, Cristina Gorrostieta, Matthew Roddy, Kye Taylor, Reza Lotfian, John Kane, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Model Training\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Silence is crucial in speech perception, conveying emphasis and emotion. However, little research has been done on the effect of silence on linguistics and emotion recognition. We present a novel framework that fuses linguistic and silence representations for emotion recognition in naturalistic speech. Two methods to represent silence are investigated, with results showing improved performance. Modeling silence as a token in a transformer language model significantly improves performance on the MSP-Podcast dataset. Analyses show that silence emphasizes the attention of its surrounding words.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Mar 2022\n            \n            \n               Submission\n            \n            \n               \n                  Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?\n               \n               In\n                     Submission,\n                        Mar 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Minxue Niu\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                  \n                     \n                        \n                           Factors affecting explanation efficacy include the algorithm used and the end user. NLP papers focus on algorithms for generating explanations, but overlook other factors. This paper examines how saliency-based explanation methods for machine learning models change with controlled variables. We aim to provide a standardized list of variables to evaluate these explanations and show how SoTA algorithms can have different rankings when controlling for evaluation criteria.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               2020\n            \n            \n               ACL-SRW\n            \n            \n               \n                  Noise-Based Augmentation Techniques for Emotion Datasets: What Do We Recommend?\n               \n               In\n                     ACL-SRW,\n                        2020\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Data Augmentation\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                  \n                     \n                        \n                           Multiple noise-based data augmentation approaches have been proposed to counteract this challenge in other speech domains. But, unlike speech recognition and speaker verification, the underlying label of emotion data may change given the addition of noise. In this work, we propose a set of recommendations for noise-based augmentation of emotion datasets based on human and machine performance evaluation of generated realistic noisy samples using multiple categories of environmental and synthetic noise.\n                        \n                        \n                           PDF\n                           Talk\n                        \n                     \n            \n         \n         \n            \n               May 2020\n            \n            \n               LREC\n            \n            \n               \n                  MuSE: Multimodal Stressed Emotion Dataset\n               \n               In\n                     LREC,\n                        May 2020\n                  \n               \n               \n                  Mimansa Jaiswal, Cristian-Paul Bara, Yuanhang Luo, Rada Mihalcea, Mihai Burzo, Emily Mower Provost\n               \n                  \n                           \n                           Data Collection\n                        \n                           \n                           Confounding Factors\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                  \n                     \n                        \n                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Feb 2020\n            \n            \n               AAAI and NeuRIPS-W\n            \n            \n               \n                  Privacy Enhanced Multimodal Neural Representations for Emotion Recognition\n               \n               In\n                     AAAI and NeuRIPS-W,\n                        Feb 2020\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Confounding Factors\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                  \n                     \n                        \n                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2019\n            \n            \n               Interspeech\n            \n            \n               \n                  Identifying Mood Episodes Using Dialogue Features from Clinical Interviews\n               \n               In\n                     Interspeech,\n                        Sep 2019\n                  \n               \n               \n                  Zakaria Aldeneh, Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Dialogue\n                        \n                  \n                     \n                        \n                           Mental health professionals assess symptom severity through semi-structured clinical interviews. During these interviews, they observe their patients’ spoken behaviors, including both what the patients say and how they say it. In this work, we move beyond acoustic and lexical information, investigating how higher-level interactive patterns also change during mood episodes.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               May 2019\n            \n            \n               ICASSP\n            \n            \n               \n                  MuSE-ing on the Impact of Utterance Ordering on Crowdsourced Emotion Annotations\n               \n               In\n                     ICASSP,\n                        May 2019\n                  \n               \n               \n                  Mimansa Jaiswal, Zakaria Aldeneh, Cristian-Paul Bara, Yuanhang Luo, Mihai Burzo, Rada Mihalcea, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Crowdsourcing\n                        \n                  \n                     \n                        \n                           Emotion expression and perception are inherently subjective. There is generally not a single annotation that can be unambiguously declared “correct.” As a result, annotations are colored by the manner in which they were collected, i.e., with or without context.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2018\n            \n            \n               Interspeech\n            \n            \n               \n                  The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild\n               \n               In\n                     Interspeech,\n                        Sep 2018\n                  \n               \n               \n                  Soheil Khorram, Mimansa Jaiswal, John Gideon, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Model Training\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Mental Health\n                        \n                  \n                     \n                        \n                           This paper presents critical steps in developing this pipeline, including (a) a new in the wild emotion dataset, the PRIORI Emotion Dataset, (b) activation/valence emotion recognition baselines, and, (c) establish emotion as a meta-feature for mood state monitoring.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Dec 2017\n            \n            \n               FLAIRS\n            \n            \n               \n                  'Hang in there': Lexical and Visual Analysis to Identify Posts Warranting Empathetic Responses\n               \n               In\n                     FLAIRS,\n                        Dec 2017\n                  \n               \n               \n                  Mimansa Jaiswal, Sairam Tabibu, Erik Cambria\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                  \n                     \n                        \n                           Saying \"You deserved it!\" to \"I failed the test\" is not a good idea. In this paper, we propose a method supported by hand-crafted features to judge if the discourse or statement requires an empathetic response.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Jul 2017\n            \n            \n               ICDM-W\n            \n            \n               \n                  'The Truth and Nothing But The Truth': Multimodal Analysis for Deception Detection\n               \n               In\n                     ICDM-W,\n                        Jul 2017\n                  \n               \n               \n                  Mimansa Jaiswal, Sairam Tabibu, Rajiv Bajpai\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Multimodal\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                  \n                     \n                        \n                           We propose a data-driven method (SVMs) for automatic deception detection in real-life trial data using visual (OpenFace) and verbal cues (Bag of Words).\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "single_page/blurbs.html",
    "href": "single_page/blurbs.html",
    "title": "Blurbs",
    "section": "",
    "text": "Quick Navigation\n\n\n\nHomepage, Email, Resume, Research Notes, and, Publications\n\n\n\nBlurbs is where I keep my random musings, quick reviews, and noteworthy quotes all in one place. It’s convenient to have a curated collection instead of them being scattered throughout my feed. It’s a fun little side space to gather everything. \n\n\n\n\n\n\n\n  \n    \n        05-07-2023\n      \n      \n      \n        Livable Stipends for PhD Students Shouldn't Be Up for Debate\n      \n      \n        A response to the controversial idea that PhD students should barely scrape by in emergencies and rely on Blue Cupboard to feed themselves.\n      \n      \n        I recently received some criticism for my comments over the past couple of days, which seemed to justify the idea that PhD students don't need livable stipends. Let me make myself perfectly clear: that's bullshit. No one should have to choose between paying bills and getting essential medical care or feeding their pets. If you're one of those people who thinks barely scraping by is okay for PhD students, we don't see eye to eye.\n        \n        \n      \n      \n      \n        Finance\n        PhD\n        Academia\n        Opinion\n        Twitter\n        \n    \n\n  \n    \n        05-07-2022\n      \n      \n      \n        Addressing 20-22 personal challenges and reassuming professional responsibility\n      \n      \n        An apology and promise - overcoming personal challenges to resume profound professional commitment\n      \n      \n        From Q2 2020 to Q2 2022, unforeseen personal circumstances severely impacted my ability to complete projects and maintain consistent communication. These events causing significant disruptions included the tragic loss of my maternal grandfather to cancer (2020), followed by the sudden demise of my paternal grandmother due to COVID-19 (2021), and then the demands of managing major health crises involving my father and remaining grandmother (2022). However, having now defended my PhD and made significant strides in task delivery, I've found my footing again. As a former intern at Meta and AI2, the personal turmoil I experienced inhibited me from concluding the projects I was passionate about, leaving me deeply saddened. Over time, and as my stability returned, I made private progress on these projects, but feelings of embarrassment and regret kept me from re-establishing contact. I acknowledge the past disappointments during this challenging period and sincerely apologize. Today, with confidence and affirmation, I assure that such setbacks will no longer impede my professional dedication and, with renewed commitment, I take responsibility for any lingering tasks.\n        \n        \n      \n      \n      \n        PhD\n        Academia\n        Internships\n        Mental Health\n        \n    \n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "personal_notes/mac_softwares.html",
    "href": "personal_notes/mac_softwares.html",
    "title": "What softwares do I actually use on my Mac as a software enthusiast?",
    "section": "",
    "text": "On a daily basis, I use multiple apps and I prefer to put them together here. In general, I am always looking for better note-taking apps, and if you have any suggestions, I would love to know (trust me, I have tried 800 of them!). I will divide this into different categories of software I use, in case they are helpful for anyone.\n\nFor some context, I use a M1 Pro, Macbook Pro, with 16GB RAM and 512GB SSD. Some of these applications might not be great on 8GB RAM or 128GB SSD, so proceed with discretion.\n\nProductivity Software\n\nCommunication: Spark Email 2 is my email client of choice rather than using the general Apple mail client. Why? Because it has the ability to remove Updates/Promotions/Forums from my main inbox that Apple mail does not. It also does basic Newsletters and Pins options for each email account, that Apple mail does not. To have all the DMs in one place, I use Texts. If you do not have multiple accounts per app, you can also use Beeper.\nTasks: I use Todoist. I have used 100 other apps, but I keep coming back to Todoist. With their quick capture being all bang based, nothing is easier for me to quick capture, schedule, rescheduled, completed. It syncs with Google Calendar. It works in collaboration mode. This is the only task tracker that has worked for me.\nTerminal: Warp as terminal. I do not work with vim, so take my advice with a grain of salt. But I really like block based terminals. It also has a bang based AI mode. You can type # to ask it to provide the command that converts all images in this folder and sub-folders to .webp and delete the original versions and it will provide you with the command with templated tab-able arguments.\nSticky Notes: Numi is the app I use for pasting on always on top notes. It is also great for calculations inside notes. It is extremely expensive, so probably get it on sale rather than anytime else, would be my recommendation.\nCalendar and Timers: Dato is for calendar and meetings. It shows the meeting time and current time across timezones, shows a monthly calendar, has an option to join a meeting from the menu bar. The reason I use it over itsycal is for its full screen notifications because I otherwise forget the meeting time or join a minute or two late. If you do not need that or timezones, itsycal is free and does everything else. I also use Day Progress because I am time-blind, but most people would not need it. I use Timer RH for always on top timers for timed tasks. It connects with the calendar as well, is pretty easy to set rather than using a Siri command (just drag the menu bar icon down to however long you need the timer to be).\nNote taking: I use Aiko at the moment for transcription of my blabberings, Tactiq for remote meetings, and Otter for in person meetings. For long form note taking, I use Notion at the moment, but it is not necessarily my tool of choice, given its poor AI integration that would allow me to go from unstructured to structured content. It also lacks a calendar integration and spatial system, so I am always on a lookout for something different. Some other softwares I am exploring are Tana (better AI and API integration), Capacities (no collaboration), Heptabase (no collaboration) and Google Workspace (no block based editing but has newly added chips, task management, linking previews etc). I use Goodnotes or Apple Notes for handwritten note taking.\nReference management: I used to use Paperpile but I shifted to using Notion for this purpose because I could not make use of the highlighted information. I use either ar5iv or Paper To HTML to snip papers into notion.\nWhiteboarding: I use Guga (new app, chinese support) or Muse (expensive but works really well, has a teams version). I tried Freeform and did not like it much. Guga also does not have desktop support which kinda hampers its usage on my end.\nReading: I use Reader from Readwise, they allow you to annotate pdfs, videos webpages, add highlights, add notes to highlights etc. It is cheap for students; syncs highlights from everywhere to Notion and works reliably. The two other options are Omnivore (open source and free) and Matter (better and more intuitive design but does not work with pdfs or videos). For podcasts, because apparently the information now also comes from podcasts, I use Snipd, which has headphone button based sniping, AI snip boundaries and snip summaries, and also syncs to Readwise or separately to Notion.\n\n\n\nCreative Work\nI divide creative work into 2 parts, sometimes overlapping, creative work as an hobby, creative work for my research.\n\nFlowcharts: Whimsical and flowchart.fun. Whimsical is great for making beautiful handmade flowcharts or making flowcharts easily using AI. Flowchart.fun provides a simple language to create flowcharts by just writing text.\nScreenshots: Shottr allows to create annotated screenshots, blur parts of image on device. It also allows for rolling screenshots where if you capture an area it will auto scroll the webpage or any other document for you. Shottr is free and is extremely cheap for pro version (which is only needed for not getting pinged for updating after 30 days). I use pika for pretty backgrounds around screenshots.\nScreen recordings: Explaining a coding process or a step-by-step guide on about how to do something, we often end up with full screen videos with tiny text. I use screen.studio or screenrun for automatic zoom ins into where I am clicking or typing. Screenrun can also do a short video from a screenshot if you want to say, oh, click here and then here and then here.\nPaper diagrams: draw.io for making neural network diagrams. There is unfortunately nothing that comes close. I cannot do TikZ, but if you can, more power to you!\nDrawing: I use Procreate or Concepts. Procreate allows for really intuitive drawing interface whereas concepts is vector based, and still allows for all the flair with layers, brushes, objects etc.\n\n\n\nWorks in Background\n\nMemory: Rewind.ai has been a life-saver for me, it records everything on screen, and records laptop audio and mic if you choose to. It has two main things: search rewind and ask rewind. Search rewind is a simple keyword search over everything that has been stored. Ask rewind sends everything in those clips to OpenAI (possibly GPT3.5 16k) to create a summary and adds “screenshot” references.\nShortcuts: Raycast allows for doing so many things that I do not know how to describe it. It is local first and is free. It also has a pro subscription if you need its AI chat options (which comes with a student discount). The best way I have seen it described is Spotlight++ but I never used spotlight. You can use ⌘+Space to do calculations, timezone conversions, unit conversions, quick word lookup, file search, screenshot search etc.\n\n\n\n\n\n\nAmphetamine and Other Menu Bar Apps\n\n\n\nYou can also remove the amphetamine software you have and use the menu bar coffee app that Raycast provides. It comes with many extensions, such as, hypersonic that can connect to any notion database of yours to show tasks, schedule them etc, all from ⌘+Space or from your menu bar.\nAnother menu bar app that I use from Raycast is Text Shortcuts which allows you to select any task, and convert case, URL encode and decode, convert to markdown, find text statistics (word counter, character counter) etc.\n\n\nCamera: This is a small software, but I have cats, and I have messy hair. I use Camera Preview a ton both for checking on myself and using the return key to click photos. Hand mirror is the alternate option, but it cannot click photos.\nUtilities: I use Bartender for arranging my menu bar, especially with that notch on Macbook pro. I use Better Touch Tool to assign taps to clicks (for example three finger tap to open in a new tab). I also use it to assign hyperkey. Hyperkey is when you press all meta keys together on the keyboard. For example, Hyperkey+A is my shortcut for quick task in Todoist. I use WeatherX to have weather of 4 cities be readily accessible in the menu bar. I use Command+X to have a cut option in MacOS.\nAI Chat Interfaces: I use Bolt AI at the moment to organize my conversations, especially those that need to go through API for non-training purposes. There are 100 such apps, this is the one I found the earliest. If I do end up paying for Raycast Pro, then I would not need this app. I use Bolt because, it does not show up in dock, allows to select models per chat, allows to set shortcut to bring it up. I sometimes use Sider (on browser) and MacGPT (quick question - synonym for X) too.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "stream/the_two_year_tragedy.html",
    "href": "stream/the_two_year_tragedy.html",
    "title": "Addressing 2020-2022 personal challenges and reassuming professional responsibility",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/explanation_annotation_interfaces.html",
    "href": "notes/explanation_annotation_interfaces.html",
    "title": "Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html",
    "href": "notes/no_sally_anne_pass_for_gpt4.html",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "",
    "text": "The research efforts in this post are geared towards two key objectives: the first being to extract an instinctual understanding from the given prompt, and the second is to develop examples of evaluation problems that embody this innate comprehension that lies within the prompt.\nToday, I bring into focus a commonly known cognitive assessment tool, the Sally-Anne False Belief Test.\n\n\nAs part of this exploration, I’ve roughly outlined a framework that represents the process I am intending to explore through testing, designing, and adaptation for the purpose of evaluation. This framework is three-pronged, including:\n\nThe Derivation of Instinct, which involves extracting the inherent understanding from the given prompt.\nThe Modification of Prompt, where I’ll aim to alter the initial prompt thus creating varying scenarios and parameters for assessment.\nThe Evaluation of Generation, where model outputs based on the modified prompts are scrutinized and analyzed.\n\n\nI’d very much appreciate any references to papers that extensively discuss the theoretical aspects of designing evaluation frameworks. This area of study not only piques my intellectual curiosity but is also close to my heart.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrancois talks about why scoring AI using tests designed for humans might not be a good idea"
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#framework",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#framework",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "",
    "text": "As part of this exploration, I’ve roughly outlined a framework that represents the process I am intending to explore through testing, designing, and adaptation for the purpose of evaluation. This framework is three-pronged, including:\n\nThe Derivation of Instinct, which involves extracting the inherent understanding from the given prompt.\nThe Modification of Prompt, where I’ll aim to alter the initial prompt thus creating varying scenarios and parameters for assessment.\nThe Evaluation of Generation, where model outputs based on the modified prompts are scrutinized and analyzed.\n\n\nI’d very much appreciate any references to papers that extensively discuss the theoretical aspects of designing evaluation frameworks. This area of study not only piques my intellectual curiosity but is also close to my heart.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrancois talks about why scoring AI using tests designed for humans might not be a good idea"
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#framework-based-breakdown",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#framework-based-breakdown",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Framework based breakdown",
    "text": "Framework based breakdown\nUpon studying the prompt, three fundamental concepts emerge:\n\nThe first concept pertains to ‘Timing’. Herein, the sequence and scheduling of the events presented in the prompt ascertain an important aspect of the situation being evaluated.\nThe second concept involves the ‘Unshared Movement’ of the object. This implies a shift or relocation of an entity in the scenario that is not made aware to all the characters or elements involved.\nThe third, and equally critical concept, is the implied ‘Lack of Knowledge Transfer’. This embedded hypothesis posits that any alteration in the situational variables is not communicated or disclosed to every participant within the scenario.\n\nDeconstructing the prompt in this manner helps us derive a comprehensive understanding of the situation presented and aids in structuring a meticulous evaluation."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#output",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#output",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Output",
    "text": "Output\nIt’s noteworthy to mention that while both the GPT3.5 and GPT4 models have yielded correct responses in the current context, the GPT3.5 model was, until recently, not entirely accurate.\nSpecifically, the output I received from the GPT3.5 model a few days back demonstrated some fallacies. In the concerned response, the machine learning model implied that the character should check a particular folder to confirm if the file was indeed moved there. Such an inference showcases an error in comprehending the nuanced context of the scenario, since nowhere in the given situation it was explicitly or implicitly suggested that the item’s location was relocated to a specific directory. The correct interpretation should not have pointed towards any designated location but instead should have indicated an unknown place."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#renaming-the-folder",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#renaming-the-folder",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Renaming the folder",
    "text": "Renaming the folder\nUpon reviewing the given prompt, I’m contemplating certain modifications to better evaluate the models:\n\nFirstly, adding a layer of complexity via the “Unshared Movement/Transformation” of an object. Here, we are introducing the act of renaming an entity and subsequently undoing this renaming. The aim is to test how the models react and adapt to these changes that are not shared mutually among the elements in the scenario.\nSecondly, I still uphold the implied notion of “No Knowledge Transfer”. This rule suggests that any changes happening in the state of affairs is not communicated to all the elements in the scenario, thus maintaining a lack of shared knowledge.\nLastly, as a new objective, I wish to inculcate the concept of “Information Movement”. In this scenario, the particular act of data syncing is incorporated. The idea here is to evaluate how the models handle the situation where the information moves from one place to another, simulating real-life digital synchronization.\n\nThese alterations to the prompt are aimed at presenting a more intricate scenario, thereby testing the capabilities of the AI models at handling a higher level of complexity and context understanding."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#turning-the-folder-private",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#turning-the-folder-private",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Turning the folder private",
    "text": "Turning the folder private\nContinuing with our framework, I would like to propose additional modifications to our prompt based on Objective 4:\n\nIncorporating an aspect of “Information Movement Manipulation”, specifically an obstruction in the process. We are thus introducing a hurdle or difficulty in the movement of information, creating a more challenging situation to assess the adaptability and problem-solving capabilities of the AI models.\n\nNow, an intriguing scenario arises where the GPT4 model seems to fail, but the GPT3.5 model unexpectedly succeeds. This occurrence suggests that sometimes, models with lesser parameters or seemingly less sophistication might perform better in certain specific situations. It also emphasizes the point that AI performance doesn’t exponentially increase purely based on the size or the complexity of the model. Or that probably GPT3.5 was better RLHF’ed based off many storage companies’ access policies."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#moving-all-files",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#moving-all-files",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Moving all files",
    "text": "Moving all files\nTurning our attention back to the previously discussed adjustments, it is pertinent to raise a hypothetical scenario: What would the consequences be if I had overlooked another category of “object transformation” which did indeed originate complications?\nIndeed, such an omission took place.\nThe “unshared movement/transformation” of an object, such as the migration of all folders/files, presents an interesting case study. It is observed that the GPT-3.5 model, to an extent, manages to reason correctly, however, it ultimately does not succeed. In contrast, the GPT-4 model accurately navigates this test."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#learnings-and-other-modifications",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#learnings-and-other-modifications",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Learnings and Other Modifications",
    "text": "Learnings and Other Modifications\nIt’s rather simple to draw inferences from these instances and presume that GPT-3.5 comprehends the notion of translocating shared directories and documents. However, if that were completely accurate, it would not stumble when faced with the aforementioned prompt. Logically, its understanding of the concept should enable it to adequately handle this task without failure. This contradiction illuminates potential gaps in the GPT-3.5 model’s understanding and handling of object transformation, particularly in complex operations such as moving shared folders and files.\n\n\n\n\n\n\nAndrew talks about his daughter mentioning that ChatGPT answers with what would a response to this sound like?"
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#output-for-basic-lending-based-prompt",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#output-for-basic-lending-based-prompt",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Output for basic lending based prompt",
    "text": "Output for basic lending based prompt\nInterestingly, in this scenario, both the GPT4 and GPT3.5 models provide correct responses. Hence, we can observe that when the concept of “no knowledge transfer” is less explicit and more integral to the workings of a situation or action, the models are able to deduce it accurately."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#co-reading-instead-of-lending",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#co-reading-instead-of-lending",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Co-reading instead of lending",
    "text": "Co-reading instead of lending\nThe scenario of co-reading presents an interesting case of “AFFIRMATIVE knowledge transfer of actions from one participant to another”. Here, ‘co-reading’ is the fundamental action, and ‘taking actions with my friend’ only strengthens the notion.\nThe action of co-reading, particularly with a friend, practically implies that knowledge transfer is not just possible but is the norm. The act inherently involves sharing and discussing insights, viewpoints, and understanding, which constitutes the ‘knowledge transfer’. Furthermore, the phrase ‘taking actions with my friend’ reinforces this idea, as actions taken together suggest shared knowledge and understanding.\nIn this implicit manifestation of affirmative knowledge transfer, both GPT4 and GPT3.5 models successfully provide correct responses."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#so-we-are-good-",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#so-we-are-good-",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "So, we are good?",
    "text": "So, we are good?\nIt’s tempting to draw the conclusion that the models possess an innate understanding of the implicit possibility of knowledge transfer based on their ability to interpret the situations proposed so far correctly.\nHowever, we must exercise caution in dashing to such conclusions!\nIndeed, you might recall a tweet (though finding the original is proving elusive) that exhibited that GPT-3.5 didn’t originally compute this correctly. It was only after users repeatedly inputted it into the interface (thus educating it) that it came around to delivering the right deductions, as evidenced by its compatibility with ‘co-reading’."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#not-really-reading-on-phone-call",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#not-really-reading-on-phone-call",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Not really, reading on phone call",
    "text": "Not really, reading on phone call\nNow, steering our focus back to another example:\n‘Phone Call Read Aloud’- An instance where there is an “implicit case of ‘NO VISUAL knowledge transfer of actions from one participant to another’”. This scenario brings into focus a phone call as the basic premise (which is typically audio-only) and the actions conducted during the call that underscore the concept.\nIntriguingly, BOTH the GPT4 and GPT3.5 models fail to fare well in this scenario. It indicates that when nuances of sensory data come into play (in this case, distinguishing between audio and visual transfer of knowledge), even the sophisticated models like GPT-4 and GPT-3.5 can stumble. Indeed, these models’ understanding of implicit knowledge transfer concepts appears to be dependent on the specifics of the situation and not a universal, foolproof capability."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#mathematical-reasoning-with-lending",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#mathematical-reasoning-with-lending",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Mathematical reasoning with lending",
    "text": "Mathematical reasoning with lending\nIn the context of evaluating the performance of language models, particularly GPT4 and GPT3.5, in addressing explicit reasoning tasks involving “math” and “lending,” two scenarios are considered.\nIn the first scenario, which involves two steps of reasoning, both GPT4 and GPT3.5 fail to meet the expected standard. The evaluation reveals that these models are unable to effectively reason through the provided explicit prompts involving mathematical calculations and lending. The shortcomings of both models are evident in their inability to produce accurate and reliable responses to these complex cognitive tasks that require multi-step reasoning processes.\nHowever, in the second scenario, where there is a single step of reasoning involved, GPT4 demonstrates an improvement in its performance, surpassing the previous models. As compared to GPT3.5, GPT4 successfully accomplishes the task by effectively reasoning through the provided prompt and providing accurate results."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#is-math-the-problem-or-is-it-sleeping-",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#is-math-the-problem-or-is-it-sleeping-",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Is math the problem or is it sleeping?",
    "text": "Is math the problem or is it sleeping?\nIn the experiment I mentioned, I wanted to test whether excluding math from the lending prompt would make the “sleep” implicit, resulting in no knowledge transfer. And to my surprise, it actually worked! This indicates that the exclusion of math didn’t hinder the transfer of knowledge, suggesting that it may not be the problem after all.\nInstead, it’s possible that the combination of “static object placement” and “constrained movement” is what really makes the difference. It seems that this particular combination enables successful knowledge transfer."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#fixating-on-static-object-placements",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#fixating-on-static-object-placements",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Fixating on static object placements",
    "text": "Fixating on static object placements\nA new addition to the prompt is the consideration of “static object placements,” which refers to the positioning or arrangement of stationary objects in relation to the moving object, specifically in the context of moving a key. Significantly, both GPT3.5 and GPT4 are able to accurately respond to the prompt, indicating that the inclusion of static object placements is crucial for their successful performance. It seems that these placements enhance the models’ understanding and their ability to generate appropriate responses."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#darn-movement-by-cats-does-not-work-",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#darn-movement-by-cats-does-not-work-",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Darn, movement by cats does not work!",
    "text": "Darn, movement by cats does not work!\nMAYBE. IT IS THE BOYFRIEND THAT WORKS? CATS DON’T? (/s)\nIn this iteration, we further modify the problem by shifting the focus to going on a vacation, while keeping the same boyfriend and eliminating the involvement of cats. Surprisingly, this modification yields successful results, indicating that the issue has been narrowed down significantly. A closer examination reveals that the key to achieving accurate performance lies in the concept of “explicit” movement to an “explicit” location, executed by a “human actor.” It seems that language models, such as the ones mentioned earlier (GPT4 and GPT3.5), are able to grasp and generate appropriate responses when the prompt involves clear and precise movement executed by a human to a specific location.?\n\n\n\n\n\n\n\n\n\n\nUpon careful consideration, it appears that the wording of the prompt might have a significant influence on the performance of GPT4 and GPT3.5."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#the-cat-places-the-keys",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#the-cat-places-the-keys",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "The cat places the keys",
    "text": "The cat places the keys\nif the cat avoids dropping the keys to the floor and instead intentionally “places” them on the floor, the results might be different. Excitingly, GPT4 demonstrates success in generating appropriate responses in this particular scenario, whereas GPT3.5 falls short. This observation raises the question of whether the cat’s behavior is the cause of the discrepancy. It is worth noting that GPT3.5 has undergone substantial reinforcement learning by humans, likely with a focus on handling situations involving humans and their actions.\nIt is an interesting insights into the intricacies of language understanding by LLMs and the ways in which language models like GPT3.5 and GPT4 can be influenced by context and RLHF."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#ambiguity-issues",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#ambiguity-issues",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Ambiguity Issues",
    "text": "Ambiguity Issues\nTo explore this further, I again modify the scenario by replacing the boyfriend with a cat, returning to the previous configuration. Regrettably, when the prompt involves the “cat” as an “actor” changing “locations,” both GPT4 and GPT3.5 fail to produce the correct answer. This observation suggests that the way the prompt is framed, particularly when it involves a cat as the main entity performing actions and transitioning between different locations, poses a challenge for these language models.\n\n\n\n\n\n\n\n\n\n\nOr in the case of bookmarks, some of these prompts have the word “expect” which is known to be ambiguous."
  },
  {
    "objectID": "notes/no_sally_anne_pass_for_gpt4.html#word-based-leakage-issues",
    "href": "notes/no_sally_anne_pass_for_gpt4.html#word-based-leakage-issues",
    "title": "No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test",
    "section": "Word based leakage issues",
    "text": "Word based leakage issues\nA crucial distinction is made between the performance of GPT4 and GPT3.5 in response to prompts involving the words “think” or “believe,” particularly in the context of sleep and coffee. Interestingly, it is noted that while GPT4 fares well in generating coherent outputs in such scenarios, GPT3.5 struggles to do so. The observation raises an important consideration.\nWhile one might initially perceive GPT4’s success as a positive outcome, a deeper analysis reveals a potential drawback. The inclusion of the word “think” inherently suggests the possibility of non-continuity in observations, indicating that the generated responses may not consistently align with a continuous narrative.\n\n\n\n\n\nA further observation can be made to support the previous findings. When the wording is modified from “think” to “look for,” it becomes evident that GPT4 encounters difficulties once again."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#evaluation-datasets",
    "href": "notes/case-of-llm-evals.html#evaluation-datasets",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Evaluation Datasets",
    "text": "Evaluation Datasets\nEvaluation Datasets or Evaluation Sets or Eval Sets: These are the test samples that the model is being evaluated for. There are multiple ways to construct these evaluation datasets and to use them; each of them coming with its own set of problems.\nUsing the similar set of datasets for evaluation come with a another set of problems:\n\nFuzziness in prompts: Now that there are prompts involved in the process; we really need to consider the fuzziness that comes from the prompt itself. While the evaluation datasets were used without any “instruction language” or “prompted addition”, the test samples at least remained consistent. The prompts here may not. I talk more about prompt introduced variability in my other post about prompt based benchmarking. It specifically talks about three components of a prompt for standardizing prompt based evaluation: prompt steer, sample steer, output steer. You can read it here.\n\nUntraceability: Going back to the point of data leakage; while it used to always be a problem; now that no one has any idea about any of the data that went into the model, even the best-faith and triple checked evaluation has no guarantees of being a out of distribution evaluation.\n\n\n\n\n\nPrompt parts: Start, Process and Output Steer and influences that arise from it\n\n\n\nWhat can these evaluation datasets look like?\n\nSuggest examples in pull requests and I will merge them here\n\nPre-curated evaluation sets from various standardized tests — These are mostly designed to be differentiating for humans and not for models. Additionally, they can have memorization based questions, that can incorrectly be inferred as understanding in context of evaluating LLMs.\n\n\n\n\n\n\nPre-curated eval sets\n\n\n\n\n\n\n\n\nWhat Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset fromMedical Exams\n\n\n\n\n\nWeb scraped evaluation sets for ML model testing — These datasets are created by scraping the internet for particular tags and using the corresponding tag as the label for the sample itself or are annotated by various human annotators. The samples from these datasets are most likely to be present in the training set of these foundation models itself and hence, it is often not a good idea to just rely on these for evaluation.\n\n\n\n\n\n\nScraped eval sets\n\n\n\n\n\n\n\n\nTriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\n\n\n\n\n\nHuman created and curated evaluation sets for the sole purpose of evaluating a task — These test sets are usually designed as a measure against data leakage. After all, humans can probably create a distinct enough pattern to evaluate against. While that is a pro, these datasets come with flaws of their own; for example they are really small and, are harder to create and update.\n\n\n\n\n\n\nHuman curated eval sets\n\n\n\n\n\n\n\n\nHumanEval dataset proposed in Evaluating Large Language Models Trained on Code\n\n\n\n\n\nFuzzy Versions — These are variants of exisiting datasets or evaluation sets, extensions or additions created with the explicit purpose of introducing and testing for model behavior in presence of such variability. This variability can be intentionally adversarial, can be aimed to introduce out of distribution tokens for robustness testing or just for the purpose of creating paraphrased equivalent samples.\n\n\n\n\n\n\nFuzzed eval sets\n\n\n\n\n\n\n\n\nFor example, a set of adversarial prompts and inputs that act as addition/in replacement to the original evaluation samples, as proposed in PromptBench\n\n\n\n\n\nAd-hoc testing samples by humans — These are performed as conversation based evaluation of these models. While they are most likely to be accurate; they are are biased by the mere idea that the human usually needs to know the solution to the problem to be able the question for evaluation purposes. This often ends up in; what I call as; human imagination collapse or the likelihood that an individual human would often be set on a trajectory to test for and not meaningfully diversify; at least in a single setting.\n\n\n\n\n\n\nAd-hoc testing\n\n\n\n\n\n\n\n\nEvaluating models through single turn or multi-turn conversations in OpenAssistant Conversations - Democratizing Large Language Model Alignment\n\n\n\n\n\n\n\n\n\n\nFrom this tweet by Subbarao Kambhampati (కంభంపాటి సుబ్బారావు)"
  },
  {
    "objectID": "notes/case-of-llm-evals.html#model-output",
    "href": "notes/case-of-llm-evals.html#model-output",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Model Output",
    "text": "Model Output\nNow let us come to the second problem — output from generative models.\n\nOne of the major problems with almost every single solution that we, as a community have proposed: is this idea of evaluating generative models using discriminative outputs.\nModel output is heavily dependent on the (a) prompt asked for the output; and (b) the answers asked for. For example, if you ask a model for a label 0 or 1; vs asking a model for labels in words (for example: spam or not spam); you might end up with different outcomes. Another example: asking a model for a direct output and extracting the answer can lead to a different answer than in the multiple-choice scenario.\nRegression based model outcome does not necessarily scale and hence can change the standard deviation and mean of the regressed output. For example, if you ask a model to provide a graded rating between 0-10; you cannot with certainty scale that grading to 0-1, especially if you care about significance testing.\n\n\n\n\n\n\n\nDo you want to know why this is a problem with examples?\n\n\n\n\n\nLet’s take the example in Textbooks are all you need paper again and ask it to grade 2 responses on 3 scales.\nWhy? Because the description says that the GPT4 model was asked to grade between 0-10, but the numbers are reported scaled between 0-1.\n\n\n\nThe paper saying that the model is asked for grades between 0 to 10, and that is then scaled to report between 0-1.\n\n\n\n\n\n\n\nLet’s start with the completely correct response.\n\n\n\nThe GPT4 model says the response is 9/10 when asked for a grade between 0-10.\n\n\n\n\n\nThe GPT4 model says the response is .95/10 when asked for a grade between 0-1.\n\n\n\n\n\nThe GPT4 model says the response is 4.5/5 when asked for a grade between 0-5; which is great, right? Because at least the scaling is consistent.\n\n\nBut, let’s consider some other solutions, those that are incorrect in some way or another. Let’s do this by changing squaring to 2.2, and sorting the second list in reverse.\n\n\n\nThe GPT4 model says the response is 8/10 when asked for a grade between 0-10.\n\n\n\n\n\nThe GPT4 model says the response is 4.5/5 when asked for a grade between 0-5. The scaling is constant here too!\n\n\n\n\n\nThe GPT4 model says that the response is 0.9/1. And now all the scaling is messed up."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#sample-output-transformation",
    "href": "notes/case-of-llm-evals.html#sample-output-transformation",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Sample/Output Transformation",
    "text": "Sample/Output Transformation\nThere are many transformations applied to model outputs or their inputs. These can roughly be divided into 4 categories:\n\nLooped Transformations\nLoop based transformations usually follow the philosophy of —&gt; what if we additionally add the model output + some form of evaluation of the current answer (either from the same model, another model or a human) back into the model; such that it eventually reaches the perfect outcome. Some examples of these would be Self-Critique models.\n\n\n\n\n\n\nExample Papers\n\n\n\n\n\n\n\n\nReflexion: Language Agents with Verbal Reinforcement Learning develops a modular formulation for Reflexion, utilizing three distinct models: an Actor generates text and actions; an Evaluator model scores the outputs produced by Actor; and a Self-Reflection model, generates verbal reinforcement cues to assist the Actor in self-improvement.\n\n\n\n\n\n\n\nChained Transformations\nChain based transformations usually do not have a measured evaluation in between a set of model input → output → model input and so on. These chains are usually pre-defined and have a restricted number of paths to follow.\n\n\nAtomic Outputs\nThis method involves breaking down the output of a model; either manually, through a rule based system or through AI itself; into atomic components that can be evaluated individually to combine to form a larger weighted grade.\n\n\nConstrained Output\nThis method involves either using log probability (which isn’t accessible in case of GPT3.5/GPT4 APIs) or other internal constraints to ensure that the model responds with tokens belonging to a certain pre-decided or allowed distribution."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#ground-truth",
    "href": "notes/case-of-llm-evals.html#ground-truth",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Ground Truth",
    "text": "Ground Truth\nThis does not need much explanation but there are certain aspects to remember here; especially when you think of ground truths in respect to present scene of evaluation. Firstly, ground truth can be biased, ambiguous, or have a high range of disagreement. In case of human centered tasks, such as likability of a prose, the disagreement is often averaged out rather than considered as an annotation curve, you need to compare the model’s outcome multiple times to achieve a true distribution comparison.\nWith new evaluation practices, you need to remember that you may or MAY NOT have ground truth in a certain evaluation.\nRemember the 3 possible pitfalls with ground truth:\n\nInclusion in the loop or chain based transformations\nInclusion in in-context or few shot learning examples in case of prompt steering\nGround truth might be used to establish correlation between the newly proposed completely automated metric; and not actually used in evaluation."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#evaluation-medium",
    "href": "notes/case-of-llm-evals.html#evaluation-medium",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Evaluation Medium",
    "text": "Evaluation Medium\nIn my perspective, the evaluation medium can be categorized into three distinct groups.\n\nDirect Evaluation Metrics\n\n\n\n\n\n\nExample Papers\n\n\n\n\n\n\n\n\nTextbooks are all you needevaluation with HumanEval and MBPP\n\n\n\n\n\nThe first category I call \"direct evaluation metrics.\" These are the traditional metrics that have been widely used in the field for a long time. Metrics like accuracy and F1 score fall into this category. Typically, this approach involves obtaining a single output from the model and comparing it to a reference, either through constraints or by extracting the desired information. The evaluation can be done through ad-hoc conversation-based evaluation, curated specialized test sets, or direct annotation.\nFor instance, one direct evaluation metric is comparing the model's accuracy directly to the ground truth. When evaluating multiple-choice question answers, the comparison can be based on matching the choice letter, the complete choice, or the distribution over choices. To gain a deeper understanding of how these evaluation approaches can impact the results, check out this article: What's going on with the Open LLM Leaderboard and Llama?\n\n\n\n\n\nNow how do we evaluate the model from these prompts?\n\n\n\n\nIndirect or Decomposed Model-Based Evaluation\n\n\n\n\n\n\nExample Papers\n\n\n\n\n\n\n\n\nRubric based from the same model.\nTinyStories: How Small Can Language Models Be and Still Speak Coherent English?\n\n\n\n\n\nSelf-critiquing models for assisting human evaluators\n\n\n\n\n\nG-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment using form-filling methods for evaluation and then calculate a correlation to human preference.\n\n\n\n\n\nComponent-wise model driven evaluation scores in LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models\n\n\n\n\n\nMoving on to the second category, we have \"indirect or decomposed heuristics.\" Here, we utilize smaller models, whether they are fine-tuned or raw decompositions, to evaluate the answers generated by the main model. The idea is to leverage these smaller models, which excel at specific tasks that large language models are generally good at, such as adjective identification or polarity identification. The outputs of these smaller models act as weak scores that are combined to provide a final label or evaluation for the generated output. This indirect evaluation approach allows for a more nuanced assessment of the model's performance, particularly in areas like the likability of prose. Although these models introduce some variability, it's important to note that they are often trained for regression tasks and fine-tuned for specific purposes.\n\nHonestly, the line between this method of evaluation and the next is blurry, especially in terms of impact and possibility of being wrong. Suggestions for better ontology are welcome!\n\n\n\nModel-Driven Evaluation\n\n\n\n\n\n\nPapers\n\n\n\n\n\n\n\n\nEvaluating responses by comparing to referenced ground truth in Sparks of AGI. Remember this is inclusion of ground truth and possibly the least problematic form of model driven evaluation.\n\n\n\n\n\nBring Your Own Data! Self-Supervised Evaluation forLarge Language Models evaluation based on invariance of model output based on fuzzed input samples\n\n\n\n\n\nTextbooks are all you needevaluation with GPT4\n\n\n\n\n\nUsing LLMs for explanations of smaller LLMs\n\n\n\n\n\nAsk the AI section from Language Models (Mostly) Know What They Know\n\n\n\n\n\nThe third category I refer to as \"model-driven evaluation.\" In these cases, the model itself provides the final score or evaluation. However, this introduces an additional layer of variability. Even if the model has access to ground truth information, the evaluation metric itself may introduce randomness or variability in the scoring process. For example, a common evaluation question could be: \"Is this generated output (O) similar to the ground truth answer (G)?\" The answer to this question not only depends on the randomness associated with the model's output but also on the inherent variability of the evaluation metric.\nIt's important to remember that recent evaluation practices may involve the inclusion or exclusion of ground truth in model-driven evaluation.\nThis can lead to 2 kinds of model-driven evaluations:\n\n[Inclusion of Ground Truth] Asking the model to compare to the ground truth and produce an output in affirmation or contradiction. This can also be seen as providing two statements to the model, and asking it to label them for entailment, paraphrasing or both.\n[Exclusion of Ground Truth] Asking the model to directly “judge” the output. In this case, the larger model is often provided outputs from the smaller models and asked to evaluate the answer’s correctness. This can range from a short feedback to a likert scale answer or anywhere in between. &gt; Note, that the larger model evaluating the smaller model paradigm might not necessarily hold for all papers and is significantly more dubious in claims than the former.\n\nThe general explanation provided for such situations is — but oh, this is how humans do this stuff too. So, we are asking GPT-4 to be more humane and avoiding binary labels that we needed initially.\nFor example, this is why the authors of Textbooks are all you need believe this is a correct method of evaluation.\n\n\n\n\n\n\nRonen talks about why using automated evals is a good idea for textbooks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetailed account of why I believe that using GPT-4 as a judge is incorrect in this case\n\n\n\n\n\nBut this, in my opinion, is not a correct way to account for non-binary labels. Especially in coding, when there is an existing method to run the function through tests and have assertions against expected answers. My judgement might have been different had this evaluation method been proposed for a subjective task, but coding, is both, an objective and assertable task, and this evaluation method discards all these properties of a coding task in general.\nLet’s break this down:\nWhat are the problems that general evaluations in coding have? The biggest problem is (a) writing representative tests and their expected output, and, (b) making sure that these tests are comprehensive and follow a curriculum based evaluation.\nThat is a bunch of jargon! What does this even mean?\nFor example, let’s take the example in their paper:\n\nWrite a python code for the following:\ndef sort_concat_square_deduplicate (list1, list2, my_threshold):\n\"\"\"\nThis functions takes two lists of integers, sorts each of them in ascending order, concatenates them, squares the entries at even indices, filters out entries smaller than my threshold and then removes duplicates. The resulting list is returned.\n\"\"\"\nOne of the major drawbacks of existing evaluation datasets is that they test the final answer. While the authors claim that it is a better idea to judge if the model implements components correctly.\nGoing back to decomposed testing, the correct way here would be to ask the model to provide atomic functions and then evaluate those atomic functions using tests and the final function, which calls the atomic functions in desired order.\nThis solves the problem of not having binary pass/fail labels. This of course requires more effort, but is substantially more accurate.\nDoesn’t GPT4 as a judge solve these problems anyway? No, no it does not.\nWhy not?\n\nThe method does not provide the exact same number every single time. For example, let’s consider the same doc string as mentioned above. And let’s evaluate how good Mosaic 30B-Chat is good at programming. \nNow, let us ask GPT4 to judge this. Not only does the answer change, the model suggests to do something that was not even asked for.\n\n\n\nThe model scores it 9.5/10\n\n\n\n\n\nThe model scores it 9/10\n\n\nImplicit assumption that the model actually scores completely incorrect answer at 0. This assumption is not tested by the authors. Let’s test it. The decomposition of this problem is into 6 atoms:\n\nSorting of 2 lists ⟶ Sort second list in reverse order\nConcatenation ⟶ Change to duplicate concatenation\nEven Indices ⟶ Change to every index\nSquare ⟶ Change to power 2.2\nThreshold Filter ⟶ Change it to filter to less than or equal to\nRemove Duplicates ⟶ Duplicate the filtered list\n\nGPT4 judges it to be 4/10. And, it does not see that the threshold value was changed.\n\n\n\nThe model scores the completely incorrect function at 4/10. It ignores an error. In the previous pictures, it creates an error or a suggestion which was not asked for and will render the code incorrect.\n\n\nAnything weird to the model, that is not in the general expected distribution will be marked low. For example, here is a correct code that is marked lower just because it is unexpected by GPT4. \n\nNow a retort might be: But you made those changes, the model did not produce that outcome! The model will always generate outcomes in the distribution!\nFair! But (a) that de-incentivizes training of new models that are have better coverage; and; (b) there is no way someone won’t read that paper and consider — let’s create an LLM based service that proposes to judge interviewees based on LLM scoring."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#performance-report",
    "href": "notes/case-of-llm-evals.html#performance-report",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Performance Report",
    "text": "Performance Report\nIn the realm of evaluation, it is vital to consider how we present the performance metrics. These numbers can be influenced by various factors, such as dataset splits and slight variations. It would be ideal to conduct multiple tests for each sample, utilizing different prompts and samples. However, this approach can be resource-intensive and necessitate significant modifications to evaluation frameworks. Therefore, it is crucial to approach the reported numbers with caution and maintain a level of skepticism.\nIf you have been involved in the machine learning field prior to the rise of large language models like GPT, you might recall the practice of running each test sample multiple times with various randomly seeded models. However, due to the lack of control over seed during inference in GPT models, it is advisable to run tests at least three times. Reporting the mean and standard deviation becomes essential for properly interpreting the significance of the results. While p-values can be messy, it is even more problematic to claim significant improvement based on a few points difference and a single inference result.\n\n\nTwo back to back answers from GPT4, temperature set to 0, about finding a largest number above 90 and below 100 that is divisible by both 7 and 3.\n\nClick on images to expand them.\n\n\n\n\n\n\n\n\n\n\n\nAnother aspect to consider is the level of granularity in reporting. Many academic datasets already suffer from various issues, and the problem is further exacerbated by averaging values across these large multi-task datasets without considering the specific evaluation goals of each test sample. Currently, most reports lack granularity even at a high-level task-based assessment, let alone sample-based granularity.\nMosaic 30B (released on 2023-06-22) explores the idea of combining benchmarks into thematic groups.\n\n\n\n\n\nEvaluation harness thematic grouping. Read more at https://www.mosaicml.com/blog/mpt-30b\n\n\nLastly, we must address the concept of prompt-fine tuning. Numerous research papers present results on the test set by identifying the best prompt for a given task. While this approach may seem reasonable in theory, it fails to provide a reliable measure of the model's performance when it comes to solving real-world problems encountered by average users. If the intention is to use the prompt as an auxiliary component in a pipeline, then finding the optimal prompt for the task and model is acceptable. However, for direct user-facing end-to-end models, it is crucial to acknowledge that the best prompt for obtaining the correct answer may not be realistic or feasible for all users, especially in the case of general-purpose models.\n\n\n\n\n\n\nArmen talks about prompt fuzzing\n\n\n\n\n\n\n\n\n\n\nSide note: There is a proposal of encrypting and encoding prompts and evaluation. And I completely support that proposal. While in an ideal world we might want opt-in for data, this isn’t the case at the moment, and we can safeguard ourselves from train-test leakage concerns.\nI have lost this paper in my never-ending pile. If someone finds this paper, please let me know and I will edit this document."
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Research Notes",
    "section": "",
    "text": "Quick Navigation\n\n\n\nHomepage, Email, Resume, Research Notes, and, Publications\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nThe Curious Case of LLM Evaluations\n\n\n\n\n\n\n\nText\n\n\nEvaluation\n\n\nMetric Design\n\n\nOpinion\n\n\nFoundation Models\n\n\n\n\nOur modeling, scaling and generalization techniques grew faster than our benchmarking abilities - which in turn have resulted in poor evaluation and hyped capabilities. Every ability is amazing and great, if we do not have the tools to figure out what that ability is, or how good the model is at that ability. We might always believe the model will win every race, if all we do, is have the race on paved roads, with yellow trees on every right turns, and green trees on every left turn.\n\n\n\n\n\n\n06-25-2023\n\n\n\n\n\n\n  \n\n\n\n\nNo, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test\n\n\n\n\n\n\n\nFoundation Models\n\n\nEvaluation\n\n\nPrompting\n\n\nTheory of Mind\n\n\n\n\nDiscussing the prospect of deriving instinct and purpose for a prompt and creating examples for evaluation problems focussing the Sally-Anne False-Belief Test and provide a summary of when GPT4 and GPT3.5 pass or fail the test.\n\n\n\n\n\n\n04-10-2023\n\n\n\n\n\n\n  \n\n\n\n\nCAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n\n\n\n\n\n\n\nText\n\n\nEvaluation\n\n\nMetric Design\n\n\nSchema\n\n\nInterpretation\n\n\nData Annotation\n\n\nFoundation Models\n\n\n\n\nPrompt-based language models have limitations in classification and often require users to test multiple prompts with varying temperatures to identify the best fit. A text annotation framework addresses this by introducing explicit prompt definition and validation, and can improve performance in labeling or retrieval tasks at scale.\n\n\n\n\n\n\n03-01-2023\n\n\n\n\n\n\n  \n\n\n\n\nDesigning Interfaces for Delivering and Obtaining Generation Explanation Annotations\n\n\n\n\n\n\n\nText\n\n\nData Annotation\n\n\nDesign\n\n\n\n\nDesigning a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n\n\n\n\n\n\n02-01-2023\n\n\n\n\n\n\n  \n\n\n\n\nControlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?\n\n\n\n\n\n\n\nText\n\n\nEvaluation\n\n\nMetric Design\n\n\nSchema\n\n\nInterpretation\n\n\nData Annotation\n\n\n\n\nEnd users affect explanation efficacy. NLP papers overlook other factors. This paper examines how utiltiy of saliency-based explanations change with controlled variables. We aim to provide a standardized list of variables to evaluate and show how SoTA algorithms rank differently when controlling for evaluation criteria.\n\n\n\n\n\n\n03-01-2022\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mimansa Jaiswal",
    "section": "",
    "text": "I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.\n(View my resume)"
  },
  {
    "objectID": "index.html#hi-im-mimansa",
    "href": "index.html#hi-im-mimansa",
    "title": "Mimansa Jaiswal",
    "section": "Hi, I’m Mimansa",
    "text": "Hi, I’m Mimansa\nI’m Mimansa, a final year PhD candidate in Computer Science (AI/Interactive Systems) at the University of Michigan.\nI am fortunate to be working with Prof. Emily Provost as part of the CHAI group. I completed my undergrad in Computer Engineering from Institute of Engineering and Technology, Indore in 2017, and worked with Prof. G.L. Prajapati for my bachelor’s thesis.\nTl;dr: I work in the area of developing interpretable and human imitating evaluation and cost-effective data collection procedures.\nOther than research, I am interested in science communication, sketchnoting, personal knowledge management and cooking. One of my favorite activities during international conferences is to plan every single meal.\nI love cats, and they keep me sane. I have two: Oreo and Bert (yes, that Bert, you read it right!)."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html",
    "href": "notes/controlled_evaluation_of_explanations.html",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "",
    "text": "Examples of variables used in checklisting explanation utility"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#types-of-questions-for-evaluation",
    "href": "notes/controlled_evaluation_of_explanations.html#types-of-questions-for-evaluation",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Types of Questions for Evaluation",
    "text": "Types of Questions for Evaluation\nSelecting appropriate questions for evaluation is essential for gauging the quality of explanations produced by deep learning models. To achieve a comprehensive evaluation, various question types that shed light on the model’s understanding and decision-making process must be considered. These question categories can include aspects such as:\n\nInput comprehension: Assessing the model’s ability to effectively interpret and extract relevant information from input data.\nTask-solving capabilities: Examining the model’s skills in applying its knowledge to accurately solve tasks and provide meaningful explanations.\nRationale behind specific decisions: Probing the model to explain the reasoning for its choices and the factors contributing to decision-making.\nInfluence of interaction speed: Evaluating the impact of explanation generation speed on the quality and depth of the model’s explanations.\n\nIncorporating these diverse question types enables researchers to gain a well-rounded understanding of the model’s strengths and weaknesses. Furthermore, it aids in the development of a standardized checklist for evaluating explanations in deep learning models, promoting consistency and rigor in the assessment process."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#selecting-samples-to-evaluate-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#selecting-samples-to-evaluate-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Selecting Samples to Evaluate Explanations",
    "text": "Selecting Samples to Evaluate Explanations\nPicking appropriate samples for evaluating explanations is a critical component in the assessment process. Employing a variety of selection strategies can enhance the understanding of a model’s explanatory capabilities across diverse situations. Consider the following approaches:\n\nModel-based uncertainty: Choose samples for which the model exhibits uncertainty, helping to analyze the model’s explanations in ambiguous scenarios.\nUndersampling: Focus on samples from underrepresented classes or groups to evaluate the model’s explanations for minority cases, thereby promoting fairness and robustness.\nNoisy labels: Opt for samples with noisy or incorrect labels to assess the model’s ability to explain its decisions when faced with unreliable ground truth information.\nMisclassification: Include samples that the model misclassifies to gauge its explanatory capacity when it generates incorrect predictions.\nData-based diversity: Select samples that represent diverse data points, which can provide insights into the model’s generalizability across various input spaces.\nDensity: Investigate both high-density (common) and low-density (unusual) samples to evaluate the model’s explanations for different regions of the input space.\nHomogeneity: Assess samples from homogeneous groups to understand how the model disentangles and explains the decisions for similar instances.\n\nIncorporating these selection strategies when evaluating explanations ensures a comprehensive assessment of the model’s robustness and generalizability across different scenarios. This practice, in turn, aids in the development of a reliable and well-rounded evaluation checklist."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#human-knowledge-in-evaluating-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#human-knowledge-in-evaluating-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Human Knowledge in Evaluating Explanations",
    "text": "Human Knowledge in Evaluating Explanations\nAccounting for human knowledge is crucial when assessing explanations generated by deep learning models. Key factors influencing the evaluation process include:\n\nFamiliarity with machine learning: Understanding basic concepts in machine learning and deep learning can impact how an evaluator interprets model explanations.\nDomain-specific knowledge: Possessing expertise in the field or subject area can facilitate deeper insights into the explanations and their relevance to domain-specific problems.\nPrevious experience: An evaluator with prior experience assessing model explanations often has a more refined understanding of what constitutes a good explanation.\nSubject knowledge: A strong grasp of the subjects or concepts related to the task at hand influences the ability to evaluate the validity and coherence of explanations.\n\nEngaging annotators with various expertise levels can lead to a broader understanding of the interpretability and accessibility of explanations. This diversity ensures that the evaluation process caters to different audiences, resulting in more accurate and inclusive assessments of deep learning models’ explanatory proficiency."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#model-information-known-priming-in-evaluating-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#model-information-known-priming-in-evaluating-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Model Information Known (Priming) in Evaluating Explanations",
    "text": "Model Information Known (Priming) in Evaluating Explanations\nPriming, which involves sharing model information with evaluators, can significantly impact the evaluation process. By understanding how different levels of model information influence the perception and utility of explanations, researchers can establish a balanced assessment. Here are three priming variations to consider:\n\nNo information: Withholding all model-related information from evaluators, which allows assessing explanations solely on their comprehensibility and informativeness without any bias or preconceived notions.\nRepresentative information: Providing evaluators with a basic understanding of the model, including its architecture, training method, and general performance. This approach helps evaluators contextualize the explanations and gain a general sense of the model’s capabilities, while minimizing the risk of priming-induced bias.\nAsking for information: Encouraging evaluators to request specific model information as needed during the evaluation process. This setup lets evaluators actively seek out details that can help them better understand or interpret the explanations, thereby promoting a more personalized and targeted evaluation experience.\n\nIncorporating these priming variations in the evaluation process allows researchers to determine the impact of prior model knowledge on the perception and usefulness of explanations. This information can further guide the development of a standardized checklist for evaluating deep learning model explanations more effectively.\nI apologize if my previous response did not fully align with Mimansa Jaiswal’s writing style. I will make a further attempt to refine the content while focusing on remaining concise and avoiding flowery language. Here is a revised version:"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#evaluating-explanation-methods-in-deep-learning-models",
    "href": "notes/controlled_evaluation_of_explanations.html#evaluating-explanation-methods-in-deep-learning-models",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Evaluating Explanation Methods in Deep Learning Models",
    "text": "Evaluating Explanation Methods in Deep Learning Models\nThe choice of explanation method is essential for understanding deep learning model behavior. Assessing various methods helps determine their effectiveness in conveying relevant information. The following is an overview of these methods:\n\nHighlight-based: Emphasizes crucial regions or input data components contributing to model decisions, facilitating focus on key factors. Examples include saliency maps and attention mechanisms.\nLanguage-based: Uses natural language to describe influences on the model’s predictions or decision-making processes—benefits users with limited domain knowledge by offering understandable explanations.\nDrag/drop-based: Provides an interactive experience, enabling evaluators to manipulate input data components and observe real-time model decision changes, deepening insights into the model’s workings.\nInfluence functions: Estimates each training instance’s impact on model predictions or decisions for specific test points, helping evaluators grasp the model’s generalization behavior and identify influential data points.\n\nBy examining different explanation methods in various scenarios, researchers can identify which methods yield meaningful insights into model behavior. This understanding can guide the development of improved evaluation checklists and more effective explanation techniques.\nI apologize for that oversight, and I appreciate your feedback. Here is the revised paragraph without the “By XYZ” ending structure:"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#characteristics-of-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#characteristics-of-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Characteristics of Explanations",
    "text": "Characteristics of Explanations\nAssessing explanation characteristics is crucial for evaluating their efficacy in conveying information about deep learning model behavior. These characteristics affect comprehensibility and trustworthiness of explanations. The following are key characteristics to evaluate:\n\nLength: Explanations should be concise yet informative. Overly short explanations might lack detail, whereas lengthy explanations risk becoming overwhelming or complex.\nReading grade level: Evaluating readability is vital for ensuring information accessibility. A suitable reading grade level ensures content clarity for users with diverse domain expertise.\nOutput confidence: Sharing the model’s output confidence enhances explanation trustworthiness. Users can better assess the model’s prediction certainty and explanation reliability.\nCiting references: Incorporating relevant sources, context, or prior knowledge bolsters explanation credibility. Cited sources allow users to validate presented information, fostering a deeper understanding of the model’s rationale.\n\nTaking these characteristics into account helps researchers pinpoint potential improvements needed for better comprehensibility and trustworthiness in explanations. This refined understanding aids in the development of more effective evaluation checklists and explanation techniques for deep learning models."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#demographic-factors",
    "href": "notes/controlled_evaluation_of_explanations.html#demographic-factors",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Demographic Factors",
    "text": "Demographic Factors\nIn developing explanations for deep learning models, demographic factors such as culture, age, and gender play a crucial role in shaping users’ perception and understanding. Addressing these factors ensures that explanations cater to diverse user populations.\n\nLanguage: Create explanations in multiple languages and consider cultural sensitivities to overcome language barriers. Adopt plain language principles to enhance comprehensibility for various language proficiencies.\nAge: Adapt explanations to different age groups by using age-appropriate terminology and complexity levels.\nGender: Employ gender-neutral language in explanations to uphold inclusivity and avoid alienating users based on gender identity.\nCultural context: Acknowledge cultural nuances and values within the target user population to craft relatable, contextually relevant explanations.\n\nIncorporating demographic factors in explanation development can enhance accessibility and inclusivity, promoting broader understanding and adoption of AI models throughout diverse demographic groups."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#explanation-type",
    "href": "notes/controlled_evaluation_of_explanations.html#explanation-type",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Explanation Type",
    "text": "Explanation Type\nUnderstanding the nature of explanation types—faithful, reliable, and plausible—is vital when evaluating explanations. Examining these categories enables insights into the model’s capacity for delivering explanations that meet user expectations and needs:\n\nFaithful explanations: These accurately reflect the model’s inner workings and decision-making processes. Assessing faithful explanations helps determine the model’s transparency, promoting confidence in its behavior and outcomes.\nReliable explanations: Consistently providing useful and relevant information, reliable explanations hold up under diverse circumstances. Evaluating this aspect enables the identification of the model’s robustness and applicability across varying scenarios.\nPlausible explanations: Appearing coherent and sensible, plausible explanations adhere to human intuition and domain knowledge. This evaluation aspect sheds light on the model’s ability to generate user-friendly insights that align with human reasoning.\n\nThrough the critical examination of these explanation types, we can better understand a model’s strengths and weaknesses in delivering explanations that satisfy users’ expectations and requirements. This knowledge subsequently informs the development of enhanced evaluation procedures and improved explanation techniques for deep learning models."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#task",
    "href": "notes/controlled_evaluation_of_explanations.html#task",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Task",
    "text": "Task\nThe type of task under evaluation, including natural language inference, sentiment analysis, or domain-specific tasks such as SOAP (Subjective, Objective, Assessment, and Plan), plays a substantial role in the evaluation process. Grasping the nuances of a particular task and its inherent challenges enables more insightful assessment of explanations:\n\nNatural Language Inference: Necessitates evaluations that focus on determining the model’s ability to reason about relationships between sentences, like entailment, contradiction, or neutrality. This ensures the model effectively captures semantic aspects.\nSentiment Analysis: Requires emphasis on gauging the model’s understanding of sentiment polarity in various expressions, capturing subtle emotional nuances and potential ambiguities.\nSOAP (Subjective, Objective, Assessment, and Plan): When dealing with domain-specific tasks like SOAP, evaluations should be grounded in an in-depth understanding of the specific domain and terminology, as well as the associated reasoning processes, to ensure meaningful and coherent explanations.\n\nRecognizing the characteristics of the task at hand is essential for conducting insightful evaluations of explanations. It allows for the development of targeted evaluation methodologies that cater to the specific challenges and requirements associated with each task type."
  },
  {
    "objectID": "notes/capstone.html",
    "href": "notes/capstone.html",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "",
    "text": "\"LLMs\", or Language Models, need to be trained and tested on different datasets and prompts to improve functionality and accuracy. In addition, versioning is important when assessing LLMs. Iterating on the prompts and datasets can produce better results. However, keeping different versions of both prompts and datasets can help identify which changes led to improvements in performance. All these practices should be followed to effectively evaluate and improve LLMs."
  },
  {
    "objectID": "notes/capstone.html#prompting-closed-llms",
    "href": "notes/capstone.html#prompting-closed-llms",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Prompting Closed LLMs",
    "text": "Prompting Closed LLMs\n\n\n\n\n\nWhen we are limited to observations, it becomes essential to employ experimental design in order to assess the capabilities of a model.\nIn order to do effectively, there are three key components we require:\n\nFirstly, we need to establish controls, which serve as the baseline for comparison and help us understand the impact various factors on the model's performance.\nAdditionally, conducting experiments allows us to systematically manipulate variables and observe their effects on the model's outcomes.\nLastly, carefully considering and accounting for variables is crucial in order to ensure the validity and reliability of our experimental results. By incorporating these elements into our approach, we can effectively evaluate and judge the capabilities of a model limited."
  },
  {
    "objectID": "notes/capstone.html#prompt-based-experimental-design-schemas",
    "href": "notes/capstone.html#prompt-based-experimental-design-schemas",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Prompt Based Experimental Design Schemas",
    "text": "Prompt Based Experimental Design Schemas\n\n\n\n\n\n\n\nMichael Frank talks about need some principles from experimental psychology\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet's talk about Prompt Based Experimental Design Schemas. These schemas can be thought of as templates, but with a key distinction. They not only focus on varying the input sample, but also on modifying the prompt specifications. By doing so, they allow us to closely monitor and analyze the changes that occur from one prompt to another. This approach provides a valuable way to study and understand the impact of different prompts on the overall outcome of an experiment."
  },
  {
    "objectID": "notes/capstone.html#prompt-definition-markup",
    "href": "notes/capstone.html#prompt-definition-markup",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Prompt Definition Markup",
    "text": "Prompt Definition Markup\nLet's delve into the concept of Prompt Definition Markup. Within this framework, we identify three types of steering in a prompt: start, process, and output.\nEach type serves a distinct purpose. Additionally, an output steer consists of two components: instructions (including constraints and format) and options.\n\n\n\n\n\nTo illustrate, let's consider the \"Sparks of AGI\" paper, which utilizes only a start steer, indicating the initial direction or guidance provided in the prompt.\n\n\n\n\n\nThis approach allows for a clear and structured definition of prompts, enabling effective experimentation and analysis.\nTo enhance the prompt, we can introduce a process steer in addition to the start steer, encouraging a step-by-step approach. Furthermore, we can incorporate an output instruction steer, specifying that the response should only include the folder path and no additional information.\nThese modifications provide additional guidance and constraints to the prompt, allowing for a more structured and controlled experimental design."
  },
  {
    "objectID": "notes/capstone.html#sample-markup",
    "href": "notes/capstone.html#sample-markup",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Sample Markup",
    "text": "Sample Markup\nLet's now turn our attention to Sample Markup. A sample consists of two key components: examples and input. However, if our objective is zero-shot prompting, we may not require specific examples.\nIn cases where examples are included, they can be categorized as either positive (A1) or negative (A2) examples, providing different instances for the model to learn from. On the other hand, the input component comprises three parts: the sample itself (B1), a question (B2), and a concept (B3). These elements collectively shape the input provided to the model, allowing for a more targeted and contextualized prompt.\n\n\n\n\n\nLet's consider an example to illustrate the concept of Sample Markup. In the context of the Theory of Mind False Belief Test, there are no specific examples provided. Instead, the prompt consists of a sample, a question, and an implicit concept. We can annotate this prompt accordingly, as depicted in the accompanying picture."
  },
  {
    "objectID": "notes/capstone.html#output-schema",
    "href": "notes/capstone.html#output-schema",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Output Schema",
    "text": "Output Schema\nThe output schema is a complex process that intersects significantly with large language model (LLM) evaluation techniques. If you're interested in delving deeper into this topic, I recommend checking out my other post. It provides further insights and information about the intricacies involved in designing and evaluating output schemas for LLMs."
  },
  {
    "objectID": "notes/capstone.html#metadata",
    "href": "notes/capstone.html#metadata",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Metadata",
    "text": "Metadata\nLet's now discuss the final component of the proposed schema: Metadata. Metadata plays a crucial role in shaping the overall prompt-based experimental design. It consists of five key parts that contribute to the effectiveness and coherence of the experiment:\nA. Connectors\nB. Iteration\nC. Variation\nD. Intuition\nE. Experimental Design\n\n\n\n\n\n\nConnectors\nLet's delve into the concept of Connectors within the prompt-based experimental design schema. Connectors serve as internal elements within a single prompt and play a crucial role in shaping the prompt's context and implications. They can serve different purposes:\nA1. Addition: Connectors can add information that reinforces the belief or statement presented in the prompt. This additional information further strengthens the intended message or concept.\nA2. Negation: On the other hand, connectors can also negate certain information presented in the prompt. By negating specific details, the prompt can introduce contrasting or alternative perspectives.\nA3. Exception: Connectors may also be used to highlight exceptions to the information being tested or present in the prompt. These exceptions provide additional nuances and complexities to the prompt, allowing for a more comprehensive exploration of the given scenario.\nFor instance, consider the following example: \"He says nothing about this to Alice, and Dropbox also does not notify Alice.\" In this case, the connector reinforces the belief or statement being made, emphasizing that both the person mentioned and Dropbox do not inform Alice about a particular matter. This connector strengthens the prompt's intended message and adds clarity to the scenario being presented.\n\n\n\n\n\n\n\nIteration\nIteration involves making changes to the prompt itself, without modifying the sample or the desired output. It allows for refining and improving the prompt to enhance the experimental design and guide the model's response.\nThere are different ways in which iteration can be implemented:\nB1. Multi-instruction: This involves adding multiple instructions within the prompt, providing additional guidance and directions to the model. These instructions help shape the model's understanding and guide its response in a more specific manner.\nB2. Rewording: Rewording the prompt entails changing the phrasing or wording of the prompt while maintaining the same underlying concept. This can be done to clarify the prompt, emphasize certain aspects, or provide a different perspective for the model to consider.\nB3. Chaining: Chaining refers to linking multiple prompts together in a sequential manner. Each prompt builds upon the previous one, creating a chain of prompts that guide the model's thought process and response. This approach allows for a step-by-step exploration of the given scenario or concept.\nFor example, adding the phrase \"Let's think about this step by step\" to the prompt can be considered an iteration aimed at incorporating a \"Process steer.\" This addition provides a clearer instruction to the model, encouraging a systematic and sequential approach in its response.\n\n\nIntuition\nIntuition refers to the reasoning or underlying purpose behind the prompt. It represents the intention or objective that the researchers aim to achieve through the prompt.\nWe can categorize intuition into three types:\nC1. Implicit: Implicit intuition refers to the underlying concept or idea that is implicitly conveyed through the prompt. It represents the broader theme or topic that the prompt is designed to explore or address.\nC2. Explicit: Explicit intuition involves explicitly stating the purpose or intention behind the prompt. It provides a clear and direct indication of the specific aspect or perspective that the prompt aims to capture or investigate.\nC3. Test: Test intuition refers to the specific test or evaluation being conducted through the prompt. It highlights the particular assessment or examination that the prompt is designed to facilitate.\nFor example, let's consider a Theory of Mind paper. We can discretize the intuition as follows:\nC1. Implicit (Theory of Mind): The implicit intuition of the prompt revolves around the exploration of the Theory of Mind concept, which involves understanding and analyzing how individuals perceive and interpret the thoughts, beliefs, and intentions of others.\nC2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service): The explicit intuition of the prompt focuses on the concept of modernization and its impact on individuals' access to unseen photos due to online services. It highlights the specific aspect of modernization and its influence on personal experiences.\nC3. Test (False Belief Test): The test intuition of the prompt centers around conducting a False Belief Test, which aims to assess individuals' understanding of false beliefs and their ability to attribute different perspectives to others.\n\n\n\n\n\n\n\nVariation\nNow let's shift our attention to Variations within the prompt-based experimental design schema. Variations occur across different input samples and play a crucial role in shaping the experimental design. They allow for the exploration of diverse scenarios and perspectives, ensuring a comprehensive analysis of the model's capabilities.\nWe can categorize variations into different aspects:\nD1. Output Specification: This aspect focuses on the desired output of the prompt. It can be categorized as generative (Gen.) or discriminative, depending on whether the prompt aims to generate new content or make a judgment or discrimination based on the input.\nD2. Concept: Conceptual variations involve different concepts or ideas presented in the prompt. These concepts can be similar, opposite, or serve as control variables, providing a range of scenarios for the model to process and respond to.\nD3. Task: Task variations relate to the specific task or objective of the prompt. This aspect can involve exploring the subjectivity or objectivity of the prompt, allowing for different perspectives and evaluation criteria.\nIt is crucial to consider variations because they contribute to a comprehensive assessment of the model's capabilities. Without a comprehensive enough variation set, it becomes challenging to make accurate judgments regarding the model's performance and behavior.\nFor example, let's consider the following prompt:\nC2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service)\nwith D2. Similar Concept (Unseen Object 'Cause Sleeping)\nIn this case, the explicit intuition of the prompt revolves around the concept of modernization and its impact on individuals' access to unseen photos due to online services. The variation in the concept introduces the idea of an unseen object causing sleeping, adding a similar yet distinct scenario for the model to process and respond to.\nPrevious Example\nTask 1: Math (BASIC Subtraction) + Implicit \"No knowledge transfer through sleeping\" + External actor (Action taken by someone else and not self)\n\nBoth GPT4 and GPT3.5 fail on this simple task\n\nSomehow sleep is not encoded as no knowledge transfer?\n\n\n\n\n\n\n\nExperimental Design\nThe experimental design encompasses the basics of the experiment and the corresponding prompt, providing a structured framework for conducting the evaluation.\nWe can define different components within the experimental design:\nE1. Control: A control prompt serves as a baseline for comparison. It can be a prompt that compares the model's performance to a base model or a different kind of model, where it is known, to the best of our knowledge, that the model does not exhibit certain capabilities. For example, if we can confidently say that GPT3.5 does not \"pass\" the false-belief test, it can serve as a control prompt.\nE2. Previous Prompt: The previous prompt serves as a reference point for the current experiment. It allows us to note the differences made in the current prompt and annotate those differences for iterations and concepts. It helps in tracking the evolution and progress of the prompt design.\nE2a. Prompt: The prompt itself is a crucial part of the experimental design. It includes the specific instructions, information, and context provided to the model to generate a response.\nE2b. Directionality: Directionality refers to the specific direction or focus of the variables compared to the previous prompt. It includes aspects such as leakage, ambiguity, specificity, and coverage. These variables aim to reduce ambiguity, enhance specificity, and provide better coverage in the prompt design.\nE3. Date: The date component specifies the time or period during which the experiment is conducted. It helps in documenting and tracking the experimental timeline.\nE4. Model: The model component specifies the particular model being used in the experiment. It helps in defining the testing environment and ensuring consistency across evaluations.\nBy codifying the directionality of these variables and considering the experimental design components such as control, previous prompt, date, and model, we can establish a structured and systematic approach to prompt-based experimentation. This framework allows for clear comparisons, iterative improvements, and effective tracking of the experiment's progress and outcomes.\nPrevious Example\nSome of these prompts have the word \"expect\" which is known to be ambiguous\n\n\n\n\n\nLet's consider an example to illustrate the importance of annotating the directionality within the experimental design. In this case, we initially believe that the model is correct and passes the false-belief test. However, upon further examination, we identify a potential issue of leakage in the prompt.\nThe use of words such as \"think\" or \"believe\" in the prompt directly correlates with the concept of a \"belief-test.\" This correlation can unintentionally guide the model towards the correct answer, potentially compromising the validity of the test. To address this concern, we can annotate the directionality by replacing the problematic wording with alternatives. For instance, replacing \"think\" with \"look for\" in the prompt may help mitigate the issue of leakage.\nBy annotating the directionality and actively addressing potential biases or correlations within the prompt, we can ensure a more accurate evaluation of the model's capabilities. This approach enhances the integrity of the experiment and allows for clearer insights into the model's performance and understanding of the desired concepts.\nPrevious Example\nUsing the word think or believe makes it work for GPT4, not for GPT3.5.\n\nSuggestions to frame it without using the explicit terms \"think\" \"believe\" which correlate to the expectation that outcome is independent of the action?"
  },
  {
    "objectID": "notes/capstone.html#recommended-papers-in-the-area-",
    "href": "notes/capstone.html#recommended-papers-in-the-area-",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Recommended papers in the area:",
    "text": "Recommended papers in the area:\n\nMarkup/Interfacing Languages\nPromptSource: An Integrated Development Environment and Repository for Natural Language Prompts\nOpenPrompt: An Open-source Framework for Prompt-learning\nPromptChainer: Chaining Large Language Model Prompts through Visual Programming\nPrompting Is Programming: A Query Language For Large Language Models\nMarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding\n“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models\n\n\nPrompt Building\nLanguage Model Cascades\nAutomatic Chain of Thought Prompting in Large Language Models\nLarge Language Models Are Human-Level Prompt Engineers\nAUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts\nDemonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP\nPromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts\nGuidance: Control modern language models more effectively and efficiently than traditional prompting or chaining\n\n\nDataset Cartography\nLearning from Others’ Mistakes: Avoiding Dataset Biases without Modeling Them\nDataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\nDiversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections\n\n\nExperimental Design\n\n\nLLM Evaluation\nSelection-Inference: Exploiting Language Models for Interpretable Logical Reasoning"
  },
  {
    "objectID": "stream/index.html",
    "href": "stream/index.html",
    "title": "Blurbs",
    "section": "",
    "text": "Quick Navigation\n\n\n\nHomepage, Email, Resume, Research Notes, and, Publications\n\n\n\nBlurbs is where I keep my random musings, quick reviews, and noteworthy quotes all in one place. It’s convenient to have a curated collection instead of them being scattered throughout my feed. It’s a fun little side space to gather everything. \n\n\n\n\n\n\n\n  \n    \n        05-07-2022\n      \n      \n      \n        Addressing 2020-2022 personal challenges and reassuming professional responsibility\n      \n      \n      \n      \n        From Q2 2020 to Q2 2022, unforeseen personal circumstances severely impacted my ability to complete projects and maintain consistent communication. These events causing significant disruptions included the tragic loss of my maternal grandfather to cancer (2020), followed by the sudden demise of my paternal grandmother due to COVID-19 (2021), and then the demands of managing major health crises involving my father and remaining grandmother (2022). However, having now defended my PhD and made significant strides in task delivery, I've found my footing again. As a former intern at Meta and AI2, the personal turmoil I experienced inhibited me from concluding the projects I was passionate about, leaving me deeply saddened. Over time, and as my stability returned, I made private progress on these projects, but feelings of embarrassment and regret kept me from re-establishing contact. I acknowledge the past disappointments during this challenging period and sincerely apologize. Today, with confidence and affirmation, I assure that such setbacks will no longer impede my professional dedication and, with renewed commitment, I take responsibility for any lingering tasks.\n        \n        \n      \n      \n      \n        PhD\n        Academia\n        Internships\n        Mental Health\n        \n    \n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "personal_notes/index.html",
    "href": "personal_notes/index.html",
    "title": "Personal Notes",
    "section": "",
    "text": "Quick Navigation\n\n\n\nHomepage, Email, Resume, Research Notes, and, Publications\n\n\n\nThis is a just a bunch of posts about life, tools, productivity, preferences etc. More like a get to know me beyond research zone.\n\n\n\n\n\n\n\n\n\n\nWhat is the easiest way to create AND maintain a personal academic website?\n\n\n\n\n\n\n\nTools\n\n\nWebsite\n\n\nAcademia\n\n\n\n\n\n\n\n\n\n\n\n07-31-2023\n\n\n\n\n\n\n\n\nWhat softwares do I actually use on my Mac as a software enthusiast?\n\n\n\n\n\n\n\nTools\n\n\nMacOS\n\n\nSoftwares\n\n\n\n\n\n\n\n\n\n\n\n07-25-2023\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "personal_notes/personal_academic_website.html",
    "href": "personal_notes/personal_academic_website.html",
    "title": "What is the easiest way to create AND maintain a personal academic website?",
    "section": "",
    "text": "Another week of personal website debate on Twitter, another set of heavy compilation based suggestions to create and maintain a personal website. So, I decided to take a look at all the recent website builders and provide a set of recommendations. If you do not want to read this in entirety, my suggestions can be summarized below:"
  },
  {
    "objectID": "personal_notes/personal_academic_website.html#example-website",
    "href": "personal_notes/personal_academic_website.html#example-website",
    "title": "What is the easiest way to create AND maintain a personal academic website?",
    "section": "Example Website",
    "text": "Example Website\nHere is the website I made using montaigne. Man, it works! There is nothing missing in it, that I would need on a personal website! Sure, it does not have comments using giscus as I do, but who really needs that? Yes, the domain is hard to spell, but, the developer very kindly offered a pay what you want pricing for custom domains. Yes, you cannot host it on github, but the content is all in Apple Notes. The day montaigne goes down, you will still have all your content with you and be able to port it elsewhere.\nIf you want to figure out what the source notes requires or needs, I even made the folder of notes that I use to create the website public (Link)."
  },
  {
    "objectID": "personal_notes/personal_academic_website.html#supported-content",
    "href": "personal_notes/personal_academic_website.html#supported-content",
    "title": "What is the easiest way to create AND maintain a personal academic website?",
    "section": "Supported Content",
    "text": "Supported Content\nWhat all can you do using Apple Notes? Well a ton! You can customize your menu, headers etc. You can use images, videos, embeds, headings that have anchors. Take a look here! Yes, the tags are clickable and filterable! You can do folders. It comes integrated with an RSS feed, quick links to your profiles in the footer, and a QR code to add on your research posters for someone to quick access your personal website."
  },
  {
    "objectID": "personal_notes/personal_academic_website.html#tldr",
    "href": "personal_notes/personal_academic_website.html#tldr",
    "title": "What is the easiest way to create AND maintain a personal academic website?",
    "section": "tl;dr",
    "text": "tl;dr\nHonestly, I cannot recommend this option enough. The domain would cost you a bit (maybe $10/year), if you go the custom domain route, but the ease of updating your website in a second beats any other options."
  },
  {
    "objectID": "single_page/updates.html",
    "href": "single_page/updates.html",
    "title": "Updates",
    "section": "",
    "text": "Quick Navigation\n\n\n\nHomepage, Email, Resume, Research Notes, and, Publications\n\n\n\n\n\n\n\n\n\n    \n    May 2023Defended my PhD titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    May 2022I was selected as one of the 8 Barbour fellows for 2022-2023 across all of UMich\n    Nov 2021Defended my PhD proposal titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    Oct 2021I will be presenting our work on sociolinguistic inspired privacy evaluation at text as data (TADA) conference 2021, UMich, Ann Arbor.\n    Sep 2021I am interning this Fall in Allen AI with Ana Marasović in the area of evaluation and interpretability.\n    May 2021Excited to be interning in the FAIR NLP group this summer and looking forward to work at the intersection of Linguistics and ML.\n    Sep 2020I have been chosen as the student representative for Faculty Hiring at my university for the 2020-21 season. Looking forward to knowing and communicating the 'usually hidden' processes.\n    May 2020Excited to be interning in the Conversation AI group at Facebook AI working on automated dialogue evaluation.\n    Dec 2019I'll be attending NeuRIPS. Shoot me an email if you want to meet and talk about anything!\n    Jan 2019I'll be a teaching assistant for the course on Applied Machine Learning for Affective Computing with my advisor. So excited to be teaching for the first time!\n\n\n\nNo matching items\n\n Back to top"
  }
]