[
  {
    "objectID": "notes/tom_evaluation_in_foundation_models.html",
    "href": "notes/tom_evaluation_in_foundation_models.html",
    "title": "A Framework for Evaluating Theory of Mind through Structured Testing",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/case-of-llm-evals.html#evaluation-datasets",
    "href": "notes/case-of-llm-evals.html#evaluation-datasets",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Evaluation Datasets",
    "text": "Evaluation Datasets\nEvaluation Datasets or Evaluation Sets or Eval Sets: These are the test samples that the model is being evaluated for. There are multiple ways to construct these evaluation datasets and to use them; each of them coming with its own set of problems.\nUsing the similar set of datasets for evaluation come with a another set of problems:\n\nFuzziness in prompts: Now that there are prompts involved in the process; we really need to consider the fuzziness that comes from the prompt itself. While the evaluation datasets were used without any “instruction language” or “prompted addition”, the test samples at least remained consistent. The prompts here may not. I talk more about prompt introduced variability in my other post about prompt based benchmarking. It specifically talks about three components of a prompt for standardizing prompt based evaluation: prompt steer, sample steer, output steer. You can read it here.\n\nUntraceability: Going back to the point of data leakage; while it used to always be a problem; now that no one has any idea about any of the data that went into the model, even the best-faith and triple checked evaluation has no guarantees of being a out of distribution evaluation.\n\n\n\n\n\nPrompt parts: Start, Process and Output Steer and influences that arise from it\n\n\n\nWhat can these evaluation datasets look like?\n\nSuggest examples in pull requests and I will merge them here\n\nPre-curated evaluation sets from various standardized tests — These are mostly designed to be differentiating for humans and not for models. Additionally, they can have memorization based questions, that can incorrectly be inferred as understanding in context of evaluating LLMs.\n\n\n\n\n\n\nPre-curated eval sets\n\n\n\n\n\n\n\n\nWhat Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset fromMedical Exams\n\n\n\n\n\nWeb scraped evaluation sets for ML model testing — These datasets are created by scraping the internet for particular tags and using the corresponding tag as the label for the sample itself or are annotated by various human annotators. The samples from these datasets are most likely to be present in the training set of these foundation models itself and hence, it is often not a good idea to just rely on these for evaluation.\n\n\n\n\n\n\nScraped eval sets\n\n\n\n\n\n\n\n\nTriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\n\n\n\n\n\nHuman created and curated evaluation sets for the sole purpose of evaluating a task — These test sets are usually designed as a measure against data leakage. After all, humans can probably create a distinct enough pattern to evaluate against. While that is a pro, these datasets come with flaws of their own; for example they are really small and, are harder to create and update.\n\n\n\n\n\n\nHuman curated eval sets\n\n\n\n\n\n\n\n\nHumanEval dataset proposed in Evaluating Large Language Models Trained on Code\n\n\n\n\n\nFuzzy Versions — These are variants of exisiting datasets or evaluation sets, extensions or additions created with the explicit purpose of introducing and testing for model behavior in presence of such variability. This variability can be intentionally adversarial, can be aimed to introduce out of distribution tokens for robustness testing or just for the purpose of creating paraphrased equivalent samples.\n\n\n\n\n\n\nFuzzed eval sets\n\n\n\n\n\n\n\n\nFor example, a set of adversarial prompts and inputs that act as addition/in replacement to the original evaluation samples, as proposed in PromptBench\n\n\n\n\n\nAd-hoc testing samples by humans — These are performed as conversation based evaluation of these models. While they are most likely to be accurate; they are are biased by the mere idea that the human usually needs to know the solution to the problem to be able the question for evaluation purposes. This often ends up in; what I call as; human imagination collapse or the likelihood that an individual human would often be set on a trajectory to test for and not meaningfully diversify; at least in a single setting.\n\n\n\n\n\n\nAd-hoc testing\n\n\n\n\n\n\n\n\nEvaluating models through single turn or multi-turn conversations in OpenAssistant Conversations - Democratizing Large Language Model Alignment\n\n\n\n\n\n\n\n\n\n\nFrom this tweet by Subbarao Kambhampati (కంభంపాటి సుబ్బారావు)"
  },
  {
    "objectID": "notes/case-of-llm-evals.html#model-output",
    "href": "notes/case-of-llm-evals.html#model-output",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Model Output",
    "text": "Model Output\nNow let us come to the second problem — output from generative models.\n\nOne of the major problems with almost every single solution that we, as a community have proposed: is this idea of evaluating generative models using discriminative outputs.\nModel output is heavily dependent on the (a) prompt asked for the output; and (b) the answers asked for. For example, if you ask a model for a label 0 or 1; vs asking a model for labels in words (for example: spam or not spam); you might end up with different outcomes. Another example: asking a model for a direct output and extracting the answer can lead to a different answer than in the multiple-choice scenario.\nRegression based model outcome does not necessarily scale and hence can change the standard deviation and mean of the regressed output. For example, if you ask a model to provide a graded rating between 0-10; you cannot with certainty scale that grading to 0-1, especially if you care about significance testing.\n\n\n\n\n\n\n\nDo you want to know why this is a problem with examples?\n\n\n\n\n\nLet’s take the example in Textbooks are all you need paper again and ask it to grade 2 responses on 3 scales.\nWhy? Because the description says that the GPT4 model was asked to grade between 0-10, but the numbers are reported scaled between 0-1.\n\n\n\nThe paper saying that the model is asked for grades between 0 to 10, and that is then scaled to report between 0-1.\n\n\n\n\n\n\n\nLet’s start with the completely correct response.\n\n\n\nThe GPT4 model says the response is 9/10 when asked for a grade between 0-10.\n\n\n\n\n\nThe GPT4 model says the response is .95/10 when asked for a grade between 0-1.\n\n\n\n\n\nThe GPT4 model says the response is 4.5/5 when asked for a grade between 0-5; which is great, right? Because at least the scaling is consistent.\n\n\nBut, let’s consider some other solutions, those that are incorrect in some way or another. Let’s do this by changing squaring to 2.2, and sorting the second list in reverse.\n\n\n\nThe GPT4 model says the response is 8/10 when asked for a grade between 0-10.\n\n\n\n\n\nThe GPT4 model says the response is 4.5/5 when asked for a grade between 0-5. The scaling is constant here too!\n\n\n\n\n\nThe GPT4 model says that the response is 0.9/1. And now all the scaling is messed up."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#sample-output-transformation",
    "href": "notes/case-of-llm-evals.html#sample-output-transformation",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Sample/Output Transformation",
    "text": "Sample/Output Transformation\nThere are many transformations applied to model outputs or their inputs. These can roughly be divided into 4 categories:\n\nLooped Transformations\nLoop based transformations usually follow the philosophy of —&gt; what if we additionally add the model output + some form of evaluation of the current answer (either from the same model, another model or a human) back into the model; such that it eventually reaches the perfect outcome. Some examples of these would be Self-Critique models.\n\n\n\n\n\n\nExample Papers\n\n\n\n\n\n\n\n\nReflexion: Language Agents with Verbal Reinforcement Learning develops a modular formulation for Reflexion, utilizing three distinct models: an Actor generates text and actions; an Evaluator model scores the outputs produced by Actor; and a Self-Reflection model, generates verbal reinforcement cues to assist the Actor in self-improvement.\n\n\n\n\n\n\n\nChained Transformations\nChain based transformations usually do not have a measured evaluation in between a set of model input → output → model input and so on. These chains are usually pre-defined and have a restricted number of paths to follow.\n\n\nAtomic Outputs\nThis method involves breaking down the output of a model; either manually, through a rule based system or through AI itself; into atomic components that can be evaluated individually to combine to form a larger weighted grade.\n\n\nConstrained Output\nThis method involves either using log probability (which isn’t accessible in case of GPT3.5/GPT4 APIs) or other internal constraints to ensure that the model responds with tokens belonging to a certain pre-decided or allowed distribution."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#ground-truth",
    "href": "notes/case-of-llm-evals.html#ground-truth",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Ground Truth",
    "text": "Ground Truth\nThis does not need much explanation but there are certain aspects to remember here; especially when you think of ground truths in respect to present scene of evaluation. Firstly, ground truth can be biased, ambiguous, or have a high range of disagreement. In case of human centered tasks, such as likability of a prose, the disagreement is often averaged out rather than considered as an annotation curve, you need to compare the model’s outcome multiple times to achieve a true distribution comparison.\nWith new evaluation practices, you need to remember that you may or MAY NOT have ground truth in a certain evaluation.\nRemember the 3 possible pitfalls with ground truth:\n\nInclusion in the loop or chain based transformations\nInclusion in in-context or few shot learning examples in case of prompt steering\nGround truth might be used to establish correlation between the newly proposed completely automated metric; and not actually used in evaluation."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#evaluation-medium",
    "href": "notes/case-of-llm-evals.html#evaluation-medium",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Evaluation Medium",
    "text": "Evaluation Medium\nIn my perspective, the evaluation medium can be categorized into three distinct groups.\n\nDirect Evaluation Metrics\n\n\n\n\n\n\nExample Papers\n\n\n\n\n\n\n\n\nTextbooks are all you needevaluation with HumanEval and MBPP\n\n\n\n\n\nThe first category I call \"direct evaluation metrics.\" These are the traditional metrics that have been widely used in the field for a long time. Metrics like accuracy and F1 score fall into this category. Typically, this approach involves obtaining a single output from the model and comparing it to a reference, either through constraints or by extracting the desired information. The evaluation can be done through ad-hoc conversation-based evaluation, curated specialized test sets, or direct annotation.\nFor instance, one direct evaluation metric is comparing the model's accuracy directly to the ground truth. When evaluating multiple-choice question answers, the comparison can be based on matching the choice letter, the complete choice, or the distribution over choices. To gain a deeper understanding of how these evaluation approaches can impact the results, check out this article: What's going on with the Open LLM Leaderboard and Llama?\n\n\n\n\n\nNow how do we evaluate the model from these prompts?\n\n\n\n\nIndirect or Decomposed Model-Based Evaluation\n\n\n\n\n\n\nExample Papers\n\n\n\n\n\n\n\n\nRubric based from the same model.\nTinyStories: How Small Can Language Models Be and Still Speak Coherent English?\n\n\n\n\n\nSelf-critiquing models for assisting human evaluators\n\n\n\n\n\nG-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment using form-filling methods for evaluation and then calculate a correlation to human preference.\n\n\n\n\n\nComponent-wise model driven evaluation scores in LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models\n\n\n\n\n\nMoving on to the second category, we have \"indirect or decomposed heuristics.\" Here, we utilize smaller models, whether they are fine-tuned or raw decompositions, to evaluate the answers generated by the main model. The idea is to leverage these smaller models, which excel at specific tasks that large language models are generally good at, such as adjective identification or polarity identification. The outputs of these smaller models act as weak scores that are combined to provide a final label or evaluation for the generated output. This indirect evaluation approach allows for a more nuanced assessment of the model's performance, particularly in areas like the likability of prose. Although these models introduce some variability, it's important to note that they are often trained for regression tasks and fine-tuned for specific purposes.\n\nHonestly, the line between this method of evaluation and the next is blurry, especially in terms of impact and possibility of being wrong. Suggestions for better ontology are welcome!\n\n\n\nModel-Driven Evaluation\n\n\n\n\n\n\nExample Papers\n\n\n\n\n\n\n\n\nEvaluating responses by comparing to referenced ground truth in Sparks of AGI. Remember this is inclusion of ground truth and possibly the least problematic form of model driven evaluation.\n\n\n\n\n\nBring Your Own Data! Self-Supervised Evaluation forLarge Language Models evaluation based on invariance of model output based on fuzzed input samples\n\n\n\n\n\nTextbooks are all you needevaluation with GPT4\n\n\n\n\n\nUsing LLMs for explanations of smaller LLMs\n\n\n\n\n\nAsk the AI section from Language Models (Mostly) Know What They Know\n\n\n\n\n\nThe third category I refer to as \"model-driven evaluation.\" In these cases, the model itself provides the final score or evaluation. However, this introduces an additional layer of variability. Even if the model has access to ground truth information, the evaluation metric itself may introduce randomness or variability in the scoring process. For example, a common evaluation question could be: \"Is this generated output (O) similar to the ground truth answer (G)?\" The answer to this question not only depends on the randomness associated with the model's output but also on the inherent variability of the evaluation metric.\nIt's important to remember that recent evaluation practices may involve the inclusion or exclusion of ground truth in model-driven evaluation.\nThis can lead to 2 kinds of model-driven evaluations:\n\n[Inclusion of Ground Truth] Asking the model to compare to the ground truth and produce an output in affirmation or contradiction. This can also be seen as providing two statements to the model, and asking it to label them for entailment, paraphrasing or both.\n[Exclusion of Ground Truth] Asking the model to directly “judge” the output. In this case, the larger model is often provided outputs from the smaller models and asked to evaluate the answer’s correctness. This can range from a short feedback to a likert scale answer or anywhere in between. &gt; Note, that the larger model evaluating the smaller model paradigm might not necessarily hold for all papers and is significantly more dubious in claims than the former.\n\nThe general explanation provided for such situations is — but oh, this is how humans do this stuff too. So, we are asking GPT-4 to be more humane and avoiding binary labels that we needed initially.\nFor example, this is why the authors of Textbooks are all you need believe this is a correct method of evaluation.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetailed account of why I believe that using GPT-4 as a judge is incorrect in this case\n\n\n\n\n\nBut this, in my opinion, is not a correct way to account for non-binary labels. Especially in coding, when there is an existing method to run the function through tests and have assertions against expected answers. My judgement might have been different had this evaluation method been proposed for a subjective task, but coding, is both, an objective and assertable task, and this evaluation method discards all these properties of a coding task in general.\nLet’s break this down:\nWhat are the problems that general evaluations in coding have? The biggest problem is (a) writing representative tests and their expected output, and, (b) making sure that these tests are comprehensive and follow a curriculum based evaluation.\nThat is a bunch of jargon! What does this even mean?\nFor example, let’s take the example in their paper:\n\nWrite a python code for the following:\ndef sort_concat_square_deduplicate (list1, list2, my_threshold):\n\"\"\"\nThis functions takes two lists of integers, sorts each of them in ascending order, concatenates them, squares the entries at even indices, filters out entries smaller than my threshold and then removes duplicates. The resulting list is returned.\n\"\"\"\nOne of the major drawbacks of existing evaluation datasets is that they test the final answer. While the authors claim that it is a better idea to judge if the model implements components correctly.\nGoing back to decomposed testing, the correct way here would be to ask the model to provide atomic functions and then evaluate those atomic functions using tests and the final function, which calls the atomic functions in desired order.\nThis solves the problem of not having binary pass/fail labels. This of course requires more effort, but is substantially more accurate.\nDoesn’t GPT4 as a judge solve these problems anyway? No, no it does not.\nWhy not?\n\nThe method does not provide the exact same number every single time. For example, let’s consider the same doc string as mentioned above. And let’s evaluate how good Mosaic 30B-Chat is good at programming. \nNow, let us ask GPT4 to judge this. Not only does the answer change, the model suggests to do something that was not even asked for.\n\n\n\nThe model scores it 9.5/10\n\n\n\n\n\nThe model scores it 9/10\n\n\nImplicit assumption that the model actually scores completely incorrect answer at 0. This assumption is not tested by the authors. Let’s test it. The decomposition of this problem is into 6 atoms:\n\nSorting of 2 lists ⟶ Sort second list in reverse order\nConcatenation ⟶ Change to duplicate concatenation\nEven Indices ⟶ Change to every index\nSquare ⟶ Change to power 2.2\nThreshold Filter ⟶ Change it to filter to less than or equal to\nRemove Duplicates ⟶ Duplicate the filtered list\n\nGPT4 judges it to be 4/10. And, it does not see that the threshold value was changed.\n\n\n\nThe model scores the completely incorrect function at 4/10. It ignores an error. In the previous pictures, it creates an error or a suggestion which was not asked for and will render the code incorrect.\n\n\nAnything weird to the model, that is not in the general expected distribution will be marked low. For example, here is a correct code that is marked lower just because it is unexpected by GPT4.  Now a retort might be: But you made those changes, the model did not produce that outcome! The model will always generate outcomes in the distribution!\n\nFair! But (a) that de-incentivizes training of new models that are have better coverage; and; (b) there is no way someone won’t read that paper and consider — let’s create an LLM based service that proposes to judge interviewees based on LLM scoring."
  },
  {
    "objectID": "notes/case-of-llm-evals.html#performance-report",
    "href": "notes/case-of-llm-evals.html#performance-report",
    "title": "The Curious Case of LLM Evaluations",
    "section": "Performance Report",
    "text": "Performance Report\nIn the realm of evaluation, it is vital to consider how we present the performance metrics. These numbers can be influenced by various factors, such as dataset splits and slight variations. It would be ideal to conduct multiple tests for each sample, utilizing different prompts and samples. However, this approach can be resource-intensive and necessitate significant modifications to evaluation frameworks. Therefore, it is crucial to approach the reported numbers with caution and maintain a level of skepticism.\nIf you have been involved in the machine learning field prior to the rise of large language models like GPT, you might recall the practice of running each test sample multiple times with various randomly seeded models. However, due to the lack of control over seed during inference in GPT models, it is advisable to run tests at least three times. Reporting the mean and standard deviation becomes essential for properly interpreting the significance of the results. While p-values can be messy, it is even more problematic to claim significant improvement based on a few points difference and a single inference result.\n\n\nTwo back to back answers from GPT4, temperature set to 0, about finding a largest number above 90 and below 100 that is divisible by both 7 and 3.\n\nClick on images to expand them.\n\n\n\n\n\n\n\n\n\n\n\nAnother aspect to consider is the level of granularity in reporting. Many academic datasets already suffer from various issues, and the problem is further exacerbated by averaging values across these large multi-task datasets without considering the specific evaluation goals of each test sample. Currently, most reports lack granularity even at a high-level task-based assessment, let alone sample-based granularity.\nMosaic 30B (released on 2023-06-22) explores the idea of combining benchmarks into thematic groups.\n\n\n\n\n\nEvaluation harness thematic grouping. Read more at https://www.mosaicml.com/blog/mpt-30b\n\n\nLastly, we must address the concept of prompt-fine tuning. Numerous research papers present results on the test set by identifying the best prompt for a given task. While this approach may seem reasonable in theory, it fails to provide a reliable measure of the model's performance when it comes to solving real-world problems encountered by average users. If the intention is to use the prompt as an auxiliary component in a pipeline, then finding the optimal prompt for the task and model is acceptable. However, for direct user-facing end-to-end models, it is crucial to acknowledge that the best prompt for obtaining the correct answer may not be realistic or feasible for all users, especially in the case of general-purpose models.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\nSide note: There is a proposal of encrypting and encoding prompts and evaluation. And I completely support that proposal. While in an ideal world we might want opt-in for data, this isn’t the case at the moment, and we can safeguard ourselves from train-test leakage concerns.\nI have lost this paper in my never-ending pile. If someone finds this paper, please let me know and I will edit this document."
  },
  {
    "objectID": "notes/explanation_annotation_interfaces.html",
    "href": "notes/explanation_annotation_interfaces.html",
    "title": "Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html",
    "href": "notes/controlled_evaluation_of_explanations.html",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "",
    "text": "Examples of variables used in checklisting explanation utility"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#types-of-questions-for-evaluation",
    "href": "notes/controlled_evaluation_of_explanations.html#types-of-questions-for-evaluation",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Types of Questions for Evaluation",
    "text": "Types of Questions for Evaluation\nSelecting appropriate questions for evaluation is essential for gauging the quality of explanations produced by deep learning models. To achieve a comprehensive evaluation, various question types that shed light on the model’s understanding and decision-making process must be considered. These question categories can include aspects such as:\n\nInput comprehension: Assessing the model’s ability to effectively interpret and extract relevant information from input data.\nTask-solving capabilities: Examining the model’s skills in applying its knowledge to accurately solve tasks and provide meaningful explanations.\nRationale behind specific decisions: Probing the model to explain the reasoning for its choices and the factors contributing to decision-making.\nInfluence of interaction speed: Evaluating the impact of explanation generation speed on the quality and depth of the model’s explanations.\n\nIncorporating these diverse question types enables researchers to gain a well-rounded understanding of the model’s strengths and weaknesses. Furthermore, it aids in the development of a standardized checklist for evaluating explanations in deep learning models, promoting consistency and rigor in the assessment process."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#selecting-samples-to-evaluate-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#selecting-samples-to-evaluate-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Selecting Samples to Evaluate Explanations",
    "text": "Selecting Samples to Evaluate Explanations\nPicking appropriate samples for evaluating explanations is a critical component in the assessment process. Employing a variety of selection strategies can enhance the understanding of a model’s explanatory capabilities across diverse situations. Consider the following approaches:\n\nModel-based uncertainty: Choose samples for which the model exhibits uncertainty, helping to analyze the model’s explanations in ambiguous scenarios.\nUndersampling: Focus on samples from underrepresented classes or groups to evaluate the model’s explanations for minority cases, thereby promoting fairness and robustness.\nNoisy labels: Opt for samples with noisy or incorrect labels to assess the model’s ability to explain its decisions when faced with unreliable ground truth information.\nMisclassification: Include samples that the model misclassifies to gauge its explanatory capacity when it generates incorrect predictions.\nData-based diversity: Select samples that represent diverse data points, which can provide insights into the model’s generalizability across various input spaces.\nDensity: Investigate both high-density (common) and low-density (unusual) samples to evaluate the model’s explanations for different regions of the input space.\nHomogeneity: Assess samples from homogeneous groups to understand how the model disentangles and explains the decisions for similar instances.\n\nIncorporating these selection strategies when evaluating explanations ensures a comprehensive assessment of the model’s robustness and generalizability across different scenarios. This practice, in turn, aids in the development of a reliable and well-rounded evaluation checklist."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#human-knowledge-in-evaluating-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#human-knowledge-in-evaluating-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Human Knowledge in Evaluating Explanations",
    "text": "Human Knowledge in Evaluating Explanations\nAccounting for human knowledge is crucial when assessing explanations generated by deep learning models. Key factors influencing the evaluation process include:\n\nFamiliarity with machine learning: Understanding basic concepts in machine learning and deep learning can impact how an evaluator interprets model explanations.\nDomain-specific knowledge: Possessing expertise in the field or subject area can facilitate deeper insights into the explanations and their relevance to domain-specific problems.\nPrevious experience: An evaluator with prior experience assessing model explanations often has a more refined understanding of what constitutes a good explanation.\nSubject knowledge: A strong grasp of the subjects or concepts related to the task at hand influences the ability to evaluate the validity and coherence of explanations.\n\nEngaging annotators with various expertise levels can lead to a broader understanding of the interpretability and accessibility of explanations. This diversity ensures that the evaluation process caters to different audiences, resulting in more accurate and inclusive assessments of deep learning models’ explanatory proficiency."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#model-information-known-priming-in-evaluating-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#model-information-known-priming-in-evaluating-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Model Information Known (Priming) in Evaluating Explanations",
    "text": "Model Information Known (Priming) in Evaluating Explanations\nPriming, which involves sharing model information with evaluators, can significantly impact the evaluation process. By understanding how different levels of model information influence the perception and utility of explanations, researchers can establish a balanced assessment. Here are three priming variations to consider:\n\nNo information: Withholding all model-related information from evaluators, which allows assessing explanations solely on their comprehensibility and informativeness without any bias or preconceived notions.\nRepresentative information: Providing evaluators with a basic understanding of the model, including its architecture, training method, and general performance. This approach helps evaluators contextualize the explanations and gain a general sense of the model’s capabilities, while minimizing the risk of priming-induced bias.\nAsking for information: Encouraging evaluators to request specific model information as needed during the evaluation process. This setup lets evaluators actively seek out details that can help them better understand or interpret the explanations, thereby promoting a more personalized and targeted evaluation experience.\n\nIncorporating these priming variations in the evaluation process allows researchers to determine the impact of prior model knowledge on the perception and usefulness of explanations. This information can further guide the development of a standardized checklist for evaluating deep learning model explanations more effectively.\nI apologize if my previous response did not fully align with Mimansa Jaiswal’s writing style. I will make a further attempt to refine the content while focusing on remaining concise and avoiding flowery language. Here is a revised version:"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#evaluating-explanation-methods-in-deep-learning-models",
    "href": "notes/controlled_evaluation_of_explanations.html#evaluating-explanation-methods-in-deep-learning-models",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Evaluating Explanation Methods in Deep Learning Models",
    "text": "Evaluating Explanation Methods in Deep Learning Models\nThe choice of explanation method is essential for understanding deep learning model behavior. Assessing various methods helps determine their effectiveness in conveying relevant information. The following is an overview of these methods:\n\nHighlight-based: Emphasizes crucial regions or input data components contributing to model decisions, facilitating focus on key factors. Examples include saliency maps and attention mechanisms.\nLanguage-based: Uses natural language to describe influences on the model’s predictions or decision-making processes—benefits users with limited domain knowledge by offering understandable explanations.\nDrag/drop-based: Provides an interactive experience, enabling evaluators to manipulate input data components and observe real-time model decision changes, deepening insights into the model’s workings.\nInfluence functions: Estimates each training instance’s impact on model predictions or decisions for specific test points, helping evaluators grasp the model’s generalization behavior and identify influential data points.\n\nBy examining different explanation methods in various scenarios, researchers can identify which methods yield meaningful insights into model behavior. This understanding can guide the development of improved evaluation checklists and more effective explanation techniques.\nI apologize for that oversight, and I appreciate your feedback. Here is the revised paragraph without the “By XYZ” ending structure:"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#characteristics-of-explanations",
    "href": "notes/controlled_evaluation_of_explanations.html#characteristics-of-explanations",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Characteristics of Explanations",
    "text": "Characteristics of Explanations\nAssessing explanation characteristics is crucial for evaluating their efficacy in conveying information about deep learning model behavior. These characteristics affect comprehensibility and trustworthiness of explanations. The following are key characteristics to evaluate:\n\nLength: Explanations should be concise yet informative. Overly short explanations might lack detail, whereas lengthy explanations risk becoming overwhelming or complex.\nReading grade level: Evaluating readability is vital for ensuring information accessibility. A suitable reading grade level ensures content clarity for users with diverse domain expertise.\nOutput confidence: Sharing the model’s output confidence enhances explanation trustworthiness. Users can better assess the model’s prediction certainty and explanation reliability.\nCiting references: Incorporating relevant sources, context, or prior knowledge bolsters explanation credibility. Cited sources allow users to validate presented information, fostering a deeper understanding of the model’s rationale.\n\nTaking these characteristics into account helps researchers pinpoint potential improvements needed for better comprehensibility and trustworthiness in explanations. This refined understanding aids in the development of more effective evaluation checklists and explanation techniques for deep learning models."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#demographic-factors",
    "href": "notes/controlled_evaluation_of_explanations.html#demographic-factors",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Demographic Factors",
    "text": "Demographic Factors\nIn developing explanations for deep learning models, demographic factors such as culture, age, and gender play a crucial role in shaping users’ perception and understanding. Addressing these factors ensures that explanations cater to diverse user populations.\n\nLanguage: Create explanations in multiple languages and consider cultural sensitivities to overcome language barriers. Adopt plain language principles to enhance comprehensibility for various language proficiencies.\nAge: Adapt explanations to different age groups by using age-appropriate terminology and complexity levels.\nGender: Employ gender-neutral language in explanations to uphold inclusivity and avoid alienating users based on gender identity.\nCultural context: Acknowledge cultural nuances and values within the target user population to craft relatable, contextually relevant explanations.\n\nIncorporating demographic factors in explanation development can enhance accessibility and inclusivity, promoting broader understanding and adoption of AI models throughout diverse demographic groups."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#explanation-type",
    "href": "notes/controlled_evaluation_of_explanations.html#explanation-type",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Explanation Type",
    "text": "Explanation Type\nUnderstanding the nature of explanation types—faithful, reliable, and plausible—is vital when evaluating explanations. Examining these categories enables insights into the model’s capacity for delivering explanations that meet user expectations and needs:\n\nFaithful explanations: These accurately reflect the model’s inner workings and decision-making processes. Assessing faithful explanations helps determine the model’s transparency, promoting confidence in its behavior and outcomes.\nReliable explanations: Consistently providing useful and relevant information, reliable explanations hold up under diverse circumstances. Evaluating this aspect enables the identification of the model’s robustness and applicability across varying scenarios.\nPlausible explanations: Appearing coherent and sensible, plausible explanations adhere to human intuition and domain knowledge. This evaluation aspect sheds light on the model’s ability to generate user-friendly insights that align with human reasoning.\n\nThrough the critical examination of these explanation types, we can better understand a model’s strengths and weaknesses in delivering explanations that satisfy users’ expectations and requirements. This knowledge subsequently informs the development of enhanced evaluation procedures and improved explanation techniques for deep learning models."
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html#task",
    "href": "notes/controlled_evaluation_of_explanations.html#task",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "Task",
    "text": "Task\nThe type of task under evaluation, including natural language inference, sentiment analysis, or domain-specific tasks such as SOAP (Subjective, Objective, Assessment, and Plan), plays a substantial role in the evaluation process. Grasping the nuances of a particular task and its inherent challenges enables more insightful assessment of explanations:\n\nNatural Language Inference: Necessitates evaluations that focus on determining the model’s ability to reason about relationships between sentences, like entailment, contradiction, or neutrality. This ensures the model effectively captures semantic aspects.\nSentiment Analysis: Requires emphasis on gauging the model’s understanding of sentiment polarity in various expressions, capturing subtle emotional nuances and potential ambiguities.\nSOAP (Subjective, Objective, Assessment, and Plan): When dealing with domain-specific tasks like SOAP, evaluations should be grounded in an in-depth understanding of the specific domain and terminology, as well as the associated reasoning processes, to ensure meaningful and coherent explanations.\n\nRecognizing the characteristics of the task at hand is essential for conducting insightful evaluations of explanations. It allows for the development of targeted evaluation methodologies that cater to the specific challenges and requirements associated with each task type."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "Research Publications\n\n\n\n         \n            \n               Sep 2023\n            \n            \n               Interspeech\n            \n            \n               \n                  Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder\n               \n               In\n                     Interspeech,\n                        Sep 2023\n                  \n               \n               \n                  Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Metric Design\n                        \n                  \n                     \n                        \n                           Emotion is expressed through language, vocal and facial expressions. Lack of emotional alignment between modalities is a symptom of mental disorders. We propose to quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional Mismatch (EMM). EMM patterns differ between symptomatic and euthymic moods. EMM statistics serve as an effective feature for mood recognition, reducing annotation cost while preserving mood identification.\n                        \n                        \n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Design\n                        \n                  \n                     \n                        \n                           Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                        \n                        \n                           Note\n                           Demo\n                           Repo\n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Foundation Models\n                        \n                  \n                     \n                        \n                           Prompt-based language models introduce uncertainty to classification and require users to try multiple prompts with varying temperatures to find the best fit. However, this approach lacks the ability to capture implicit differences in prompts and provide adequate vocabulary. To address this, a text annotation framework is proposed to provide a structured approach to prompt definition and annotation. Better validation structures and structured prompts are necessary for using prompt-based systems at scale for labeling or retrieval.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               Nov 2022\n            \n            \n               arXiv\n            \n            \n               \n                  Human-Centered Metric Design to Promote Generalizable and Debiased Emotion Recognition\n               \n               In\n                     arXiv,\n                        Nov 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Debiasing\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Generalization\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Metrics for emotion recognition can be challenging due to their dependence on subjective human perception. This paper proposes a template formulation that derives human-centered, automatic, optimizable evaluation metrics for emotion recognition models. The template uses model explanations and sociolinguistic wordlists and can be applied to a sample or whole dataset. The proposed metrics include generalizability and debiasing improvement, and are tested on three models, datasets and sensitive variables. The metrics correlate with the models' performance and biased representations, and can be used to train models with increased generalizability, decreased bias, or both. The template is the first to provide quantifiable metrics for training and evaluating generalizability and bias in emotion recognition models.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2022\n            \n            \n               Interspeech\n            \n            \n               \n                  Mind the Gap: On the Value of Silence Representations to Lexical-Based Speech Emotion Recognition\n               \n               In\n                     Interspeech,\n                        Sep 2022\n                  \n               \n               \n                  Matthew Perez, Mimansa Jaiswal, Minxue Niu, Cristina Gorrostieta, Matthew Roddy, Kye Taylor, Reza Lotfian, John Kane, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Model Training\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Silence is crucial in speech perception, conveying emphasis and emotion. However, little research has been done on the effect of silence on linguistics and emotion recognition. We present a novel framework that fuses linguistic and silence representations for emotion recognition in naturalistic speech. Two methods to represent silence are investigated, with results showing improved performance. Modeling silence as a token in a transformer language model significantly improves performance on the MSP-Podcast dataset. Analyses show that silence emphasizes the attention of its surrounding words.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n\n\nNo matching items\n\n\n\n\nResearch Notes\n\n\n\n\n  \n    \n      \n        \n        \n            \n            Research Notes\n            Long Form notes\n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Missing Missing Evaluations\n            The Curious Case of LLM Evaluations\n            Our rapid scaling, and generalization techniques has outpaced our ability to effectively evaluate and accurately assess their capabilities. Without robust tools to measure and gauge a model's performance, we risk overestimating its abilities. It is important to recognize that while a model may appear exceptional in certain tasks, our evaluation methods may be limited and biased. Simply conducting races on predictable paths with consistent cues can create an illusion of invincibility for the model.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Opinion\n                  \n                    Foundation Models\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Foundation Models' Gender Biases\n            Demonstrating How Gender Bias Creeps into NLP Models\n            Gender biases present in Machine Learning Models are pretty difficult to identify. GPT4, as illustrated, considers the female partner to be the problem in a disagreement surrounding crying. Gendered translations and ambiguous pronoun references add to the problem and reinforce existing biases.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                    Debiasing\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            ToM Evaluation Heuristics\n            A Framework for Evaluating Theory of Mind through Structured Testing\n            The proposed framework for evaluating theory of mind emphasizes user instinct derivation and creation of evaluation problems that reflect this instinct. Irrespective of the test, it comprises three key steps - instinct derivation, prompt modification and process evaluation - that together lead to more structured and effective evaluation methods for testing theory of mind.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            CAPSTONE for LLMs\n            CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n            Prompt-based language models have limitations in classification and often require users to test multiple prompts with varying temperatures to identify the best fit. A text annotation framework addresses this by introducing explicit prompt definition and validation, and can improve performance in labeling or retrieval tasks at scale.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                \n        \n    \n  \n\n\n\n\nNo matching items\n\n\n\n\nBlurbs\n\n\n\n  \n    \n        05-07-2023\n      \n      \n      \n        Livable Stipends for PhD Students Shouldn't Be Up for Debate\n      \n      \n        A response to the controversial idea that PhD students should barely scrape by in emergencies and rely on Blue Cupboard to feed themselves.\n      \n      \n        I recently received some criticism for my comments over the past couple of days, which seemed to justify the idea that PhD students don't need livable stipends. Let me make myself perfectly clear: that's bullshit. No one should have to choose between paying bills and getting essential medical care or feeding their pets. If you're one of those people who thinks barely scraping by is okay for PhD students, we don't see eye to eye.\n        \n        \n      \n      \n      \n        Finance\n        PhD\n        Academia\n        Opinion\n        Twitter\n        \n    \n\n\n\nNo matching items\n\n\n\n\nUpdates\n\n\n    \n    May 2023Defended my PhD titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    May 2022I was selected as one of the 8 Barbour fellows for 2022-2023 across all of UMich\n    Nov 2021Defended my PhD proposal titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    Oct 2021I will be presenting our work on sociolinguistic inspired privacy evaluation at text as data (TADA) conference 2021, UMich, Ann Arbor.\n    Sep 2021I am interning this Fall in Allen AI with Ana Marasović in the area of evaluation and interpretability.\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "stream/index.html",
    "href": "stream/index.html",
    "title": "Blurbs",
    "section": "",
    "text": "Blurbs is where I keep my random musings, quick reviews, and noteworthy quotes all in one place. It’s convenient to have a curated collection instead of them being scattered throughout my feed. It’s a fun little side space to gather everything. \n\n\n\n\n\n\n\n  \n    \n        05-07-2023\n      \n      \n      \n        Livable Stipends for PhD Students Shouldn't Be Up for Debate\n      \n      \n        A response to the controversial idea that PhD students should barely scrape by in emergencies and rely on Blue Cupboard to feed themselves.\n      \n      \n        I recently received some criticism for my comments over the past couple of days, which seemed to justify the idea that PhD students don't need livable stipends. Let me make myself perfectly clear: that's bullshit. No one should have to choose between paying bills and getting essential medical care or feeding their pets. If you're one of those people who thinks barely scraping by is okay for PhD students, we don't see eye to eye.\n        \n        \n      \n      \n      \n        Finance\n        PhD\n        Academia\n        Opinion\n        Twitter\n        \n    \n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "single_page/papers.html",
    "href": "single_page/papers.html",
    "title": "Research Publications",
    "section": "",
    "text": "Sep 2023\n            \n            \n               Interspeech\n            \n            \n               \n                  Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder\n               \n               In\n                     Interspeech,\n                        Sep 2023\n                  \n               \n               \n                  Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Metric Design\n                        \n                  \n                     \n                        \n                           Emotion is expressed through language, vocal and facial expressions. Lack of emotional alignment between modalities is a symptom of mental disorders. We propose to quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional Mismatch (EMM). EMM patterns differ between symptomatic and euthymic moods. EMM statistics serve as an effective feature for mood recognition, reducing annotation cost while preserving mood identification.\n                        \n                        \n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Design\n                        \n                  \n                     \n                        \n                           Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                        \n                        \n                           Note\n                           Demo\n                           Repo\n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Foundation Models\n                        \n                  \n                     \n                        \n                           Prompt-based language models introduce uncertainty to classification and require users to try multiple prompts with varying temperatures to find the best fit. However, this approach lacks the ability to capture implicit differences in prompts and provide adequate vocabulary. To address this, a text annotation framework is proposed to provide a structured approach to prompt definition and annotation. Better validation structures and structured prompts are necessary for using prompt-based systems at scale for labeling or retrieval.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               Nov 2022\n            \n            \n               arXiv\n            \n            \n               \n                  Human-Centered Metric Design to Promote Generalizable and Debiased Emotion Recognition\n               \n               In\n                     arXiv,\n                        Nov 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Debiasing\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Generalization\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Metrics for emotion recognition can be challenging due to their dependence on subjective human perception. This paper proposes a template formulation that derives human-centered, automatic, optimizable evaluation metrics for emotion recognition models. The template uses model explanations and sociolinguistic wordlists and can be applied to a sample or whole dataset. The proposed metrics include generalizability and debiasing improvement, and are tested on three models, datasets and sensitive variables. The metrics correlate with the models' performance and biased representations, and can be used to train models with increased generalizability, decreased bias, or both. The template is the first to provide quantifiable metrics for training and evaluating generalizability and bias in emotion recognition models.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2022\n            \n            \n               Interspeech\n            \n            \n               \n                  Mind the Gap: On the Value of Silence Representations to Lexical-Based Speech Emotion Recognition\n               \n               In\n                     Interspeech,\n                        Sep 2022\n                  \n               \n               \n                  Matthew Perez, Mimansa Jaiswal, Minxue Niu, Cristina Gorrostieta, Matthew Roddy, Kye Taylor, Reza Lotfian, John Kane, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Model Training\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Silence is crucial in speech perception, conveying emphasis and emotion. However, little research has been done on the effect of silence on linguistics and emotion recognition. We present a novel framework that fuses linguistic and silence representations for emotion recognition in naturalistic speech. Two methods to represent silence are investigated, with results showing improved performance. Modeling silence as a token in a transformer language model significantly improves performance on the MSP-Podcast dataset. Analyses show that silence emphasizes the attention of its surrounding words.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Mar 2022\n            \n            \n               Submission\n            \n            \n               \n                  Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?\n               \n               In\n                     Submission,\n                        Mar 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Minxue Niu, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                  \n                     \n                        \n                           Factors affecting explanation efficacy include the algorithm used and the end user. NLP papers focus on algorithms for generating explanations, but overlook other factors. This paper examines how saliency-based explanation methods for machine learning models change with controlled variables. We aim to provide a standardized list of variables to evaluate these explanations and show how SoTA algorithms can have different rankings when controlling for evaluation criteria.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               2020\n            \n            \n               ACL-SRW\n            \n            \n               \n                  Noise-Based Augmentation Techniques for Emotion Datasets: What Do We Recommend?\n               \n               In\n                     ACL-SRW,\n                        2020\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Data Augmentation\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                  \n                     \n                        \n                           Multiple noise-based data augmentation approaches have been proposed to counteract this challenge in other speech domains. But, unlike speech recognition and speaker verification, the underlying label of emotion data may change given the addition of noise. In this work, we propose a set of recommendations for noise-based augmentation of emotion datasets based on human and machine performance evaluation of generated realistic noisy samples using multiple categories of environmental and synthetic noise.\n                        \n                        \n                           PDF\n                           Talk\n                        \n                     \n            \n         \n         \n            \n               May 2020\n            \n            \n               LREC\n            \n            \n               \n                  MuSE: Multimodal Stressed Emotion Dataset\n               \n               In\n                     LREC,\n                        May 2020\n                  \n               \n               \n                  Mimansa Jaiswal, Cristian-Paul Bara, Yuanhang Luo, Rada Mihalcea, Mihai Burzo, Emily Mower Provost\n               \n                  \n                           \n                           Data Collection\n                        \n                           \n                           Confounding Factors\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                  \n                     \n                        \n                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Feb 2020\n            \n            \n               AAAI and NeuRIPS-W\n            \n            \n               \n                  Privacy Enhanced Multimodal Neural Representations for Emotion Recognition\n               \n               In\n                     AAAI and NeuRIPS-W,\n                        Feb 2020\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Confounding Factors\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                  \n                     \n                        \n                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2019\n            \n            \n               Interspeech\n            \n            \n               \n                  Identifying Mood Episodes Using Dialogue Features from Clinical Interviews\n               \n               In\n                     Interspeech,\n                        Sep 2019\n                  \n               \n               \n                  Zakaria Aldeneh, Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Dialogue\n                        \n                  \n                     \n                        \n                           Mental health professionals assess symptom severity through semi-structured clinical interviews. During these interviews, they observe their patients’ spoken behaviors, including both what the patients say and how they say it. In this work, we move beyond acoustic and lexical information, investigating how higher-level interactive patterns also change during mood episodes.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               May 2019\n            \n            \n               ICASSP\n            \n            \n               \n                  MuSE-ing on the Impact of Utterance Ordering on Crowdsourced Emotion Annotations\n               \n               In\n                     ICASSP,\n                        May 2019\n                  \n               \n               \n                  Mimansa Jaiswal, Zakaria Aldeneh, Cristian-Paul Bara, Yuanhang Luo, Mihai Burzo, Rada Mihalcea, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Crowdsourcing\n                        \n                  \n                     \n                        \n                           Emotion expression and perception are inherently subjective. There is generally not a single annotation that can be unambiguously declared “correct.” As a result, annotations are colored by the manner in which they were collected, i.e., with or without context.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2018\n            \n            \n               Interspeech\n            \n            \n               \n                  The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild\n               \n               In\n                     Interspeech,\n                        Sep 2018\n                  \n               \n               \n                  Soheil Khorram, Mimansa Jaiswal, John Gideon, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Model Training\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Mental Health\n                        \n                  \n                     \n                        \n                           This paper presents critical steps in developing this pipeline, including (a) a new in the wild emotion dataset, the PRIORI Emotion Dataset, (b) activation/valence emotion recognition baselines, and, (c) establish emotion as a meta-feature for mood state monitoring.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Dec 2017\n            \n            \n               FLAIRS\n            \n            \n               \n                  'Hang in there': Lexical and Visual Analysis to Identify Posts Warranting Empathetic Responses\n               \n               In\n                     FLAIRS,\n                        Dec 2017\n                  \n               \n               \n                  Mimansa Jaiswal, Sairam Tabibu, Erik Cambria\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                  \n                     \n                        \n                           Saying \"You deserved it!\" to \"I failed the test\" is not a good idea. In this paper, we propose a method supported by hand-crafted features to judge if the discourse or statement requires an empathetic response.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Jul 2017\n            \n            \n               ICDM-W\n            \n            \n               \n                  'The Truth and Nothing But The Truth': Multimodal Analysis for Deception Detection\n               \n               In\n                     ICDM-W,\n                        Jul 2017\n                  \n               \n               \n                  Mimansa Jaiswal, Sairam Tabibu, Rajiv Bajpai\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Multimodal\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                  \n                     \n                        \n                           We propose a data-driven method (SVMs) for automatic deception detection in real-life trial data using visual (OpenFace) and verbal cues (Bag of Words).\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mimansa Jaiswal",
    "section": "",
    "text": "I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design."
  },
  {
    "objectID": "index.html#hi-im-mimansa",
    "href": "index.html#hi-im-mimansa",
    "title": "Mimansa Jaiswal",
    "section": "Hi, I’m Mimansa",
    "text": "Hi, I’m Mimansa\nI’m Mimansa, a final year PhD candidate in Computer Science (AI/Interactive Systems) at the University of Michigan.\nI am fortunate to be working with Prof. Emily Provost as part of the CHAI group. I completed my undergrad in Computer Engineering from Institute of Engineering and Technology, Indore in 2017, and worked with Prof. G.L. Prajapati for my bachelor’s thesis.\nTl;dr: I work in the area of developing interpretable and human imitating evaluation and cost-effective data collection procedures.\nOther than research, I am interested in science communication, sketchnoting, personal knowledge management and cooking. One of my favorite activities during international conferences is to plan every single meal.\nI love cats, and they keep me sane. I have two: Oreo and Bert (yes, that Bert, you read it right!)."
  },
  {
    "objectID": "banner.html",
    "href": "banner.html",
    "title": "Mimansa Jaiswal",
    "section": "",
    "text": "I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.\n\n\n\n Back to top"
  },
  {
    "objectID": "single_page/updates.html",
    "href": "single_page/updates.html",
    "title": "Updates",
    "section": "",
    "text": "May 2023Defended my PhD titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    May 2022I was selected as one of the 8 Barbour fellows for 2022-2023 across all of UMich\n    Nov 2021Defended my PhD proposal titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    Oct 2021I will be presenting our work on sociolinguistic inspired privacy evaluation at text as data (TADA) conference 2021, UMich, Ann Arbor.\n    Sep 2021I am interning this Fall in Allen AI with Ana Marasović in the area of evaluation and interpretability.\n    May 2021Excited to be interning in the FAIR NLP group this summer and looking forward to work at the intersection of Linguistics and ML.\n    Sep 2020I have been chosen as the student representative for Faculty Hiring at my university for the 2020-21 season. Looking forward to knowing and communicating the 'usually hidden' processes.\n    May 2020Excited to be interning in the Conversation AI group at Facebook AI working on automated dialogue evaluation.\n    Dec 2019I'll be attending NeuRIPS. Shoot me an email if you want to meet and talk about anything!\n    Jan 2019I'll be a teaching assistant for the course on Applied Machine Learning for Affective Computing with my advisor. So excited to be teaching for the first time!\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "stream/phd_stipends.html",
    "href": "stream/phd_stipends.html",
    "title": "Livable Stipends for PhD Students Shouldn’t Be Up for Debate",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "about_me/research_interests.html",
    "href": "about_me/research_interests.html",
    "title": "Mimansa Jaiswal",
    "section": "",
    "text": "Errors\n        \n      \n    \n    \n      \n        \n          Metrics\n        \n      \n    \n    \n      \n        \n          XAI\n        \n      \n    \n    \n      \n        \n          Human⇔AI\n        \n      \n    \n        \n          METHODS\n        \n      \n    \n  \n\n\n  \n    \n      \n        \n          Design\n        \n      \n    \n    \n      \n        \n          Dialogue\n        \n      \n    \n    \n      \n        \n          Health\n        \n      \n    \n    \n      \n        \n          Productivity\n        \n      \n      \n        \n          AREAS\n        \n      \n    \n  \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Research Notes",
    "section": "",
    "text": "Missing Missing Evaluations\n            The Curious Case of LLM Evaluations\n            Our rapid scaling, and generalization techniques has outpaced our ability to effectively evaluate and accurately assess their capabilities. Without robust tools to measure and gauge a model's performance, we risk overestimating its abilities. It is important to recognize that while a model may appear exceptional in certain tasks, our evaluation methods may be limited and biased. Simply conducting races on predictable paths with consistent cues can create an illusion of invincibility for the model.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Opinion\n                  \n                    Foundation Models\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Foundation Models' Gender Biases\n            Demonstrating How Gender Bias Creeps into NLP Models\n            Gender biases present in Machine Learning Models are pretty difficult to identify. GPT4, as illustrated, considers the female partner to be the problem in a disagreement surrounding crying. Gendered translations and ambiguous pronoun references add to the problem and reinforce existing biases.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                    Debiasing\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            ToM Evaluation Heuristics\n            A Framework for Evaluating Theory of Mind through Structured Testing\n            The proposed framework for evaluating theory of mind emphasizes user instinct derivation and creation of evaluation problems that reflect this instinct. Irrespective of the test, it comprises three key steps - instinct derivation, prompt modification and process evaluation - that together lead to more structured and effective evaluation methods for testing theory of mind.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            CAPSTONE for LLMs\n            CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n            Prompt-based language models have limitations in classification and often require users to test multiple prompts with varying temperatures to identify the best fit. A text annotation framework addresses this by introducing explicit prompt definition and validation, and can improve performance in labeling or retrieval tasks at scale.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Text Annotation Interface Design\n            Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n            Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                \n                  \n                    Text\n                  \n                    Data Annotation\n                  \n                    Design\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Standardized Explanation Evaluation\n            Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?\n            End users affect explanation efficacy. NLP papers overlook other factors. This paper examines how saliency-based explanations' utiltiy change with controlled variables. We aim to provide a standardized list of variables to evaluate and show how SoTA algorithms rank differently when controlling for evaluation criteria.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                \n        \n    \n  \n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "notes/gender_biases_in_llms.html",
    "href": "notes/gender_biases_in_llms.html",
    "title": "Demonstrating How Gender Bias Creeps into NLP Models",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/capstone.html",
    "href": "notes/capstone.html",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "",
    "text": "\"LLMs\", or Language Models, need to be trained and tested on different datasets and prompts to improve functionality and accuracy. In addition, versioning is important when assessing LLMs. Iterating on the prompts and datasets can produce better results. However, keeping different versions of both prompts and datasets can help identify which changes led to improvements in performance. All these practices should be followed to effectively evaluate and improve LLMs."
  },
  {
    "objectID": "notes/capstone.html#prompting-closed-llms",
    "href": "notes/capstone.html#prompting-closed-llms",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Prompting Closed LLMs",
    "text": "Prompting Closed LLMs\n\n\n\n\n\nWhen we are limited to observations, it becomes essential to employ experimental design in order to assess the capabilities of a model.\nIn order to do effectively, there are three key components we require:\n\nFirstly, we need to establish controls, which serve as the baseline for comparison and help us understand the impact various factors on the model's performance.\nAdditionally, conducting experiments allows us to systematically manipulate variables and observe their effects on the model's outcomes.\nLastly, carefully considering and accounting for variables is crucial in order to ensure the validity and reliability of our experimental results. By incorporating these elements into our approach, we can effectively evaluate and judge the capabilities of a model limited."
  },
  {
    "objectID": "notes/capstone.html#prompt-based-experimental-design-schemas",
    "href": "notes/capstone.html#prompt-based-experimental-design-schemas",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Prompt Based Experimental Design Schemas",
    "text": "Prompt Based Experimental Design Schemas\n\n\n\n\n\n\n\nMichael Frank talks about need some principles from experimental psychology\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet's talk about Prompt Based Experimental Design Schemas. These schemas can be thought of as templates, but with a key distinction. They not only focus on varying the input sample, but also on modifying the prompt specifications. By doing so, they allow us to closely monitor and analyze the changes that occur from one prompt to another. This approach provides a valuable way to study and understand the impact of different prompts on the overall outcome of an experiment."
  },
  {
    "objectID": "notes/capstone.html#prompt-definition-markup",
    "href": "notes/capstone.html#prompt-definition-markup",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Prompt Definition Markup",
    "text": "Prompt Definition Markup\nLet's delve into the concept of Prompt Definition Markup. Within this framework, we identify three types of steering in a prompt: start, process, and output.\nEach type serves a distinct purpose. Additionally, an output steer consists of two components: instructions (including constraints and format) and options.\n\n\n\n\n\nTo illustrate, let's consider the \"Sparks of AGI\" paper, which utilizes only a start steer, indicating the initial direction or guidance provided in the prompt.\n\n\n\n\n\nThis approach allows for a clear and structured definition of prompts, enabling effective experimentation and analysis.\nTo enhance the prompt, we can introduce a process steer in addition to the start steer, encouraging a step-by-step approach. Furthermore, we can incorporate an output instruction steer, specifying that the response should only include the folder path and no additional information.\nThese modifications provide additional guidance and constraints to the prompt, allowing for a more structured and controlled experimental design."
  },
  {
    "objectID": "notes/capstone.html#sample-markup",
    "href": "notes/capstone.html#sample-markup",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Sample Markup",
    "text": "Sample Markup\nLet's now turn our attention to Sample Markup. A sample consists of two key components: examples and input. However, if our objective is zero-shot prompting, we may not require specific examples.\nIn cases where examples are included, they can be categorized as either positive (A1) or negative (A2) examples, providing different instances for the model to learn from. On the other hand, the input component comprises three parts: the sample itself (B1), a question (B2), and a concept (B3). These elements collectively shape the input provided to the model, allowing for a more targeted and contextualized prompt.\n\n\n\n\n\nLet's consider an example to illustrate the concept of Sample Markup. In the context of the Theory of Mind False Belief Test, there are no specific examples provided. Instead, the prompt consists of a sample, a question, and an implicit concept. We can annotate this prompt accordingly, as depicted in the accompanying picture."
  },
  {
    "objectID": "notes/capstone.html#output-schema",
    "href": "notes/capstone.html#output-schema",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Output Schema",
    "text": "Output Schema\nThe output schema is a complex process that intersects significantly with large language model (LLM) evaluation techniques. If you're interested in delving deeper into this topic, I recommend checking out my other post. It provides further insights and information about the intricacies involved in designing and evaluating output schemas for LLMs."
  },
  {
    "objectID": "notes/capstone.html#metadata",
    "href": "notes/capstone.html#metadata",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Metadata",
    "text": "Metadata\nLet's now discuss the final component of the proposed schema: Metadata. Metadata plays a crucial role in shaping the overall prompt-based experimental design. It consists of five key parts that contribute to the effectiveness and coherence of the experiment:\nA. Connectors\nB. Iteration\nC. Variation\nD. Intuition\nE. Experimental Design\n\n\n\n\n\n\nConnectors\nLet's delve into the concept of Connectors within the prompt-based experimental design schema. Connectors serve as internal elements within a single prompt and play a crucial role in shaping the prompt's context and implications. They can serve different purposes:\nA1. Addition: Connectors can add information that reinforces the belief or statement presented in the prompt. This additional information further strengthens the intended message or concept.\nA2. Negation: On the other hand, connectors can also negate certain information presented in the prompt. By negating specific details, the prompt can introduce contrasting or alternative perspectives.\nA3. Exception: Connectors may also be used to highlight exceptions to the information being tested or present in the prompt. These exceptions provide additional nuances and complexities to the prompt, allowing for a more comprehensive exploration of the given scenario.\nFor instance, consider the following example: \"He says nothing about this to Alice, and Dropbox also does not notify Alice.\" In this case, the connector reinforces the belief or statement being made, emphasizing that both the person mentioned and Dropbox do not inform Alice about a particular matter. This connector strengthens the prompt's intended message and adds clarity to the scenario being presented.\n\n\n\n\n\n\n\nIteration\nIteration involves making changes to the prompt itself, without modifying the sample or the desired output. It allows for refining and improving the prompt to enhance the experimental design and guide the model's response.\nThere are different ways in which iteration can be implemented:\nB1. Multi-instruction: This involves adding multiple instructions within the prompt, providing additional guidance and directions to the model. These instructions help shape the model's understanding and guide its response in a more specific manner.\nB2. Rewording: Rewording the prompt entails changing the phrasing or wording of the prompt while maintaining the same underlying concept. This can be done to clarify the prompt, emphasize certain aspects, or provide a different perspective for the model to consider.\nB3. Chaining: Chaining refers to linking multiple prompts together in a sequential manner. Each prompt builds upon the previous one, creating a chain of prompts that guide the model's thought process and response. This approach allows for a step-by-step exploration of the given scenario or concept.\nFor example, adding the phrase \"Let's think about this step by step\" to the prompt can be considered an iteration aimed at incorporating a \"Process steer.\" This addition provides a clearer instruction to the model, encouraging a systematic and sequential approach in its response.\n\n\nIntuition\nIntuition refers to the reasoning or underlying purpose behind the prompt. It represents the intention or objective that the researchers aim to achieve through the prompt.\nWe can categorize intuition into three types:\nC1. Implicit: Implicit intuition refers to the underlying concept or idea that is implicitly conveyed through the prompt. It represents the broader theme or topic that the prompt is designed to explore or address.\nC2. Explicit: Explicit intuition involves explicitly stating the purpose or intention behind the prompt. It provides a clear and direct indication of the specific aspect or perspective that the prompt aims to capture or investigate.\nC3. Test: Test intuition refers to the specific test or evaluation being conducted through the prompt. It highlights the particular assessment or examination that the prompt is designed to facilitate.\nFor example, let's consider a Theory of Mind paper. We can discretize the intuition as follows:\nC1. Implicit (Theory of Mind): The implicit intuition of the prompt revolves around the exploration of the Theory of Mind concept, which involves understanding and analyzing how individuals perceive and interpret the thoughts, beliefs, and intentions of others.\nC2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service): The explicit intuition of the prompt focuses on the concept of modernization and its impact on individuals' access to unseen photos due to online services. It highlights the specific aspect of modernization and its influence on personal experiences.\nC3. Test (False Belief Test): The test intuition of the prompt centers around conducting a False Belief Test, which aims to assess individuals' understanding of false beliefs and their ability to attribute different perspectives to others.\n\n\n\n\n\n\n\nVariation\nNow let's shift our attention to Variations within the prompt-based experimental design schema. Variations occur across different input samples and play a crucial role in shaping the experimental design. They allow for the exploration of diverse scenarios and perspectives, ensuring a comprehensive analysis of the model's capabilities.\nWe can categorize variations into different aspects:\nD1. Output Specification: This aspect focuses on the desired output of the prompt. It can be categorized as generative (Gen.) or discriminative, depending on whether the prompt aims to generate new content or make a judgment or discrimination based on the input.\nD2. Concept: Conceptual variations involve different concepts or ideas presented in the prompt. These concepts can be similar, opposite, or serve as control variables, providing a range of scenarios for the model to process and respond to.\nD3. Task: Task variations relate to the specific task or objective of the prompt. This aspect can involve exploring the subjectivity or objectivity of the prompt, allowing for different perspectives and evaluation criteria.\nIt is crucial to consider variations because they contribute to a comprehensive assessment of the model's capabilities. Without a comprehensive enough variation set, it becomes challenging to make accurate judgments regarding the model's performance and behavior.\nFor example, let's consider the following prompt:\nC2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service)\nwith D2. Similar Concept (Unseen Object 'Cause Sleeping)\nIn this case, the explicit intuition of the prompt revolves around the concept of modernization and its impact on individuals' access to unseen photos due to online services. The variation in the concept introduces the idea of an unseen object causing sleeping, adding a similar yet distinct scenario for the model to process and respond to.\nPrevious Example\nTask 1: Math (BASIC Subtraction) + Implicit \"No knowledge transfer through sleeping\" + External actor (Action taken by someone else and not self)\n\nBoth GPT4 and GPT3.5 fail on this simple task\n\nSomehow sleep is not encoded as no knowledge transfer?\n\n\n\n\n\n\n\nExperimental Design\nThe experimental design encompasses the basics of the experiment and the corresponding prompt, providing a structured framework for conducting the evaluation.\nWe can define different components within the experimental design:\nE1. Control: A control prompt serves as a baseline for comparison. It can be a prompt that compares the model's performance to a base model or a different kind of model, where it is known, to the best of our knowledge, that the model does not exhibit certain capabilities. For example, if we can confidently say that GPT3.5 does not \"pass\" the false-belief test, it can serve as a control prompt.\nE2. Previous Prompt: The previous prompt serves as a reference point for the current experiment. It allows us to note the differences made in the current prompt and annotate those differences for iterations and concepts. It helps in tracking the evolution and progress of the prompt design.\nE2a. Prompt: The prompt itself is a crucial part of the experimental design. It includes the specific instructions, information, and context provided to the model to generate a response.\nE2b. Directionality: Directionality refers to the specific direction or focus of the variables compared to the previous prompt. It includes aspects such as leakage, ambiguity, specificity, and coverage. These variables aim to reduce ambiguity, enhance specificity, and provide better coverage in the prompt design.\nE3. Date: The date component specifies the time or period during which the experiment is conducted. It helps in documenting and tracking the experimental timeline.\nE4. Model: The model component specifies the particular model being used in the experiment. It helps in defining the testing environment and ensuring consistency across evaluations.\nBy codifying the directionality of these variables and considering the experimental design components such as control, previous prompt, date, and model, we can establish a structured and systematic approach to prompt-based experimentation. This framework allows for clear comparisons, iterative improvements, and effective tracking of the experiment's progress and outcomes.\nPrevious Example\nSome of these prompts have the word \"expect\" which is known to be ambiguous\n\n\n\n\n\nLet's consider an example to illustrate the importance of annotating the directionality within the experimental design. In this case, we initially believe that the model is correct and passes the false-belief test. However, upon further examination, we identify a potential issue of leakage in the prompt.\nThe use of words such as \"think\" or \"believe\" in the prompt directly correlates with the concept of a \"belief-test.\" This correlation can unintentionally guide the model towards the correct answer, potentially compromising the validity of the test. To address this concern, we can annotate the directionality by replacing the problematic wording with alternatives. For instance, replacing \"think\" with \"look for\" in the prompt may help mitigate the issue of leakage.\nBy annotating the directionality and actively addressing potential biases or correlations within the prompt, we can ensure a more accurate evaluation of the model's capabilities. This approach enhances the integrity of the experiment and allows for clearer insights into the model's performance and understanding of the desired concepts.\nPrevious Example\nUsing the word think or believe makes it work for GPT4, not for GPT3.5.\n\nSuggestions to frame it without using the explicit terms \"think\" \"believe\" which correlate to the expectation that outcome is independent of the action?"
  },
  {
    "objectID": "notes/capstone.html#recommended-papers-in-the-area-",
    "href": "notes/capstone.html#recommended-papers-in-the-area-",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "Recommended papers in the area:",
    "text": "Recommended papers in the area:\n\nMarkup/Interfacing Languages\nPromptSource: An Integrated Development Environment and Repository for Natural Language Prompts\nOpenPrompt: An Open-source Framework for Prompt-learning\nPromptChainer: Chaining Large Language Model Prompts through Visual Programming\nPrompting Is Programming: A Query Language For Large Language Models\nMarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding\n“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models\n\n\nPrompt Building\nLanguage Model Cascades\nAutomatic Chain of Thought Prompting in Large Language Models\nLarge Language Models Are Human-Level Prompt Engineers\nAUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts\nDemonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP\nPromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts\nGuidance: Control modern language models more effectively and efficiently than traditional prompting or chaining\n\n\nDataset Cartography\nLearning from Others’ Mistakes: Avoiding Dataset Biases without Modeling Them\nDataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\nDiversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections\n\n\nExperimental Design\n\n\nLLM Evaluation\nSelection-Inference: Exploiting Language Models for Interpretable Logical Reasoning"
  }
]