[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mimansa Jaiswal",
    "section": "",
    "text": "I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design."
  },
  {
    "objectID": "index.html#hi-im-mimansa",
    "href": "index.html#hi-im-mimansa",
    "title": "Mimansa Jaiswal",
    "section": "Hi, I’m Mimansa",
    "text": "Hi, I’m Mimansa\nI’m Mimansa, a final year PhD candidate in Computer Science (AI/Interactive Systems) at the University of Michigan.\nI am fortunate to be working with Prof. Emily Provost as part of the CHAI group. I completed my undergrad in Computer Engineering from Institute of Engineering and Technology, Indore in 2017, and worked with Prof. G.L. Prajapati for my bachelor’s thesis.\nTl;dr: I work in the area of developing interpretable and human imitating evaluation and cost-effective data collection procedures.\nOther than research, I am interested in science communication, sketchnoting, personal knowledge management and cooking. One of my favorite activities during international conferences is to plan every single meal.\nI love cats, and they keep me sane. I have two: Oreo and Bert (yes, that Bert, you read it right!)."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "Research Publications\n\n\n\n         \n            \n               Sep 2023\n            \n            \n               Interspeech\n            \n            \n               \n                  Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder\n               \n               In\n                     Interspeech,\n                        Sep 2023\n                  \n               \n               \n                  Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Metric Design\n                        \n                  \n                     \n                        \n                           Emotion is expressed through language, vocal and facial expressions. Lack of emotional alignment between modalities is a symptom of mental disorders. We propose to quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional Mismatch (EMM). EMM patterns differ between symptomatic and euthymic moods. EMM statistics serve as an effective feature for mood recognition, reducing annotation cost while preserving mood identification.\n                        \n                        \n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Design\n                        \n                  \n                     \n                        \n                           Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                        \n                        \n                           Note\n                           Demo\n                           Repo\n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Foundation Models\n                        \n                  \n                     \n                        \n                           Prompt-based language models introduce uncertainty to classification and require users to try multiple prompts with varying temperatures to find the best fit. However, this approach lacks the ability to capture implicit differences in prompts and provide adequate vocabulary. To address this, a text annotation framework is proposed to provide a structured approach to prompt definition and annotation. Better validation structures and structured prompts are necessary for using prompt-based systems at scale for labeling or retrieval.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               Nov 2022\n            \n            \n               arXiv\n            \n            \n               \n                  Human-Centered Metric Design to Promote Generalizable and Debiased Emotion Recognition\n               \n               In\n                     arXiv,\n                        Nov 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Debiasing\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Generalization\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Metrics for emotion recognition can be challenging due to their dependence on subjective human perception. This paper proposes a template formulation that derives human-centered, automatic, optimizable evaluation metrics for emotion recognition models. The template uses model explanations and sociolinguistic wordlists and can be applied to a sample or whole dataset. The proposed metrics include generalizability and debiasing improvement, and are tested on three models, datasets and sensitive variables. The metrics correlate with the models' performance and biased representations, and can be used to train models with increased generalizability, decreased bias, or both. The template is the first to provide quantifiable metrics for training and evaluating generalizability and bias in emotion recognition models.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2022\n            \n            \n               Interspeech\n            \n            \n               \n                  Mind the Gap: On the Value of Silence Representations to Lexical-Based Speech Emotion Recognition\n               \n               In\n                     Interspeech,\n                        Sep 2022\n                  \n               \n               \n                  Matthew Perez, Mimansa Jaiswal, Minxue Niu, Cristina Gorrostieta, Matthew Roddy, Kye Taylor, Reza Lotfian, John Kane, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Model Training\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Silence is crucial in speech perception, conveying emphasis and emotion. However, little research has been done on the effect of silence on linguistics and emotion recognition. We present a novel framework that fuses linguistic and silence representations for emotion recognition in naturalistic speech. Two methods to represent silence are investigated, with results showing improved performance. Modeling silence as a token in a transformer language model significantly improves performance on the MSP-Podcast dataset. Analyses show that silence emphasizes the attention of its surrounding words.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n\n\nNo matching items\n\n\n\n\nResearch Notes\n\n\n\n  \n    \n      \n        \n        \n            \n            Research Notes\n            Long Form notes\n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Foundation Models' Gender Biases\n            Demonstrating How Gender Bias Creeps into NLP Models\n            Gender biases present in Machine Learning Models are pretty difficult to identify. GPT4, as illustrated, considers the female partner to be the problem in a disagreement surrounding crying. Gendered translations and ambiguous pronoun references add to the problem and reinforce existing biases.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                    Debiasing\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            ToM Evaluation Heuristics\n            A Framework for Evaluating Theory of Mind through Structured Testing\n            The proposed framework for evaluating theory of mind emphasizes user instinct derivation and creation of evaluation problems that reflect this instinct. Irrespective of the test, it comprises three key steps - instinct derivation, prompt modification and process evaluation - that together lead to more structured and effective evaluation methods for testing theory of mind.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            CAPSTONE for LLMs\n            CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n            Prompt-based language models have limitations in classification and often require users to test multiple prompts with varying temperatures to identify the best fit. A text annotation framework addresses this by introducing explicit prompt definition and validation, and can improve performance in labeling or retrieval tasks at scale.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Text Annotation Interface Design\n            Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n            Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                \n                  \n                    Text\n                  \n                    Data Annotation\n                  \n                    Design\n                  \n                \n        \n    \n  \n\n\n\n\nNo matching items\n\n\n\n\nBlurbs\n\n\n\n  \n    \n        05-07-2023\n      \n      \n      \n        Livable Stipends for PhD Students Shouldn't Be Up for Debate\n      \n      \n        A response to the controversial idea that PhD students should barely scrape by in emergencies and rely on Blue Cupboard to feed themselves.\n      \n      \n        I recently received some criticism for my comments over the past couple of days, which seemed to justify the idea that PhD students don't need livable stipends. Let me make myself perfectly clear: that's bullshit. No one should have to choose between paying bills and getting essential medical care or feeding their pets. If you're one of those people who thinks barely scraping by is okay for PhD students, we don't see eye to eye.\n        \n        \n      \n      \n      \n        Finance\n        PhD\n        Academia\n        Opinion\n        Twitter\n        \n    \n\n\n\nNo matching items\n\n\n\n\nUpdates\n\n\n    \n    May 2023Defended my PhD titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    May 2022I was selected as one of the 8 Barbour fellows for 2022-2023 across all of UMich\n    Nov 2021Defended my PhD proposal titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    Oct 2021I will be presenting our work on sociolinguistic inspired privacy evaluation at text as data (TADA) conference 2021, UMich, Ann Arbor.\n    Sep 2021I am interning this Fall in Allen AI with Ana Marasović in the area of evaluation and interpretability.\n\n\n\nNo matching items\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "stream/index.html",
    "href": "stream/index.html",
    "title": "Blurbs",
    "section": "",
    "text": "Blurbs is where I keep my random musings, quick reviews, and noteworthy quotes all in one place. It’s convenient to have a curated collection instead of them being scattered throughout my feed. It’s a fun little side space to gather everything. \n\n\n\n\n\n\n\n  \n    \n        05-07-2023\n      \n      \n      \n        Livable Stipends for PhD Students Shouldn't Be Up for Debate\n      \n      \n        A response to the controversial idea that PhD students should barely scrape by in emergencies and rely on Blue Cupboard to feed themselves.\n      \n      \n        I recently received some criticism for my comments over the past couple of days, which seemed to justify the idea that PhD students don't need livable stipends. Let me make myself perfectly clear: that's bullshit. No one should have to choose between paying bills and getting essential medical care or feeding their pets. If you're one of those people who thinks barely scraping by is okay for PhD students, we don't see eye to eye.\n        \n        \n      \n      \n      \n        Finance\n        PhD\n        Academia\n        Opinion\n        Twitter\n        \n    \n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "stream/phd_stipends.html",
    "href": "stream/phd_stipends.html",
    "title": "Livable Stipends for PhD Students Shouldn’t Be Up for Debate",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "banner.html",
    "href": "banner.html",
    "title": "Mimansa Jaiswal",
    "section": "",
    "text": "I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.\n\n\n\n Back to top"
  },
  {
    "objectID": "about_me/research_interests.html",
    "href": "about_me/research_interests.html",
    "title": "Mimansa Jaiswal",
    "section": "",
    "text": "Errors\n        \n      \n    \n    \n      \n        \n          Metrics\n        \n      \n    \n    \n      \n        \n          XAI\n        \n      \n    \n    \n      \n        \n          Human⇔AI\n        \n      \n    \n        \n          METHODS\n        \n      \n    \n  \n\n\n  \n    \n      \n        \n          Design\n        \n      \n    \n    \n      \n        \n          Dialogue\n        \n      \n    \n    \n      \n        \n          Health\n        \n      \n    \n    \n      \n        \n          Productivity\n        \n      \n      \n        \n          AREAS\n        \n      \n    \n  \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "single_page/updates.html",
    "href": "single_page/updates.html",
    "title": "Updates",
    "section": "",
    "text": "May 2023Defended my PhD titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    May 2022I was selected as one of the 8 Barbour fellows for 2022-2023 across all of UMich\n    Nov 2021Defended my PhD proposal titled `Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation`\n    Oct 2021I will be presenting our work on sociolinguistic inspired privacy evaluation at text as data (TADA) conference 2021, UMich, Ann Arbor.\n    Sep 2021I am interning this Fall in Allen AI with Ana Marasović in the area of evaluation and interpretability.\n    May 2021Excited to be interning in the FAIR NLP group this summer and looking forward to work at the intersection of Linguistics and ML.\n    Sep 2020I have been chosen as the student representative for Faculty Hiring at my university for the 2020-21 season. Looking forward to knowing and communicating the 'usually hidden' processes.\n    May 2020Excited to be interning in the Conversation AI group at Facebook AI working on automated dialogue evaluation.\n    Dec 2019I'll be attending NeuRIPS. Shoot me an email if you want to meet and talk about anything!\n    Jan 2019I'll be a teaching assistant for the course on Applied Machine Learning for Affective Computing with my advisor. So excited to be teaching for the first time!\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "single_page/papers.html",
    "href": "single_page/papers.html",
    "title": "Research Publications",
    "section": "",
    "text": "Sep 2023\n            \n            \n               Interspeech\n            \n            \n               \n                  Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder\n               \n               In\n                     Interspeech,\n                        Sep 2023\n                  \n               \n               \n                  Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Metric Design\n                        \n                  \n                     \n                        \n                           Emotion is expressed through language, vocal and facial expressions. Lack of emotional alignment between modalities is a symptom of mental disorders. We propose to quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional Mismatch (EMM). EMM patterns differ between symptomatic and euthymic moods. EMM statistics serve as an effective feature for mood recognition, reducing annotation cost while preserving mood identification.\n                        \n                        \n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Design\n                        \n                  \n                     \n                        \n                           Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                        \n                        \n                           Note\n                           Demo\n                           Repo\n                        \n                     \n            \n         \n         \n            \n               Mar 2023\n            \n            \n               Submission\n            \n            \n               \n                  CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n               \n               In\n                     Submission,\n                        Mar 2023\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Foundation Models\n                        \n                  \n                     \n                        \n                           Prompt-based language models introduce uncertainty to classification and require users to try multiple prompts with varying temperatures to find the best fit. However, this approach lacks the ability to capture implicit differences in prompts and provide adequate vocabulary. To address this, a text annotation framework is proposed to provide a structured approach to prompt definition and annotation. Better validation structures and structured prompts are necessary for using prompt-based systems at scale for labeling or retrieval.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               Nov 2022\n            \n            \n               arXiv\n            \n            \n               \n                  Human-Centered Metric Design to Promote Generalizable and Debiased Emotion Recognition\n               \n               In\n                     arXiv,\n                        Nov 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Debiasing\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Generalization\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Metrics for emotion recognition can be challenging due to their dependence on subjective human perception. This paper proposes a template formulation that derives human-centered, automatic, optimizable evaluation metrics for emotion recognition models. The template uses model explanations and sociolinguistic wordlists and can be applied to a sample or whole dataset. The proposed metrics include generalizability and debiasing improvement, and are tested on three models, datasets and sensitive variables. The metrics correlate with the models' performance and biased representations, and can be used to train models with increased generalizability, decreased bias, or both. The template is the first to provide quantifiable metrics for training and evaluating generalizability and bias in emotion recognition models.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2022\n            \n            \n               Interspeech\n            \n            \n               \n                  Mind the Gap: On the Value of Silence Representations to Lexical-Based Speech Emotion Recognition\n               \n               In\n                     Interspeech,\n                        Sep 2022\n                  \n               \n               \n                  Matthew Perez, Mimansa Jaiswal, Minxue Niu, Cristina Gorrostieta, Matthew Roddy, Kye Taylor, Reza Lotfian, John Kane, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Model Training\n                        \n                           \n                           Interpretation\n                        \n                  \n                     \n                        \n                           Silence is crucial in speech perception, conveying emphasis and emotion. However, little research has been done on the effect of silence on linguistics and emotion recognition. We present a novel framework that fuses linguistic and silence representations for emotion recognition in naturalistic speech. Two methods to represent silence are investigated, with results showing improved performance. Modeling silence as a token in a transformer language model significantly improves performance on the MSP-Podcast dataset. Analyses show that silence emphasizes the attention of its surrounding words.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Mar 2022\n            \n            \n               Submission\n            \n            \n               \n                  Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?\n               \n               In\n                     Submission,\n                        Mar 2022\n                  \n               \n               \n                  Mimansa Jaiswal, Minxue Niu, Emily Mower Provost\n               \n                  \n                           \n                           Text\n                        \n                           \n                           Evaluation\n                        \n                           \n                           Metric Design\n                        \n                           \n                           Schema\n                        \n                           \n                           Interpretation\n                        \n                           \n                           Data Annotation\n                        \n                  \n                     \n                        \n                           Factors affecting explanation efficacy include the algorithm used and the end user. NLP papers focus on algorithms for generating explanations, but overlook other factors. This paper examines how saliency-based explanation methods for machine learning models change with controlled variables. We aim to provide a standardized list of variables to evaluate these explanations and show how SoTA algorithms can have different rankings when controlling for evaluation criteria.\n                        \n                        \n                           Note\n                        \n                     \n            \n         \n         \n            \n               2020\n            \n            \n               ACL-SRW\n            \n            \n               \n                  Noise-Based Augmentation Techniques for Emotion Datasets: What Do We Recommend?\n               \n               In\n                     ACL-SRW,\n                        2020\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Data Augmentation\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                  \n                     \n                        \n                           Multiple noise-based data augmentation approaches have been proposed to counteract this challenge in other speech domains. But, unlike speech recognition and speaker verification, the underlying label of emotion data may change given the addition of noise. In this work, we propose a set of recommendations for noise-based augmentation of emotion datasets based on human and machine performance evaluation of generated realistic noisy samples using multiple categories of environmental and synthetic noise.\n                        \n                        \n                           PDF\n                           Talk\n                        \n                     \n            \n         \n         \n            \n               May 2020\n            \n            \n               LREC\n            \n            \n               \n                  MuSE: Multimodal Stressed Emotion Dataset\n               \n               In\n                     LREC,\n                        May 2020\n                  \n               \n               \n                  Mimansa Jaiswal, Cristian-Paul Bara, Yuanhang Luo, Rada Mihalcea, Mihai Burzo, Emily Mower Provost\n               \n                  \n                           \n                           Data Collection\n                        \n                           \n                           Confounding Factors\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                  \n                     \n                        \n                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Feb 2020\n            \n            \n               AAAI and NeuRIPS-W\n            \n            \n               \n                  Privacy Enhanced Multimodal Neural Representations for Emotion Recognition\n               \n               In\n                     AAAI and NeuRIPS-W,\n                        Feb 2020\n                  \n               \n               \n                  Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Confounding Factors\n                        \n                           \n                           Emotion Recognition\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                  \n                     \n                        \n                           This paper presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2019\n            \n            \n               Interspeech\n            \n            \n               \n                  Identifying Mood Episodes Using Dialogue Features from Clinical Interviews\n               \n               In\n                     Interspeech,\n                        Sep 2019\n                  \n               \n               \n                  Zakaria Aldeneh, Mimansa Jaiswal, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Text\n                        \n                           \n                           Model Training\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Dialogue\n                        \n                  \n                     \n                        \n                           Mental health professionals assess symptom severity through semi-structured clinical interviews. During these interviews, they observe their patients’ spoken behaviors, including both what the patients say and how they say it. In this work, we move beyond acoustic and lexical information, investigating how higher-level interactive patterns also change during mood episodes.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               May 2019\n            \n            \n               ICASSP\n            \n            \n               \n                  MuSE-ing on the Impact of Utterance Ordering on Crowdsourced Emotion Annotations\n               \n               In\n                     ICASSP,\n                        May 2019\n                  \n               \n               \n                  Mimansa Jaiswal, Zakaria Aldeneh, Cristian-Paul Bara, Yuanhang Luo, Mihai Burzo, Rada Mihalcea, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Data Annotation\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Crowdsourcing\n                        \n                  \n                     \n                        \n                           Emotion expression and perception are inherently subjective. There is generally not a single annotation that can be unambiguously declared “correct.” As a result, annotations are colored by the manner in which they were collected, i.e., with or without context.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Sep 2018\n            \n            \n               Interspeech\n            \n            \n               \n                  The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild\n               \n               In\n                     Interspeech,\n                        Sep 2018\n                  \n               \n               \n                  Soheil Khorram, Mimansa Jaiswal, John Gideon, Melvin McInnis, Emily Mower Provost\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Model Training\n                        \n                           \n                           Speech and Audio\n                        \n                           \n                           Empirical Analysis\n                        \n                           \n                           Mental Health\n                        \n                  \n                     \n                        \n                           This paper presents critical steps in developing this pipeline, including (a) a new in the wild emotion dataset, the PRIORI Emotion Dataset, (b) activation/valence emotion recognition baselines, and, (c) establish emotion as a meta-feature for mood state monitoring.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Dec 2017\n            \n            \n               FLAIRS\n            \n            \n               \n                  'Hang in there': Lexical and Visual Analysis to Identify Posts Warranting Empathetic Responses\n               \n               In\n                     FLAIRS,\n                        Dec 2017\n                  \n               \n               \n                  Mimansa Jaiswal, Sairam Tabibu, Erik Cambria\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Text\n                        \n                  \n                     \n                        \n                           Saying \"You deserved it!\" to \"I failed the test\" is not a good idea. In this paper, we propose a method supported by hand-crafted features to judge if the discourse or statement requires an empathetic response.\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n         \n            \n               Jul 2017\n            \n            \n               ICDM-W\n            \n            \n               \n                  'The Truth and Nothing But The Truth': Multimodal Analysis for Deception Detection\n               \n               In\n                     ICDM-W,\n                        Jul 2017\n                  \n               \n               \n                  Mimansa Jaiswal, Sairam Tabibu, Rajiv Bajpai\n               \n                  \n                           \n                           Emotion Recognition\n                        \n                           \n                           Mental Health\n                        \n                           \n                           Multimodal\n                        \n                           \n                           Text\n                        \n                           \n                           Speech and Audio\n                        \n                  \n                     \n                        \n                           We propose a data-driven method (SVMs) for automatic deception detection in real-life trial data using visual (OpenFace) and verbal cues (Bag of Words).\n                        \n                        \n                           PDF\n                        \n                     \n            \n         \n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Research Notes",
    "section": "",
    "text": "Foundation Models' Gender Biases\n            Demonstrating How Gender Bias Creeps into NLP Models\n            Gender biases present in Machine Learning Models are pretty difficult to identify. GPT4, as illustrated, considers the female partner to be the problem in a disagreement surrounding crying. Gendered translations and ambiguous pronoun references add to the problem and reinforce existing biases.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                    Debiasing\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            ToM Evaluation Heuristics\n            A Framework for Evaluating Theory of Mind through Structured Testing\n            The proposed framework for evaluating theory of mind emphasizes user instinct derivation and creation of evaluation problems that reflect this instinct. Irrespective of the test, it comprises three key steps - instinct derivation, prompt modification and process evaluation - that together lead to more structured and effective evaluation methods for testing theory of mind.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                    Example\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            CAPSTONE for LLMs\n            CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise\n            Prompt-based language models have limitations in classification and often require users to test multiple prompts with varying temperatures to identify the best fit. A text annotation framework addresses this by introducing explicit prompt definition and validation, and can improve performance in labeling or retrieval tasks at scale.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                    Foundation Models\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Text Annotation Interface Design\n            Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations\n            Designing a user interface where human annotators can provide explanations for text data. This can help improve the transparency and interpretability of machine learning models, as well as improve their performance.\n                \n                  \n                    Text\n                  \n                    Data Annotation\n                  \n                    Design\n                  \n                \n        \n    \n  \n\n  \n    \n      \n        \n        \n            Standardized Explanation Evaluation\n            Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?\n            End users affect explanation efficacy. NLP papers overlook other factors. This paper examines how saliency-based explanations' utiltiy change with controlled variables. We aim to provide a standardized list of variables to evaluate and show how SoTA algorithms rank differently when controlling for evaluation criteria.\n                \n                  \n                    Text\n                  \n                    Evaluation\n                  \n                    Metric Design\n                  \n                    Schema\n                  \n                    Interpretation\n                  \n                    Data Annotation\n                  \n                \n        \n    \n  \n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "notes/tom_evaluation_in_foundation_models.html",
    "href": "notes/tom_evaluation_in_foundation_models.html",
    "title": "A Framework for Evaluating Theory of Mind through Structured Testing",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/explanation_annotation_interfaces.html",
    "href": "notes/explanation_annotation_interfaces.html",
    "title": "Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/controlled_evaluation_of_explanations.html",
    "href": "notes/controlled_evaluation_of_explanations.html",
    "title": "Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/gender_biases_in_llms.html",
    "href": "notes/gender_biases_in_llms.html",
    "title": "Demonstrating How Gender Bias Creeps into NLP Models",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  },
  {
    "objectID": "notes/capstone.html",
    "href": "notes/capstone.html",
    "title": "CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise",
    "section": "",
    "text": "Coming soon. \n\n\n\n Back to top"
  }
]