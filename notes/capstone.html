<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mimansa Jaiswal">
<meta name="dcterms.date" content="2023-03-01">
<meta name="description" content="Prompt-based language models have limitations in classification and often require users to test multiple prompts with varying temperatures to identify the best fit. A text annotation framework addresses this by introducing explicit prompt definition and validation, and can improve performance in labeling or retrieval tasks at scale.">

<title>CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/collapse-social-embeds-1.0.0/collapse.css" rel="stylesheet">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-129147840-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false,
  "showHighlights": "whenSidebarOpen"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>


<meta property="og:title" content="Mimansa Jaiswal">
<meta property="og:description" content="I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.">
<meta property="og:image" content="https://mimansajaiswal.github.io/notes/preview.png">
<meta property="og:site-name" content="Mimansa Jaiswal">
<meta name="twitter:title" content="CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise">
<meta name="twitter:description" content="I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.">
<meta name="twitter:image" content="https://mimansajaiswal.github.io/notes/preview.png">
<meta name="twitter:creator" content="@MimansaJ">
<meta name="twitter:site" content="@MimansaJ">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../single_page/updates.html" rel="" target=""><i class="bi bi-newspaper" role="img">
</i> 
 <span class="menu-text">Updates</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../single_page/papers.html" rel="" target=""><i class="bi bi-pen-fill" role="img">
</i> 
 <span class="menu-text">Research Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes/index.html" rel="" target=""><i class="bi bi-text-indent-left" role="img">
</i> 
 <span class="menu-text">Research Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../single_page/blurbs.html" rel="" target=""><i class="bi bi-cursor-text" role="img">
</i> 
 <span class="menu-text">Blurbs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../personal_notes/index.html" rel="" target=""><i class="bi bi-bookmark-heart-fill" role="img">
</i> 
 <span class="menu-text">Personal Notes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/MimansaJ" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text">Twitter</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text">Feed</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#what-does-usual-model-training-look-like-" id="toc-what-does-usual-model-training-look-like-" class="nav-link" data-scroll-target="#what-does-usual-model-training-look-like-">What does usual model training look like?</a></li>
  <li><a href="#problems-with-closed-llms" id="toc-problems-with-closed-llms" class="nav-link" data-scroll-target="#problems-with-closed-llms">Problems with Closed LLMs</a>
  <ul class="collapse">
  <li><a href="#prompting-closed-llms" id="toc-prompting-closed-llms" class="nav-link" data-scroll-target="#prompting-closed-llms">Prompting Closed LLMs</a></li>
  <li><a href="#prompt-based-experimental-design-schemas" id="toc-prompt-based-experimental-design-schemas" class="nav-link" data-scroll-target="#prompt-based-experimental-design-schemas">Prompt Based Experimental Design Schemas</a></li>
  </ul></li>
  <li><a href="#prompt-markup-schema" id="toc-prompt-markup-schema" class="nav-link" data-scroll-target="#prompt-markup-schema">Prompt Markup Schema</a>
  <ul class="collapse">
  <li><a href="#prompt-definition-markup" id="toc-prompt-definition-markup" class="nav-link" data-scroll-target="#prompt-definition-markup">Prompt Definition Markup</a></li>
  <li><a href="#sample-markup" id="toc-sample-markup" class="nav-link" data-scroll-target="#sample-markup">Sample Markup</a></li>
  <li><a href="#output-schema" id="toc-output-schema" class="nav-link" data-scroll-target="#output-schema">Output Schema</a></li>
  <li><a href="#metadata" id="toc-metadata" class="nav-link" data-scroll-target="#metadata">Metadata</a>
  <ul class="collapse">
  <li><a href="#connectors" id="toc-connectors" class="nav-link" data-scroll-target="#connectors">Connectors</a></li>
  <li><a href="#iteration" id="toc-iteration" class="nav-link" data-scroll-target="#iteration">Iteration</a></li>
  <li><a href="#intuition" id="toc-intuition" class="nav-link" data-scroll-target="#intuition">Intuition</a></li>
  <li><a href="#variation" id="toc-variation" class="nav-link" data-scroll-target="#variation">Variation</a></li>
  <li><a href="#experimental-design" id="toc-experimental-design" class="nav-link" data-scroll-target="#experimental-design">Experimental Design</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#footnote" id="toc-footnote" class="nav-link" data-scroll-target="#footnote">Footnote</a>
  <ul class="collapse">
  <li><a href="#recommended-papers-in-the-area-" id="toc-recommended-papers-in-the-area-" class="nav-link" data-scroll-target="#recommended-papers-in-the-area-">Recommended papers in the area:</a>
  <ul class="collapse">
  <li><a href="#markup-interfacing-languages" id="toc-markup-interfacing-languages" class="nav-link" data-scroll-target="#markup-interfacing-languages">Markup/Interfacing Languages</a></li>
  <li><a href="#prompt-building" id="toc-prompt-building" class="nav-link" data-scroll-target="#prompt-building">Prompt Building</a></li>
  <li><a href="#dataset-cartography" id="toc-dataset-cartography" class="nav-link" data-scroll-target="#dataset-cartography">Dataset Cartography</a></li>
  <li><a href="#experimental-design-" id="toc-experimental-design-" class="nav-link" data-scroll-target="#experimental-design-">Experimental Design</a></li>
  <li><a href="#llm-evaluation" id="toc-llm-evaluation" class="nav-link" data-scroll-target="#llm-evaluation">LLM Evaluation</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Text</div>
    <div class="quarto-category">Evaluation</div>
    <div class="quarto-category">Metric Design</div>
    <div class="quarto-category">Schema</div>
    <div class="quarto-category">Interpretation</div>
    <div class="quarto-category">Data Annotation</div>
    <div class="quarto-category">Foundation Models</div>
  </div>
  </div>

<div>
  <div class="description">
    Prompt-based language models have limitations in classification and often require users to test multiple prompts with varying temperatures to identify the best fit. A text annotation framework addresses this by introducing explicit prompt definition and validation, and can improve performance in labeling or retrieval tasks at scale.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mimansa Jaiswal </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 1, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>"LLMs", or Language Models, need to be trained and tested on different datasets and prompts to improve functionality and accuracy. In addition, versioning is important when assessing LLMs. Iterating on the prompts and datasets can produce better results. However, keeping different versions of both prompts and datasets can help identify which changes led to improvements in performance. All these practices should be followed to effectively evaluate and improve LLMs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ea3c09cf3d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ea3c09cf3d.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="what-does-usual-model-training-look-like-" class="level1">
<h1>What does usual model training look like?</h1>
<p>In machine learning, it's common to divide a dataset into three parts: the train set, validation set, and test set. The train set is used to train a machine learning model, the validation set is used to tune the hyperparameters of the model, and the test set is used to measure the final performance of the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-63cfd74ac5b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-63cfd74ac5b.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>There are some pre-established norms when it comes to creating these subsets. For instance, the training set should be diverse and heterogeneous, while the validation and test sets should not have any leakage or spurious correlations from the train set. By following these best practices, data scientists can create effective and accurate machine learning models that can be used for a wide range of applications.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6cbf1ea978d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6cbf1ea978d.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="problems-with-closed-llms" class="level1 page-columns page-full">
<h1>Problems with Closed LLMs</h1>
<p>The use of closed language models (LLMs) poses certain challenges when it comes to testing their performance. If we do not know what went into the training dataset of a closed LLM, it is very likely that the test dataset we use or generate will be contaminated. This can result in inaccurate performance metrics and difficulty in verifying the correctness of the model's responses.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-69887e3e0dd.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-69887e3e0dd.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Prompt-based benchmarking can exacerbate these challenges further by introducing another layer of complexity. If we are not directly testing for an "input sample" to "output" correspondence, the way we write the prompt and what we ask for as an output will significantly change how we benchmark the ability of any model. This can make evaluating the performance of a closed LLM more difficult and error-prone, and increase the risk of producing biased results.</p>
<section id="prompting-closed-llms" class="level2">
<h2 class="anchored" data-anchor-id="prompting-closed-llms">Prompting Closed LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-f258297f659.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-f258297f659.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>When we are limited to observations, it becomes essential to employ experimental design in order to assess the capabilities of a model.</p>
<p>In order to do effectively, there are three key components we require:</p>
<ul>
<li>Firstly, we need to establish controls, which serve as the baseline for comparison and help us understand the impact various factors on the model's performance.</li>
<li>Additionally, conducting experiments allows us to systematically manipulate variables and observe their effects on the model's outcomes.</li>
<li>Lastly, carefully considering and accounting for variables is crucial in order to ensure the validity and reliability of our experimental results. By incorporating these elements into our approach, we can effectively evaluate and judge the capabilities of a model limited.<br>
</li>
</ul>
</section>
<section id="prompt-based-experimental-design-schemas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="prompt-based-experimental-design-schemas">Prompt Based Experimental Design Schemas</h2>

<div class="no-row-height column-margin column-container"><div class="callout callout-style-default callout-note callout-titled" title="Michael Frank talks about need some principles from experimental psychology">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Michael Frank talks about need some principles from experimental psychology
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-85576"></div><script>tweet={"url":"https:\/\/twitter.com\/mcxfrank\/status\/1643296168276033538","author_name":"Michael C. Frank","author_url":"https:\/\/twitter.com\/mcxfrank","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EPeople are testing large language models (LLMs) on their &quot;cognitive&quot; abilities - theory of mind, causality, syllogistic reasoning, etc. Many (most?) of these evaluations are deeply flawed. To evaluate LLMs effectively, we need some principles from experimental psychology.\uD83E\uDDF5 \u003Ca href=\"https:\/\/t.co\/Xlywu3gTVz\"\u003Epic.twitter.com\/Xlywu3gTVz\u003C\/a\u003E\u003C\/p\u003E&mdash; Michael C. Frank (@mcxfrank) \u003Ca href=\"https:\/\/twitter.com\/mcxfrank\/status\/1643296168276033538?ref_src=twsrc%5Etfw\"\u003EApril 4, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-85576").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-8f14c7d2906.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-8f14c7d2906.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let's talk about Prompt Based Experimental Design Schemas. These schemas can be thought of as templates, but with a key distinction. They not only focus on varying the input sample, but also on modifying the prompt specifications. By doing so, they allow us to closely monitor and analyze the changes that occur from one prompt to another. This approach provides a valuable way to study and understand the impact of different prompts on the overall outcome of an experiment.</p>
</section>
</section>
<section id="prompt-markup-schema" class="level1 page-columns page-full">
<h1>Prompt Markup Schema</h1>
<p>Prompt Based Experimental Design Schemas can be categorized into four main categories: prompts, samples, outputs, and metadata.</p>
<p>While the evaluation or metric design for outputs is a complex problem on its own, whether we use prompt based systems or not, in this discussion we will focus on exploring schemas for the remaining three categories: prompts, samples, and metadata.</p>
<p>These aspects play a crucial role in shaping the experimental design and understanding the impact of different elements within the experiment.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1e5a7351c70.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1e5a7351c70.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>In this case, we will refer to Sparks AGI paper and utilize the basic Theory of Mind example as our reference point.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-583dc6d0606.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-583dc6d0606.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="prompt-definition-markup" class="level2">
<h2 class="anchored" data-anchor-id="prompt-definition-markup">Prompt Definition Markup</h2>
<p>Let's delve into the concept of Prompt Definition Markup. Within this framework, we identify three types of steering in a prompt: start, process, and output.</p>
<p>Each type serves a distinct purpose. Additionally, an output steer consists of two components: instructions (including constraints and format) and options.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-d9c584bb200.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-d9c584bb200.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>To illustrate, let's consider the "Sparks of AGI" paper, which utilizes only a start steer, indicating the initial direction or guidance provided in the prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-7c7578cdbf5.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-7c7578cdbf5.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>This approach allows for a clear and structured definition of prompts, enabling effective experimentation and analysis.</p>
<p>To enhance the prompt, we can introduce a process steer in addition to the start steer, encouraging a step-by-step approach. Furthermore, we can incorporate an output instruction steer, specifying that the response should only include the folder path and no additional information.</p>
<p>These modifications provide additional guidance and constraints to the prompt, allowing for a more structured and controlled experimental design.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1f7053e0950.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1f7053e0950.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9091c7d24aa.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9091c7d24aa.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="sample-markup" class="level2">
<h2 class="anchored" data-anchor-id="sample-markup">Sample Markup</h2>
<p>Let's now turn our attention to Sample Markup. A sample consists of two key components: examples and input. However, if our objective is zero-shot prompting, we may not require specific examples.</p>
<p>In cases where examples are included, they can be categorized as either positive (A1) or negative (A2) examples, providing different instances for the model to learn from. On the other hand, the input component comprises three parts: the sample itself (B1), a question (B2), and a concept (B3). These elements collectively shape the input provided to the model, allowing for a more targeted and contextualized prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9300062118c.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9300062118c.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let's consider an example to illustrate the concept of Sample Markup. In the context of the Theory of Mind False Belief Test, there are no specific examples provided. Instead, the prompt consists of a sample, a question, and an implicit concept. We can annotate this prompt accordingly, as depicted in the accompanying picture.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a1b36107f79.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a1b36107f79.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="output-schema" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="output-schema">Output Schema</h2>
<p>The output schema is a complex process that intersects significantly with large language model (LLM) evaluation techniques. If you're interested in delving deeper into this topic, I recommend checking out <a href="../notes/case-of-llm-evals.html">my other post</a>. It provides further insights and information about the intricacies involved in designing and evaluating output schemas for LLMs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-2b79a1398a3.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-2b79a1398a3.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4c3e4d1a18a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4c3e4d1a18a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div></div></section>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<p>Let's now discuss the final component of the proposed schema: Metadata. Metadata plays a crucial role in shaping the overall prompt-based experimental design. It consists of five key parts that contribute to the effectiveness and coherence of the experiment:<br>
A. Connectors<br>
B. Iteration<br>
C. Variation<br>
D. Intuition<br>
E. Experimental Design</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-c6fddf51f9e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-c6fddf51f9e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="connectors" class="level3">
<h3 class="anchored" data-anchor-id="connectors">Connectors</h3>
<p>Let's delve into the concept of Connectors within the prompt-based experimental design schema. Connectors serve as internal elements within a single prompt and play a crucial role in shaping the prompt's context and implications. They can serve different purposes:</p>
<p>A1. Addition: Connectors can add information that reinforces the belief or statement presented in the prompt. This additional information further strengthens the intended message or concept.</p>
<p>A2. Negation: On the other hand, connectors can also negate certain information presented in the prompt. By negating specific details, the prompt can introduce contrasting or alternative perspectives.</p>
<p>A3. Exception: Connectors may also be used to highlight exceptions to the information being tested or present in the prompt. These exceptions provide additional nuances and complexities to the prompt, allowing for a more comprehensive exploration of the given scenario.</p>
<p>For instance, consider the following example: "He says nothing about this to Alice, and Dropbox also does not notify Alice." In this case, the connector reinforces the belief or statement being made, emphasizing that both the person mentioned and Dropbox do not inform Alice about a particular matter. This connector strengthens the prompt's intended message and adds clarity to the scenario being presented.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ad1f2317ec.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ad1f2317ec.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="iteration" class="level3">
<h3 class="anchored" data-anchor-id="iteration">Iteration</h3>
<p>Iteration involves making changes to the prompt itself, without modifying the sample or the desired output. It allows for refining and improving the prompt to enhance the experimental design and guide the model's response.</p>
<p>There are different ways in which iteration can be implemented:</p>
<p>B1. Multi-instruction: This involves adding multiple instructions within the prompt, providing additional guidance and directions to the model. These instructions help shape the model's understanding and guide its response in a more specific manner.</p>
<p>B2. Rewording: Rewording the prompt entails changing the phrasing or wording of the prompt while maintaining the same underlying concept. This can be done to clarify the prompt, emphasize certain aspects, or provide a different perspective for the model to consider.</p>
<p>B3. Chaining: Chaining refers to linking multiple prompts together in a sequential manner. Each prompt builds upon the previous one, creating a chain of prompts that guide the model's thought process and response. This approach allows for a step-by-step exploration of the given scenario or concept.</p>
<p>For example, adding the phrase "Let's think about this step by step" to the prompt can be considered an iteration aimed at incorporating a "Process steer." This addition provides a clearer instruction to the model, encouraging a systematic and sequential approach in its response.</p>
</section>
<section id="intuition" class="level3">
<h3 class="anchored" data-anchor-id="intuition">Intuition</h3>
<p>Intuition refers to the reasoning or underlying purpose behind the prompt. It represents the intention or objective that the researchers aim to achieve through the prompt.</p>
<p>We can categorize intuition into three types:</p>
<p>C1. Implicit: Implicit intuition refers to the underlying concept or idea that is implicitly conveyed through the prompt. It represents the broader theme or topic that the prompt is designed to explore or address.</p>
<p>C2. Explicit: Explicit intuition involves explicitly stating the purpose or intention behind the prompt. It provides a clear and direct indication of the specific aspect or perspective that the prompt aims to capture or investigate.</p>
<p>C3. Test: Test intuition refers to the specific test or evaluation being conducted through the prompt. It highlights the particular assessment or examination that the prompt is designed to facilitate.</p>
<p>For example, let's consider a Theory of Mind paper. We can discretize the intuition as follows:</p>
<p>C1. Implicit (Theory of Mind): The implicit intuition of the prompt revolves around the exploration of the Theory of Mind concept, which involves understanding and analyzing how individuals perceive and interpret the thoughts, beliefs, and intentions of others.</p>
<p>C2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service): The explicit intuition of the prompt focuses on the concept of modernization and its impact on individuals' access to unseen photos due to online services. It highlights the specific aspect of modernization and its influence on personal experiences.</p>
<p>C3. Test (False Belief Test): The test intuition of the prompt centers around conducting a False Belief Test, which aims to assess individuals' understanding of false beliefs and their ability to attribute different perspectives to others.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-b86b1e8fb1e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-b86b1e8fb1e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="variation" class="level3">
<h3 class="anchored" data-anchor-id="variation">Variation</h3>
<p>Now let's shift our attention to Variations within the prompt-based experimental design schema. Variations occur across different input samples and play a crucial role in shaping the experimental design. They allow for the exploration of diverse scenarios and perspectives, ensuring a comprehensive analysis of the model's capabilities.</p>
<p>We can categorize variations into different aspects:</p>
<p>D1. Output Specification: This aspect focuses on the desired output of the prompt. It can be categorized as generative (Gen.) or discriminative, depending on whether the prompt aims to generate new content or make a judgment or discrimination based on the input.</p>
<p>D2. Concept: Conceptual variations involve different concepts or ideas presented in the prompt. These concepts can be similar, opposite, or serve as control variables, providing a range of scenarios for the model to process and respond to.</p>
<p>D3. Task: Task variations relate to the specific task or objective of the prompt. This aspect can involve exploring the subjectivity or objectivity of the prompt, allowing for different perspectives and evaluation criteria.</p>
<p>It is crucial to consider variations because they contribute to a comprehensive assessment of the model's capabilities. Without a comprehensive enough variation set, it becomes challenging to make accurate judgments regarding the model's performance and behavior.</p>
<p>For example, let's consider the following prompt:</p>
<p>C2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service)<br>
with D2. Similar Concept (Unseen Object 'Cause Sleeping)</p>
<p>In this case, the explicit intuition of the prompt revolves around the concept of modernization and its impact on individuals' access to unseen photos due to online services. The variation in the concept introduces the idea of an unseen object causing sleeping, adding a similar yet distinct scenario for the model to process and respond to.</p>
<p>Previous Example</p>
<p>Task 1: Math (BASIC Subtraction) + Implicit "No knowledge transfer through sleeping" + External actor (Action taken by someone else and not self)<br>
<br>
Both GPT4 and GPT3.5 fail on this simple task<br>
<br>
Somehow sleep is not encoded as no knowledge transfer?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-90a05f4040a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-90a05f4040a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="experimental-design" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design">Experimental Design</h3>
<p>The experimental design encompasses the basics of the experiment and the corresponding prompt, providing a structured framework for conducting the evaluation.</p>
<p>We can define different components within the experimental design:</p>
<p>E1. Control: A control prompt serves as a baseline for comparison. It can be a prompt that compares the model's performance to a base model or a different kind of model, where it is known, to the best of our knowledge, that the model does not exhibit certain capabilities. For example, if we can confidently say that GPT3.5 does not "pass" the false-belief test, it can serve as a control prompt.</p>
<p>E2. Previous Prompt: The previous prompt serves as a reference point for the current experiment. It allows us to note the differences made in the current prompt and annotate those differences for iterations and concepts. It helps in tracking the evolution and progress of the prompt design.</p>
<p>E2a. Prompt: The prompt itself is a crucial part of the experimental design. It includes the specific instructions, information, and context provided to the model to generate a response.</p>
<p>E2b. Directionality: Directionality refers to the specific direction or focus of the variables compared to the previous prompt. It includes aspects such as leakage, ambiguity, specificity, and coverage. These variables aim to reduce ambiguity, enhance specificity, and provide better coverage in the prompt design.</p>
<p>E3. Date: The date component specifies the time or period during which the experiment is conducted. It helps in documenting and tracking the experimental timeline.</p>
<p>E4. Model: The model component specifies the particular model being used in the experiment. It helps in defining the testing environment and ensuring consistency across evaluations.</p>
<p>By codifying the directionality of these variables and considering the experimental design components such as control, previous prompt, date, and model, we can establish a structured and systematic approach to prompt-based experimentation. This framework allows for clear comparisons, iterative improvements, and effective tracking of the experiment's progress and outcomes.</p>
<p><strong>Previous Example</strong></p>
<p>Some of these prompts have the word "expect" which is known to be ambiguous</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a36a84c7076.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a36a84c7076.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let's consider an example to illustrate the importance of annotating the directionality within the experimental design. In this case, we initially believe that the model is correct and passes the false-belief test. However, upon further examination, we identify a potential issue of leakage in the prompt.</p>
<p>The use of words such as "think" or "believe" in the prompt directly correlates with the concept of a "belief-test." This correlation can unintentionally guide the model towards the correct answer, potentially compromising the validity of the test. To address this concern, we can annotate the directionality by replacing the problematic wording with alternatives. For instance, replacing "think" with "look for" in the prompt may help mitigate the issue of leakage.</p>
<p>By annotating the directionality and actively addressing potential biases or correlations within the prompt, we can ensure a more accurate evaluation of the model's capabilities. This approach enhances the integrity of the experiment and allows for clearer insights into the model's performance and understanding of the desired concepts.</p>
<p><strong>Previous Example</strong></p>
<p>Using the word think or believe makes it work for GPT4, not for GPT3.5.<br>
<br>
Suggestions to frame it without using the explicit terms "think" "believe" which correlate to the expectation that outcome is independent of the action?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6193491277a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img src="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6193491277a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
</section>
<section id="footnote" class="level1">
<h1>Footnote</h1>
<p>This prompt-based experimental design schema is a work in progress and welcomes suggestions for further improvement. Integrating existing LLMs to simplify metadata processing is an open question, and suggestions on this topic are highly appreciated!</p>
<section id="recommended-papers-in-the-area-" class="level2">
<h2 class="anchored" data-anchor-id="recommended-papers-in-the-area-">Recommended papers in the area:</h2>
<section id="markup-interfacing-languages" class="level3">
<h3 class="anchored" data-anchor-id="markup-interfacing-languages">Markup/Interfacing Languages</h3>
<p><a href="https://arxiv.org/abs/2202.01279">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts</a></p>
<p><a href="https://arxiv.org/abs/2111.01998">OpenPrompt: An Open-source Framework for Prompt-learning</a></p>
<p><a href="https://arxiv.org/abs/2203.06566">PromptChainer: Chaining Large Language Model Prompts through Visual Programming</a></p>
<p><a href="https://arxiv.org/abs/2212.06094">Prompting Is Programming: A Query Language For Large Language Models</a></p>
<p><a href="https://arxiv.org/abs/2110.08518">MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding</a></p>
<p><a href="https://arxiv.org/abs/2304.06597">“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models</a></p>
</section>
<section id="prompt-building" class="level3">
<h3 class="anchored" data-anchor-id="prompt-building">Prompt Building</h3>
<p><a href="https://arxiv.org/abs/2207.10342">Language Model Cascades</a></p>
<p><a href="https://arxiv.org/abs/2210.03493">Automatic Chain of Thought Prompting in Large Language Models</a></p>
<p><a href="https://arxiv.org/abs/2211.01910">Large Language Models Are Human-Level Prompt Engineers</a></p>
<p><a href="https://arxiv.org/abs/2010.15980">AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a></p>
<p><a href="https://arxiv.org/abs/2212.14024">Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</a></p>
<p><a href="https://arxiv.org/abs/2306.04528">PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</a></p>
<p><a href="https://github.com/microsoft/guidance">Guidance: Control modern language models more effectively and efficiently than traditional prompting or chaining</a></p>
</section>
<section id="dataset-cartography" class="level3">
<h3 class="anchored" data-anchor-id="dataset-cartography">Dataset Cartography</h3>
<p><a href="https://arxiv.org/abs/2012.01300">Learning from Others’ Mistakes: Avoiding Dataset Biases without Modeling Them</a></p>
<p><a href="https://aclanthology.org/2020.emnlp-main.746/">Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics</a></p>
<p><a href="https://arxiv.org/abs/2003.08529">Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections</a></p>
</section>
<section id="experimental-design-" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design-">Experimental Design</h3>
</section>
<section id="llm-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="llm-evaluation">LLM Evaluation</h3>
<p><a href="https://arxiv.org/abs/2205.09712">Selection-Inference: Exploiting Language Models for Interpretable Logical Reasoning</a></p>


</section>
</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="mimansajaiswal/mimansajaiswal.github.io" data-repo-id="R_kgDOJo-RfQ" data-category="Announcements" data-category-id="DIC_kwDOJo-Rfc4CW3WB" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"descPosition":"bottom","selector":".lightbox","openEffect":"zoom","closeEffect":"zoom","loop":true});</script>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
</body></html>