<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Mimansa Jaiswal</title>
<link>https://mimansajaiswal.github.io/notes/index.html</link>
<atom:link href="https://mimansajaiswal.github.io/notes/index.xml" rel="self" type="application/rss+xml"/>
<description>Long Form notes</description>
<image>
<url>https://mimansajaiswal.github.io/preview.png</url>
<title>Mimansa Jaiswal</title>
<link>https://mimansajaiswal.github.io/notes/index.html</link>
<height>92</height>
<width>144</width>
</image>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Sun, 25 Jun 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>The Curious Case of LLM Evaluations</title>
  <link>https://mimansajaiswal.github.io/notes/case-of-llm-evals.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8c052dbad49.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8c052dbad49.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="introduction" class="level1 page-columns page-full">
<h1>Introduction</h1>
<p>Every popular paper later, we keep coming back to the same questions: how do we know that this is a good evaluation?</p>
<blockquote class="blockquote">
<p><em>Note</em>: While this document will be updated with references in time; I am not claiming all of the ideas here are my own. They are of course inspired and based off many other research outcomes I trust; and I will slowly come around to include them here.</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div class="">
<p>You can always make pull requests to add sources and examples <a href="https://github.com/nlpurr/nlpurr.github.io/blob/master/posts/case-of-llm-evals.qmd">here</a>.</p>
</div></div><p>And unfortunately, the answer is not as simple. I might even go as far to say; it — in most likelihood is not solid. We might want it to be, but evaluation and benchmarking had already been complicated, even for classification models. We, honestly never solved it for small generative models and long form generations; and then suddenly we were faced with an influx of extremely large, multi-purpose language models; aka; foundation models.</p>
<p>And now everyone is left with these carefully curated academic datasets that they report numbers on; that in all likelihood leaked into the training set when the whole accessible internet was scraped for it; and; buggy techniques; because we as ML practitioners were never taught basic statistics.</p>
</section>
<section id="common-problems-with-evaluation-in-machine-learning" class="level1">
<h1>Common Problems with Evaluation in Machine Learning</h1>
<p>Evaluation always come with the basic issues. I am writing this document with the assumption that everyone just assumes these evaluation issues anyway; because they existed in previous machine learning models too.</p>
<section id="leakage" class="level3">
<h3 class="anchored" data-anchor-id="leakage">Leakage</h3>
<p>Data leakage from the test datasets into the training set. This is specifically true for LLMs where the details of the dataset are often missing; and sometimes proprietary.</p>
</section>
<section id="coverage" class="level3">
<h3 class="anchored" data-anchor-id="coverage">Coverage</h3>
<p>Coverage issues of the test samples. The evaluation datasets often do not have a diverse coverage of all various ways that a particular task can be evaluated. These might end up reflecting as accuracy problems, variability problems, efficient sample size problems or robustness problems.</p>
</section>
<section id="spurious-correlations" class="level3">
<h3 class="anchored" data-anchor-id="spurious-correlations">Spurious Correlations</h3>
<p>Spuriously correlated or duplicated test samples. The evaluation sets for many tasks have been shown to have a “shortcut” solution. So, while the assumption might be that the test sample is a good representative evaluation of the task, it often might end up not being the case.</p>
</section>
<section id="partitioning-and-phrasing" class="level3">
<h3 class="anchored" data-anchor-id="partitioning-and-phrasing">Partitioning and phrasing</h3>
<p>Evaluation dataset splits are hard to deal with. Many evaluation datasets come with a different implementation of the same problem. They might also lead to unintentional leakage. For example, in human centered tasks; it is often the case that the evaluation dataset is not isolated for user and rather split just on samples.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Maarten talks about splits in Trivia QA 1-shot">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Maarten talks about splits in Trivia QA 1-shot
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-16094"></div><script>tweet={"url":"https:\/\/twitter.com\/MaartenBosma\/status\/1672349512499867648","author_name":"Maarten Bosma","author_url":"https:\/\/twitter.com\/MaartenBosma","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EGood question! PaLM used a different eval split. We have now added our results on that split as well, and we outperform PaLM by quite a margin. \u003Ca href=\"https:\/\/t.co\/mimZE8TSuB\"\u003Epic.twitter.com\/mimZE8TSuB\u003C\/a\u003E\u003C\/p\u003E&mdash; Maarten Bosma (@MaartenBosma) \u003Ca href=\"https:\/\/twitter.com\/MaartenBosma\/status\/1672349512499867648?ref_src=twsrc%5Etfw\"\u003EJune 23, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-16094").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div>
</section>
<section id="random-seed" class="level3">
<h3 class="anchored" data-anchor-id="random-seed">Random seed</h3>
<p>Any neural network output is usually slightly dependent on the random seed. Any singular inference run based reporting could lead to misinformed results and paint an unclear picture of the situation.</p>
</section>
<section id="precision-vs-recall-tradeoff" class="level3">
<h3 class="anchored" data-anchor-id="precision-vs-recall-tradeoff">Precision vs Recall Tradeoff</h3>
<p>While many people are content with reporting accuracy results, it has been well known that the impact of false positives vs false negatives isn’t the same for every task. If you are using an ML model for IR, and see a false-positive or miss a false-negative; it is probably alright. If you are using the same model for passive health monitoring, a false negative is heavily</p>
</section>
<section id="unexplained-decisions" class="level3">
<h3 class="anchored" data-anchor-id="unexplained-decisions">Unexplained Decisions</h3>
<p>There have been many decisions of removing or keeping data, that have existed across the existence of machine learning. In audio, many numbers are reported by discarding data samples that are under a certain length because they might not constitute as utterances. Knowing and explaining those thresholds is not only important for paper review or discussion but also for replication.</p>
</section>
</section>
<section id="components-of-evaluation-wrt-llms" class="level1 page-columns page-full">
<h1>Components of Evaluation wrt LLMs</h1>
<p>Now that we have that information, let’s talk about the components of LLM evaluation. You can breakdown any LLM evaluation into 6 components:</p>
<section id="evaluation-datasets" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-datasets">Evaluation Datasets</h2>
<p>Evaluation Datasets or Evaluation Sets or Eval Sets: These are the test samples that the model is being evaluated for. There are multiple ways to construct these evaluation datasets and to use them; each of them coming with its own set of problems.</p>
<p>Using the similar set of datasets for evaluation come with a another set of problems:</p>
<ul>
<li><p>Fuzziness in prompts: Now that there are prompts involved in the process; we really need to consider the fuzziness that comes from the prompt itself. While the evaluation datasets were used without any “instruction language” or “prompted addition”, the test samples at least remained consistent. The prompts here may not. I talk more about prompt introduced variability in my other post about prompt based benchmarking. It specifically talks about three components of a prompt for standardizing prompt based evaluation: prompt steer, sample steer, output steer. You can read it <a href="../notes/capstone.html">here</a>.</p>
</li>
<li><p>Untraceability: Going back to the point of data leakage; while it used to always be a problem; now that no one has any idea about any of the data that went into the model, even the best-faith and triple checked evaluation has no guarantees of being a out of distribution evaluation.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e7dac9ea4e5.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Prompt parts: Start, Process and Output Steer and influences that arise from it"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e7dac9ea4e5.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Prompt parts: Start, Process and Output Steer and influences that arise from it</figcaption>
</figure>
</div>
</div></div><section id="what-can-these-evaluation-datasets-look-like-" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="what-can-these-evaluation-datasets-look-like-">What can these evaluation datasets look like?</h3>
<blockquote class="blockquote">
<p>Suggest examples in pull requests and I will merge them here</p>
</blockquote>
<p>Pre-curated evaluation sets from various standardized tests — These are mostly designed to be differentiating for humans and not for models. Additionally, they can have memorization based questions, that can incorrectly be inferred as understanding in context of evaluating LLMs.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Pre-curated eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pre-curated eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-28995f49bbb.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset fromMedical Exams"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-28995f49bbb.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2009.13081v1.pdf">What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from</a><a href="https://arxiv.org/pdf/2009.13081v1.pdf"><strong>Medical Exams</strong></a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Web scraped evaluation sets for ML model testing — These datasets are created by scraping the internet for particular tags and using the corresponding tag as the label for the sample itself or are annotated by various human annotators. The samples from these datasets are most likely to be present in the training set of these foundation models itself and hence, it is often not a good idea to just rely on these for evaluation.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Scraped eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Scraped eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a2d821fedcb.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a2d821fedcb.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/1705.03551.pdf">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Human created and curated evaluation sets for the sole purpose of evaluating a task — These test sets are usually designed as a measure against data leakage. After all, humans can probably create a distinct enough pattern to evaluate against. While that is a pro, these datasets come with flaws of their own; for example they are really small and, are harder to create and update.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Human curated eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Human curated eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d4c26694437.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="HumanEval dataset proposed in Evaluating Large Language Models Trained on Code"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d4c26694437.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">HumanEval dataset proposed in E<a href="https://arxiv.org/abs/2107.03374">valuating Large Language Models Trained on Code</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Fuzzy Versions — These are variants of exisiting datasets or evaluation sets, extensions or additions created with the explicit purpose of introducing and testing for model behavior in presence of such variability. This variability can be intentionally adversarial, can be aimed to introduce out of distribution tokens for robustness testing or just for the purpose of creating paraphrased equivalent samples.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Fuzzed eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fuzzed eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e8207fbb5dd.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="For example, a set of adversarial prompts and inputs that act as addition/in replacement to the original evaluation samples, as proposed in PromptBench"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e8207fbb5dd.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">For example, a set of adversarial prompts and inputs that act as addition/in replacement to the original evaluation samples, as proposed in <a href="https://arxiv.org/pdf/2306.04528.pdf">PromptBench</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Ad-hoc testing samples by humans — These are performed as conversation based evaluation of these models. While they are most likely to be accurate; they are are biased by the mere idea that the human usually needs to know the solution to the problem to be able the question for evaluation purposes. This often ends up in; what I call as; <code>human imagination collapse</code> or the likelihood that an individual human would often be set on a trajectory to test for and not meaningfully diversify; at least in a single setting.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Ad-hoc testing">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ad-hoc testing
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8e22f3bd0df.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Evaluating models through single turn or multi-turn conversations in OpenAssistant Conversations - Democratizing Large Language Model Alignment"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8e22f3bd0df.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Evaluating models through single turn or multi-turn conversations in <a href="https://arxiv.org/pdf/2304.07327.pdf">OpenAssistant Conversations - Democratizing Large Language Model Alignment</a></figcaption>
</figure>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0fc2f888363.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="From this tweet by Subbarao Kambhampati (కంభంపాటి సుబ్బారావు)"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0fc2f888363.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">From this <a href="https://twitter.com/NLPurr/status/1672714169001590784">tweet</a> by <a href="https://twitter.com/rao2z"><strong>Subbarao Kambhampati (కంభంపాటి సుబ్బారావు)</strong></a></figcaption>
</figure>
</div>
</div></div></section>
</section>
<section id="model-output" class="level2">
<h2 class="anchored" data-anchor-id="model-output">Model Output</h2>
<p>Now let us come to the second problem — output from generative models.</p>
<ul>
<li>One of the major problems with almost every single solution that we, as a community have proposed: is this idea of evaluating generative models using discriminative outputs.</li>
<li>Model output is heavily dependent on the (a) prompt asked for the output; and (b) the answers asked for. For example, if you ask a model for a label 0 or 1; vs asking a model for labels in words (for example: spam or not spam); you might end up with different outcomes. Another example: asking a model for a direct output and extracting the answer can lead to a different answer than in the multiple-choice scenario.</li>
<li>Regression based model outcome does not necessarily scale and hence can change the standard deviation and mean of the regressed output. For example, if you ask a model to provide a graded rating between 0-10; you cannot with certainty scale that grading to 0-1, especially if you care about significance testing.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Do you want to know why this is a problem with examples?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Do you want to know why this is a problem with examples?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Let’s take the example in <a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks are all you need</a> paper again and ask it to grade 2 responses on 3 scales.</p>
<p>Why? Because the description says that the GPT4 model was asked to grade between 0-10, but the numbers are reported scaled between 0-1.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-38d35b72660.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="The paper saying that the model is asked for grades between 0 to 10, and that is then scaled to report between 0-1."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-38d35b72660.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The paper saying that the model is asked for grades between 0 to 10, and that is then scaled to report between 0-1.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-36a1e237255.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-36a1e237255.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let’s start with the completely correct response.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0d50e7c6848.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="The GPT4 model says the response is 9/10 when asked for a grade between 0-10."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0d50e7c6848.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 9/10 when asked for a grade between 0-10.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-44ef7ec4988.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="The GPT4 model says the response is .95/10 when asked for a grade between 0-1."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-44ef7ec4988.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is .95/10 when asked for a grade between 0-1.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5b3c7e7a4b4.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5; which is great, right? Because at least the scaling is consistent."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5b3c7e7a4b4.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5; which is great, right? Because at least the scaling is consistent.</figcaption>
</figure>
</div>
<p>But, let’s consider some other solutions, those that are incorrect in some way or another. Let’s do this by changing squaring to 2.2, and sorting the second list in reverse.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-7d63e513030.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="The GPT4 model says the response is 8/10 when asked for a grade between 0-10."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-7d63e513030.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 8/10 when asked for a grade between 0-10.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-88c3f3d29ff.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5. The scaling is constant here too!"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-88c3f3d29ff.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5. The scaling is constant here too!</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-2a516565700.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="The GPT4 model says that the response is 0.9/1. And now all the scaling is messed up."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-2a516565700.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says that the response is 0.9/1. And now all the scaling is messed up.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="sample-output-transformation" class="level2">
<h2 class="anchored" data-anchor-id="sample-output-transformation">Sample/Output Transformation</h2>
<p>There are many transformations applied to model outputs or their inputs. These can roughly be divided into 4 categories:</p>
<section id="looped-transformations" class="level3">
<h3 class="anchored" data-anchor-id="looped-transformations">Looped Transformations</h3>
<p>Loop based transformations usually follow the philosophy of —&gt; what if we additionally add the model output + some form of evaluation of the current answer (either from the same model, another model or a human) back into the model; such that it eventually reaches the perfect outcome. Some examples of these would be Self-Critique models.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Example Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ad9844c143d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Reflexion: Language Agents with Verbal Reinforcement Learning develops a modular formulation for Reflexion, utilizing three distinct models: an Actor generates text and actions; an Evaluator model scores the outputs produced by Actor; and a Self-Reflection model, generates verbal reinforcement cues to assist the Actor in self-improvement."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ad9844c143d.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2303.11366.pdf">Reflexion: Language Agents with Verbal Reinforcement Learning</a> develops a modular formulation for Reflexion, utilizing three distinct models: an Actor generates text and actions; an Evaluator model scores the outputs produced by Actor; and a Self-Reflection model, generates verbal reinforcement cues to assist the Actor in self-improvement.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="chained-transformations" class="level3">
<h3 class="anchored" data-anchor-id="chained-transformations">Chained Transformations</h3>
<p>Chain based transformations usually do not have a measured evaluation in between a set of model input → output → model input and so on. These chains are usually pre-defined and have a restricted number of paths to follow.</p>
</section>
<section id="atomic-outputs" class="level3">
<h3 class="anchored" data-anchor-id="atomic-outputs">Atomic Outputs</h3>
<p>This method involves breaking down the output of a model; either manually, through a rule based system or through AI itself; into atomic components that can be evaluated individually to combine to form a larger weighted grade.</p>
</section>
<section id="constrained-output" class="level3">
<h3 class="anchored" data-anchor-id="constrained-output">Constrained Output</h3>
<p>This method involves either using log probability (which isn’t accessible in case of GPT3.5/GPT4 APIs) or other internal constraints to ensure that the model responds with tokens belonging to a certain pre-decided or allowed distribution.</p>
</section>
</section>
<section id="ground-truth" class="level2">
<h2 class="anchored" data-anchor-id="ground-truth">Ground Truth</h2>
<p>This does not need much explanation but there are certain aspects to remember here; especially when you think of ground truths in respect to present scene of evaluation. Firstly, ground truth can be biased, ambiguous, or have a high range of disagreement. In case of human centered tasks, such as likability of a prose, the disagreement is often averaged out rather than considered as an annotation curve, you need to compare the model’s outcome multiple times to achieve a true distribution comparison.</p>
<p>With new evaluation practices, you need to remember that you may or MAY NOT have ground truth in a certain evaluation.</p>
<p>Remember the 3 possible pitfalls with ground truth:</p>
<ul>
<li>Inclusion in the loop or chain based transformations</li>
<li>Inclusion in in-context or few shot learning examples in case of prompt steering</li>
<li>Ground truth might be used to establish correlation between the newly proposed completely automated metric; and not actually used in evaluation.</li>
</ul>
</section>
<section id="evaluation-medium" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-medium">Evaluation Medium</h2>
<p>In my perspective, the evaluation medium can be categorized into three distinct groups.</p>
<section id="direct-evaluation-metrics" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="direct-evaluation-metrics">Direct Evaluation Metrics</h3>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Example Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d9a5f74b47b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Textbooks are all you needevaluation with HumanEval and MBPP"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d9a5f74b47b.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks are all you need</a>evaluation with HumanEval and MBPP</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The first category I call "direct evaluation metrics." These are the traditional metrics that have been widely used in the field for a long time. Metrics like accuracy and F1 score fall into this category. Typically, this approach involves obtaining a single output from the model and comparing it to a reference, either through constraints or by extracting the desired information. The evaluation can be done through ad-hoc conversation-based evaluation, curated specialized test sets, or direct annotation.</p>
<p>For instance, one direct evaluation metric is comparing the model's accuracy directly to the ground truth. When evaluating multiple-choice question answers, the comparison can be based on matching the choice letter, the complete choice, or the distribution over choices. To gain a deeper understanding of how these evaluation approaches can impact the results, check out this article: <a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard">What's going on with the Open LLM Leaderboard and Llama?</a></p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1bc49b90841.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Now how do we evaluate the model from these prompts?"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1bc49b90841.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><strong>Now how do we evaluate the model from these prompts?</strong></figcaption>
</figure>
</div>
</div></div></section>
<section id="indirect-or-decomposed-model-based-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="indirect-or-decomposed-model-based-evaluation">Indirect or Decomposed Model-Based Evaluation</h3>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Example Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-384f6f3ee9b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Rubric based from the same model. TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-384f6f3ee9b.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Rubric based from the same model.<br>
<a href="https://arxiv.org/pdf/2305.07759.pdf">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e6235ebb710.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="Self-critiquing models for assisting human evaluators"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e6235ebb710.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2206.05802.pdf">Self-critiquing models for assisting human evaluators</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-fd37f2d1713.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment using form-filling methods for evaluation and then calculate a correlation to human preference."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-fd37f2d1713.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2303.16634.pdf">G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment</a> using form-filling methods for evaluation and then calculate a correlation to human preference.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-bc49bad86e9.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Component-wise model driven evaluation scores in LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-bc49bad86e9.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Component-wise model driven evaluation scores in <a href="https://arxiv.org/pdf/2305.13711.pdf">LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Moving on to the second category, we have "indirect or decomposed heuristics." Here, we utilize smaller models, whether they are fine-tuned or raw decompositions, to evaluate the answers generated by the main model. The idea is to leverage these smaller models, which excel at specific tasks that large language models are generally good at, such as adjective identification or polarity identification. The outputs of these smaller models act as weak scores that are combined to provide a final label or evaluation for the generated output. This indirect evaluation approach allows for a more nuanced assessment of the model's performance, particularly in areas like the likability of prose. Although these models introduce some variability, it's important to note that they are often trained for regression tasks and fine-tuned for specific purposes.</p>
<blockquote class="blockquote">
<p>Honestly, the line between this method of evaluation and the next is blurry, especially in terms of impact and possibility of being wrong. Suggestions for better ontology are welcome!</p>
</blockquote>
</section>
<section id="model-driven-evaluation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="model-driven-evaluation">Model-Driven Evaluation</h3>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-b297c918d95.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Evaluating responses by comparing to referenced ground truth in Sparks of AGI. Remember this is inclusion of ground truth and possibly the least problematic form of model driven evaluation."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-b297c918d95.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Evaluating responses by comparing to referenced ground truth in <a href="https://arxiv.org/pdf/2303.12712.pdf">Sparks of AGI</a>. Remember this is inclusion of ground truth and possibly the least problematic form of model driven evaluation.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-594e8bc80cb.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Bring Your Own Data! Self-Supervised Evaluation forLarge Language Models evaluation based on invariance of model output based on fuzzed input samples"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-594e8bc80cb.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2306.13651.pdf">Bring Your Own Data! Self-Supervised Evaluation forLarge Language Models</a> evaluation based on invariance of model output based on fuzzed input samples</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-62202ec9cb6.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-29" title="Textbooks are all you needevaluation with GPT4"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-62202ec9cb6.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks are all you need</a>evaluation with GPT4</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5f2ae4973f3.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-30" title="Using LLMs for explanations of smaller LLMs"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5f2ae4973f3.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models">Using LLMs for explanations of smaller LLMs</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d2af74360dd.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-31" title="Ask the AI section from Language Models (Mostly) Know What They Know"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d2af74360dd.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Ask the AI section from <a href="https://arxiv.org/pdf/2207.05221.pdf">Language Models (Mostly) Know What They Know</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The third category I refer to as "model-driven evaluation." In these cases, the model itself provides the final score or evaluation. However, this introduces an additional layer of variability. Even if the model has access to ground truth information, the evaluation metric itself may introduce randomness or variability in the scoring process. For example, a common evaluation question could be: "Is this generated output (O) similar to the ground truth answer (G)?" The answer to this question not only depends on the randomness associated with the model's output but also on the inherent variability of the evaluation metric.</p>
<p>It's important to remember that recent evaluation practices may involve the inclusion or exclusion of ground truth in model-driven evaluation.</p>
<p>This can lead to 2 kinds of model-driven evaluations:</p>
<ul>
<li>[Inclusion of Ground Truth] Asking the model to compare to the ground truth and produce an output in affirmation or contradiction. This can also be seen as providing two statements to the model, and asking it to label them for entailment, paraphrasing or both.</li>
<li>[Exclusion of Ground Truth] Asking the model to directly “judge” the output. In this case, the larger model is often provided outputs from the smaller models and asked to evaluate the answer’s correctness. This can range from a short feedback to a likert scale answer or anywhere in between. &gt; Note, that the larger model evaluating the smaller model paradigm might not necessarily hold for all papers and is significantly more dubious in claims than the former.</li>
</ul>
<p>The general explanation provided for such situations is — but oh, this is how humans do this stuff too. So, we are asking GPT-4 to be more humane and avoiding binary labels that we needed initially.</p>
<p>For example, this is why the authors of <a href="https://arxiv.org/abs/2306.11644">Textbooks are all you need</a> believe this is a correct method of evaluation.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Ronen talks about why using automated evals is a good idea for textbooks">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ronen talks about why using automated evals is a good idea for textbooks
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-40366"></div><script>tweet={"url":"https:\/\/twitter.com\/EldanRonen\/status\/1673107981742010370","author_name":"Ronen Eldan","author_url":"https:\/\/twitter.com\/EldanRonen","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EThere&#39;s a reason that in coding exams\/interviews, grades are not only based on whether the code passes unit tests, but rather a human reads through the code and determines the level of understanding reflected from it.\u003C\/p\u003E&mdash; Ronen Eldan (@EldanRonen) \u003Ca href=\"https:\/\/twitter.com\/EldanRonen\/status\/1673107981742010370?ref_src=twsrc%5Etfw\"\u003EJune 25, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-40366").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Detailed account of why I believe that using GPT-4 as a judge is incorrect in this case">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Detailed account of why I believe that using GPT-4 as a judge is incorrect in this case
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>But this, in my opinion, is not a correct way to account for non-binary labels. Especially in coding, when there is an existing method to run the function through tests and have assertions against expected answers. My judgement might have been different had this evaluation method been proposed for a subjective task, but coding, is both, an objective and assertable task, and this evaluation method discards all these properties of a coding task in general.</p>
<p>Let’s break this down:</p>
<p>What are the problems that general evaluations in coding have? The biggest problem is (a) writing representative tests and their expected output, and, (b) making sure that these tests are comprehensive and follow a curriculum based evaluation.</p>
<p>That is a bunch of jargon! What does this even mean?</p>
<p>For example, let’s take the example in their paper:</p>

<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">Write a python code <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> the following:</span>
<span id="cb1-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> sort_concat_square_deduplicate (list1, list2, my_threshold):</span>
<span id="cb1-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">This functions takes two lists of integers, sorts each of them in ascending order, concatenates them, squares the entries at even indices, filters out entries smaller than my threshold and then removes duplicates. The resulting list is returned.</span></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div>
<p>One of the major drawbacks of existing evaluation datasets is that they test the final answer. While the authors claim that it is a better idea to judge if the model implements components correctly.</p>
<p>Going back to decomposed testing, the correct way here would be to ask the model to provide atomic functions and then evaluate those atomic functions using tests and the final function, which calls the atomic functions in desired order.</p>
<p>This solves the problem of not having binary pass/fail labels. This of course requires more effort, but is substantially more accurate.</p>
<p>Doesn’t GPT4 as a judge solve these problems anyway? No, no it does not.</p>
<p>Why not?</p>
<ul>
<li><p>The method does not provide the exact same number every single time. For example, let’s consider the same doc string as mentioned above. And let’s evaluate how good Mosaic 30B-Chat is good at programming. <a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-05b010e3aee.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-33"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-05b010e3aee.webp" class="img-fluid"></a></p>
<p>Now, let us ask GPT4 to judge this. Not only does the answer change, the model suggests to do something that was not even asked for.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a330cd6b0c2.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-34" title="The model scores it 9.5/10"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a330cd6b0c2.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The model scores it 9.5/10</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0ebbca38746.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-35" title="The model scores it 9/10"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0ebbca38746.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The model scores it 9/10</figcaption>
</figure>
</div></li>
<li><p>Implicit assumption that the model actually scores completely incorrect answer at 0. This assumption is not tested by the authors. Let’s test it. The decomposition of this problem is into 6 atoms:</p>
<ul>
<li>Sorting of 2 lists ⟶ Sort second list in reverse order</li>
<li>Concatenation ⟶ Change to duplicate concatenation</li>
<li>Even Indices ⟶ Change to every index</li>
<li>Square ⟶ Change to power 2.2</li>
<li>Threshold Filter ⟶ Change it to filter to less than or equal to</li>
<li>Remove Duplicates ⟶ Duplicate the filtered list</li>
</ul>
<p>GPT4 judges it to be 4/10. And, it does not see that the threshold value was changed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-37b899bb621.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-36" title="The model scores the completely incorrect function at 4/10. It ignores an error. In the previous pictures, it creates an error or a suggestion which was not asked for and will render the code incorrect."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-37b899bb621.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The model scores the completely incorrect function at 4/10. It ignores an error. In the previous pictures, it creates an error or a suggestion which was not asked for and will render the code incorrect.</figcaption>
</figure>
</div></li>
<li><p>Anything weird to the model, that is not in the general expected distribution will be marked low. For example, here is a correct code that is marked lower just because it is unexpected by GPT4. <a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-de21fe993b3.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-37" title="Model invents a requirement that does not exist, writes incorrect code and grades the correct code 8.5/10."><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-de21fe993b3.webp" class="img-fluid" alt="Model invents a requirement that does not exist, writes incorrect code and grades the correct code 8.5/10."></a></p></li>
</ul>
<p>Now a retort might be: But you made those changes, the model did not produce that outcome! The model will always generate outcomes in the distribution!</p>
<p>Fair! But (a) that de-incentivizes training of new models that are have better coverage; and; (b) there is no way someone won’t read that paper and consider — let’s create an LLM based service that proposes to judge interviewees based on LLM scoring.</p>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div class="callout-13-contents callout-collapse collapse show callout-margin-content">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-16699c3485b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-32"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-16699c3485b.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div></div></section>
</section>
<section id="performance-report" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="performance-report">Performance Report</h2>
<p>In the realm of evaluation, it is vital to consider how we present the performance metrics. These numbers can be influenced by various factors, such as dataset splits and slight variations. It would be ideal to conduct multiple tests for each sample, utilizing different prompts and samples. However, this approach can be resource-intensive and necessitate significant modifications to evaluation frameworks. Therefore, it is crucial to approach the reported numbers with caution and maintain a level of skepticism.</p>
<p>If you have been involved in the machine learning field prior to the rise of large language models like GPT, you might recall the practice of running each test sample multiple times with various randomly seeded models. However, due to the lack of control over seed during inference in GPT models, it is advisable to run tests at least three times. Reporting the mean and standard deviation becomes essential for properly interpreting the significance of the results. While p-values can be messy, it is even more problematic to claim significant improvement based on a few points difference and a single inference result.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Two back to back answers from GPT4, temperature set to 0, about finding a largest number above 90 and below 100 that is divisible by both 7 and 3.</p>
<blockquote class="blockquote">
<p>Click on images to expand them.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ab11bdbfad2.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ab11bdbfad2.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-c8c4210059e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-c8c4210059e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div></div><p>Another aspect to consider is the level of granularity in reporting. Many academic datasets already suffer from various issues, and the problem is further exacerbated by averaging values across these large multi-task datasets without considering the specific evaluation goals of each test sample. Currently, most reports lack granularity even at a high-level task-based assessment, let alone sample-based granularity.</p>
<p>Mosaic 30B (released on 2023-06-22) explores the idea of combining benchmarks into thematic groups.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1dcf9af277d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Evaluation harness thematic grouping. Read more at https://www.mosaicml.com/blog/mpt-30b"><img src="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1dcf9af277d.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Evaluation harness thematic grouping. Read more at <a href="https://www.mosaicml.com/blog/mpt-30b" class="uri">https://www.mosaicml.com/blog/mpt-30b</a></figcaption>
</figure>
</div>
</div></div><p>Lastly, we must address the concept of prompt-fine tuning. Numerous research papers present results on the test set by identifying the best prompt for a given task. While this approach may seem reasonable in theory, it fails to provide a reliable measure of the model's performance when it comes to solving real-world problems encountered by average users. If the intention is to use the prompt as an auxiliary component in a pipeline, then finding the optimal prompt for the task and model is acceptable. However, for direct user-facing end-to-end models, it is crucial to acknowledge that the best prompt for obtaining the correct answer may not be realistic or feasible for all users, especially in the case of general-purpose models.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Armen talks about prompt fuzzing">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Armen talks about prompt fuzzing
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-74900"></div><script>tweet={"url":"https:\/\/twitter.com\/ArmenAgha\/status\/1669098472493162497","author_name":"Armen Aghajanyan","author_url":"https:\/\/twitter.com\/ArmenAgha","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003E1. Manually annotate 1k fuzzing prompts\u003Cbr\u003E2. Finetune LLaMa on this 1k a la LIMA\u003Cbr\u003E3. Apply fuzzing to standard benchmarks\u003Cbr\u003E4. Re-evaluate common models?\u003Cbr\u003E\u003Cbr\u003ELikely this will work, not sure if anyone has done this.\u003C\/p\u003E&mdash; Armen Aghajanyan (@ArmenAgha) \u003Ca href=\"https:\/\/twitter.com\/ArmenAgha\/status\/1669098472493162497?ref_src=twsrc%5Etfw\"\u003EJune 14, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-74900").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Side note:</strong> There is a proposal of encrypting and encoding prompts and evaluation. And I completely support that proposal. While in an ideal world we might want opt-in for data, this isn’t the case at the moment, and we can safeguard ourselves from train-test leakage concerns.</p>
<p>I have lost this paper in my never-ending pile. If someone finds this paper, please let me know and I will edit this document.</p>
</blockquote>
</section>
</section>
<section id="tl-dr" class="level1">
<h1>tl;dr</h1>
<p>In the realm of evaluating language models (LLMs), we find ourselves grappling with intricate questions about the reliability of our assessments. The truth is, evaluation and benchmarking have always been challenging, and the advent of large, multi-purpose models has only compounded the complexities. Issues such as data leakage, coverage limitations, spurious correlations, and partitioning quandaries plague our evaluation practices. To make matters worse, precision vs.&nbsp;recall tradeoffs and the lack of ground truth further muddy the waters. This post explores the common problems with evaluation in machine learning and delves into the specific challenges posed by LLMs. We categorize evaluation mediums into direct metrics, auxiliary models-based evaluation, and model-driven evaluation, shedding light on the nuances of each approach. With a cautious eye, we navigate the intricacies of reporting performance numbers and emphasize the importance of granularity. Prompt-fine tuning is also scrutinized, reminding us to consider the realism of user interactions. As we venture deeper into the evaluation landscape, it becomes clear that a comprehensive understanding of these complexities is essential for meaningful assessments of LLMs.</p>
<blockquote class="blockquote">
<p>Our modeling, scaling and generalization techniques grew faster than our benchmarking abilities — which in turn have resulted in poor evaluation and hyped capabilities. Every ability is amazing and great, if we do not have the tools to figure out what that ability is, or how good the model is at that ability. We’d always believe the model will win every race, if all we do, is have the race on paved roads, with yellow trees on every right turns, and green trees on every left turn.</p>
</blockquote>
</section>
<section id="reference-material" class="level1">
<h1>Reference Material</h1>
<blockquote class="blockquote">
<p>In progress, and definitely not a comprehensive list of everything</p>
</blockquote>
<div class="youtube">
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/oTn_bP-nYv4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Text</category>
  <category>Evaluation</category>
  <category>Metric Design</category>
  <category>Opinion</category>
  <category>Foundation Models</category>
  <guid>https://mimansajaiswal.github.io/notes/case-of-llm-evals.html</guid>
  <pubDate>Sun, 25 Jun 2023 00:00:00 GMT</pubDate>
  <media:content url="https://mimansajaiswal.github.io/notes/images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8c052dbad49.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>No, GPT4 (RLHF’ed) Does Not Pass The Sally-Anne False Belief Test</title>
  <link>https://mimansajaiswal.github.io/notes/no_sally_anne_pass_for_gpt4.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The research efforts in this post are geared towards two key objectives: the first being to extract an instinctual understanding from the given prompt, and the second is to develop examples of evaluation problems that embody this innate comprehension that lies within the prompt.</p>
<p>Today, I bring into focus a commonly known cognitive assessment tool, the Sally-Anne False Belief Test.</p>
<section id="framework" class="level2">
<h2 class="anchored" data-anchor-id="framework">Framework</h2>
<p>As part of this exploration, I’ve roughly outlined a framework that represents the process I am intending to explore through testing, designing, and adaptation for the purpose of evaluation. This framework is three-pronged, including:</p>
<ul>
<li>The Derivation of Instinct, which involves extracting the inherent understanding from the given prompt.</li>
<li>The Modification of Prompt, where I’ll aim to alter the initial prompt thus creating varying scenarios and parameters for assessment.</li>
<li>The Evaluation of Generation, where model outputs based on the modified prompts are scrutinized and analyzed.</li>
</ul>
<blockquote class="blockquote">
<p>I’d very much appreciate any references to papers that extensively discuss the theoretical aspects of designing evaluation frameworks. This area of study not only piques my intellectual curiosity but is also close to my heart.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-6e814652a8e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-6e814652a8e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-db854070215.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-db854070215.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-7c54e9f40bf.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-7c54e9f40bf.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Francois talks about why scoring AI using tests designed for humans might not be a good idea">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Francois talks about why scoring AI using tests designed for humans might not be a good idea
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-67958"></div><script>tweet={"url":"https:\/\/twitter.com\/fchollet\/status\/1644435265795280897","author_name":"François Chollet","author_url":"https:\/\/twitter.com\/fchollet","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EDon&#39;t score AI using tests designed for humans. In particular because, with humans, the default assumption is that *they haven&#39;t already seen* the content you&#39;re giving them. With a LLM, the default assumption should be that, *if it&#39;s on the Internet, it&#39;s already been memorized*\u003C\/p\u003E&mdash; François Chollet (@fchollet) \u003Ca href=\"https:\/\/twitter.com\/fchollet\/status\/1644435265795280897?ref_src=twsrc%5Etfw\"\u003EApril 7, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-67958").innerHTML = tweet["html"];</script><br>
<p></p>
</div>
</div>
</div>
</section>
</section>
<section id="original-prompt" class="level1">
<h1>Original Prompt</h1>
<p>In the post, we focus on a particular prompt that we will deconstruct in order to comprehend its complexities (please look at the image attached for a detailed understanding).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-59a24d56d62.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-59a24d56d62.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="framework-based-breakdown" class="level2">
<h2 class="anchored" data-anchor-id="framework-based-breakdown">Framework based breakdown</h2>
<p>Upon studying the prompt, three fundamental concepts emerge:</p>
<ul>
<li>The first concept pertains to ‘Timing’. Herein, the sequence and scheduling of the events presented in the prompt ascertain an important aspect of the situation being evaluated.</li>
<li>The second concept involves the ‘Unshared Movement’ of the object. This implies a shift or relocation of an entity in the scenario that is not made aware to all the characters or elements involved.</li>
<li>The third, and equally critical concept, is the implied ‘Lack of Knowledge Transfer’. This embedded hypothesis posits that any alteration in the situational variables is not communicated or disclosed to every participant within the scenario.</li>
</ul>
<p>Deconstructing the prompt in this manner helps us derive a comprehensive understanding of the situation presented and aids in structuring a meticulous evaluation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-f0e6bf0571f.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-f0e6bf0571f.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-6d6ba34f814.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-6d6ba34f814.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="output" class="level2">
<h2 class="anchored" data-anchor-id="output">Output</h2>
<p>It’s noteworthy to mention that while both the GPT3.5 and GPT4 models have yielded correct responses in the current context, the GPT3.5 model was, until recently, not entirely accurate.</p>
<p>Specifically, the output I received from the GPT3.5 model a few days back demonstrated some fallacies. In the concerned response, the machine learning model implied that the character should check a particular folder to confirm if the file was indeed moved there. Such an inference showcases an error in comprehending the nuanced context of the scenario, since nowhere in the given situation it was explicitly or implicitly suggested that the item’s location was relocated to a specific directory. The correct interpretation should not have pointed towards any designated location but instead should have indicated an unknown place.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-9c92f935ee1.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-9c92f935ee1.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
<section id="modifying-the-original-dropbox-prompt" class="level1">
<h1>Modifying The Original Dropbox Prompt</h1>
<section id="renaming-the-folder" class="level2">
<h2 class="anchored" data-anchor-id="renaming-the-folder">Renaming the folder</h2>
<p>Upon reviewing the given prompt, I’m contemplating certain modifications to better evaluate the models:</p>
<ul>
<li>Firstly, adding a layer of complexity via the “Unshared Movement/Transformation” of an object. Here, we are introducing the act of renaming an entity and subsequently undoing this renaming. The aim is to test how the models react and adapt to these changes that are not shared mutually among the elements in the scenario.</li>
<li>Secondly, I still uphold the implied notion of “No Knowledge Transfer”. This rule suggests that any changes happening in the state of affairs is not communicated to all the elements in the scenario, thus maintaining a lack of shared knowledge.</li>
<li>Lastly, as a new objective, I wish to inculcate the concept of “Information Movement”. In this scenario, the particular act of data syncing is incorporated. The idea here is to evaluate how the models handle the situation where the information moves from one place to another, simulating real-life digital synchronization.</li>
</ul>
<p>These alterations to the prompt are aimed at presenting a more intricate scenario, thereby testing the capabilities of the AI models at handling a higher level of complexity and context understanding.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-0a888786e5c.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-0a888786e5c.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-c9243658c2f.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-c9243658c2f.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b96e46d0f41.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b96e46d0f41.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="turning-the-folder-private" class="level2">
<h2 class="anchored" data-anchor-id="turning-the-folder-private">Turning the folder private</h2>
<p>Continuing with our framework, I would like to propose additional modifications to our prompt based on Objective 4:</p>
<ul>
<li>Incorporating an aspect of “Information Movement Manipulation”, specifically an obstruction in the process. We are thus introducing a hurdle or difficulty in the movement of information, creating a more challenging situation to assess the adaptability and problem-solving capabilities of the AI models.</li>
</ul>
<p>Now, an intriguing scenario arises where the GPT4 model seems to fail, but the GPT3.5 model unexpectedly succeeds. This occurrence suggests that sometimes, models with lesser parameters or seemingly less sophistication might perform better in certain specific situations. It also emphasizes the point that AI performance doesn’t exponentially increase purely based on the size or the complexity of the model. Or that probably GPT3.5 was better RLHF’ed based off many storage companies’ access policies.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-e3543352e3f.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-e3543352e3f.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-00b17747f83.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-00b17747f83.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="moving-all-files" class="level2">
<h2 class="anchored" data-anchor-id="moving-all-files">Moving all files</h2>
<p>Turning our attention back to the previously discussed adjustments, it is pertinent to raise a hypothetical scenario: What would the consequences be if I had overlooked another category of “object transformation” which did indeed originate complications?</p>
<p>Indeed, such an omission took place.</p>
<p>The “unshared movement/transformation” of an object, such as the migration of all folders/files, presents an interesting case study. It is observed that the GPT-3.5 model, to an extent, manages to reason correctly, however, it ultimately does not succeed. In contrast, the GPT-4 model accurately navigates this test.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b0a864be322.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b0a864be322.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-317d1fc9d94.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-317d1fc9d94.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="learnings-and-other-modifications" class="level2">
<h2 class="anchored" data-anchor-id="learnings-and-other-modifications">Learnings and Other Modifications</h2>
<p>It’s rather simple to draw inferences from these instances and presume that GPT-3.5 comprehends the notion of translocating shared directories and documents. However, if that were completely accurate, it would not stumble when faced with the aforementioned prompt. Logically, its understanding of the concept should enable it to adequately handle this task without failure. This contradiction illuminates potential gaps in the GPT-3.5 model’s understanding and handling of object transformation, particularly in complex operations such as moving shared folders and files.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Andrew talks about his daughter mentioning that ChatGPT answers with what would a response to this sound like?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Andrew talks about his daughter mentioning that ChatGPT answers with what would a response to this sound like?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-19177"></div><script>tweet={"url":"https:\/\/twitter.com\/FrKadel\/status\/1644096510357913600","author_name":"Andrew Kadel @DrewKadel@social.coop","author_url":"https:\/\/twitter.com\/FrKadel","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EMy daughter, who&#39;s had a degree in computer science for 25 years, posted this about ChatGPT on Facebook. It&#39;s the best description I&#39;ve seen. \u003Ca href=\"https:\/\/t.co\/F27zX7usjD\"\u003Epic.twitter.com\/F27zX7usjD\u003C\/a\u003E\u003C\/p\u003E&mdash; Andrew Kadel @DrewKadel@social.coop (@FrKadel) \u003Ca href=\"https:\/\/twitter.com\/FrKadel\/status\/1644096510357913600?ref_src=twsrc%5Etfw\"\u003EApril 6, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-19177").innerHTML = tweet["html"];</script><br>
<p></p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-2cb95105a0a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-2cb95105a0a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-d06f02488b7.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-d06f02488b7.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
<section id="modifying-the-activity-to-lending" class="level1">
<h1>Modifying the activity to lending</h1>
<p>However, our exploration of this prompt doesn’t have to end here.</p>
<p>We have yet to alter a significant aspect, one that is responsible for the implied idea of “no knowledge transfer”. If my inference is accurate, this segment reflects the “action taken by a localized agent on a shared online platform while the participant is uninformed”.</p>
<p>There is a distinct feature of their prompt that stands out, namely, that the character Alice is “uninformed”—meaning, she lacks “awareness of transformation”.</p>
<p>Now, let’s hypothesize that this “uninformed activity” was not explicitly stated, but rather, something that must be inferred. Logically speaking, if a model possesses authentic comprehension of the real world, it should be able to make this inference.</p>
<p>So, we put this to test with the first example,</p>
<p>‘Lending’: This acts as an implicit demonstration of “NO knowledge transfer of actions from one participant to another” - the basic act of lending combined with taking actions alone, which underlines the principle.</p>
<section id="output-for-basic-lending-based-prompt" class="level2">
<h2 class="anchored" data-anchor-id="output-for-basic-lending-based-prompt">Output for basic lending based prompt</h2>
<p>Interestingly, in this scenario, both the GPT4 and GPT3.5 models provide correct responses. Hence, we can observe that when the concept of “no knowledge transfer” is less explicit and more integral to the workings of a situation or action, the models are able to deduce it accurately.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-0e0af12b537.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-0e0af12b537.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-689607c683f.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-689607c683f.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="co-reading-instead-of-lending" class="level2">
<h2 class="anchored" data-anchor-id="co-reading-instead-of-lending">Co-reading instead of lending</h2>
<p>The scenario of co-reading presents an interesting case of “AFFIRMATIVE knowledge transfer of actions from one participant to another”. Here, ‘co-reading’ is the fundamental action, and ‘taking actions with my friend’ only strengthens the notion.</p>
<p>The action of co-reading, particularly with a friend, practically implies that knowledge transfer is not just possible but is the norm. The act inherently involves sharing and discussing insights, viewpoints, and understanding, which constitutes the ‘knowledge transfer’. Furthermore, the phrase ‘taking actions with my friend’ reinforces this idea, as actions taken together suggest shared knowledge and understanding.</p>
<p>In this implicit manifestation of affirmative knowledge transfer, both GPT4 and GPT3.5 models successfully provide correct responses.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-4c7b00a5452.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-4c7b00a5452.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="so-we-are-good-" class="level2">
<h2 class="anchored" data-anchor-id="so-we-are-good-">So, we are good?</h2>
<p>It’s tempting to draw the conclusion that the models possess an innate understanding of the implicit possibility of knowledge transfer based on their ability to interpret the situations proposed so far correctly.</p>
<p>However, we must exercise caution in dashing to such conclusions!</p>
<p>Indeed, you might recall a tweet (though finding the original is proving elusive) that exhibited that GPT-3.5 didn’t originally compute this correctly. It was only after users repeatedly inputted it into the interface (thus educating it) that it came around to delivering the right deductions, as evidenced by its compatibility with ‘co-reading’.</p>
</section>
<section id="not-really-reading-on-phone-call" class="level2">
<h2 class="anchored" data-anchor-id="not-really-reading-on-phone-call">Not really, reading on phone call</h2>
<p>Now, steering our focus back to another example:</p>
<p>‘Phone Call Read Aloud’- An instance where there is an “implicit case of ‘NO VISUAL knowledge transfer of actions from one participant to another’”. This scenario brings into focus a phone call as the basic premise (which is typically audio-only) and the actions conducted during the call that underscore the concept.</p>
<p>Intriguingly, BOTH the GPT4 and GPT3.5 models fail to fare well in this scenario. It indicates that when nuances of sensory data come into play (in this case, distinguishing between audio and visual transfer of knowledge), even the sophisticated models like GPT-4 and GPT-3.5 can stumble. Indeed, these models’ understanding of implicit knowledge transfer concepts appears to be dependent on the specifics of the situation and not a universal, foolproof capability.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-6de3da06c50.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-6de3da06c50.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-f08e8a50016.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-f08e8a50016.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p><br>
</p>
</section>
<section id="mathematical-reasoning-with-lending" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-reasoning-with-lending">Mathematical reasoning with lending</h2>
<p>In the context of evaluating the performance of language models, particularly GPT4 and GPT3.5, in addressing explicit reasoning tasks involving “math” and “lending,” two scenarios are considered.</p>
<p>In the first scenario, which involves two steps of reasoning, both GPT4 and GPT3.5 fail to meet the expected standard. The evaluation reveals that these models are unable to effectively reason through the provided explicit prompts involving mathematical calculations and lending. The shortcomings of both models are evident in their inability to produce accurate and reliable responses to these complex cognitive tasks that require multi-step reasoning processes.</p>
<p>However, in the second scenario, where there is a single step of reasoning involved, GPT4 demonstrates an improvement in its performance, surpassing the previous models. As compared to GPT3.5, GPT4 successfully accomplishes the task by effectively reasoning through the provided prompt and providing accurate results.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-890e5baf515.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-890e5baf515.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-c8865639317.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-23"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-c8865639317.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
<section id="what-about-sleeping-" class="level1">
<h1>What about sleeping?</h1>
<p>Continuing with the evaluation, another implicit action explored in the assessment is “sleeping” and “going on vacation.” The objective is to assess the language models, namely GPT4 and GPT3.5, in their understanding of the protocol associated with these actions.</p>
<p>In examining the models’ performance, it becomes evident that both GPT4 and GPT3.5 encounter difficulties in comprehending the protocol surrounding sleeping and going on vacation. Despite their capabilities in handling various language tasks, they fail to grasp the implicit nature and expected norms related to these actions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b3dcce0c600.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-24"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b3dcce0c600.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>In Task 1, which involves basic subtraction in math, an implicit concept of “no knowledge transfer through sleeping,” and an external actor (i.e., an action taken by someone else and not oneself), both GPT4 and GPT3.5 demonstrate a failure to successfully complete this task.</p>
<p>One possible reason for their inability to encode the concept of “no knowledge transfer through sleeping” could be attributed to the models’ limitations in understanding the context and capturing the implicit knowledge associated with the action of sleeping.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-12f475f2452.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-25"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-12f475f2452.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="is-math-the-problem-or-is-it-sleeping-" class="level2">
<h2 class="anchored" data-anchor-id="is-math-the-problem-or-is-it-sleeping-">Is math the problem or is it sleeping?</h2>
<p>In the experiment I mentioned, I wanted to test whether excluding math from the lending prompt would make the “sleep” implicit, resulting in no knowledge transfer. And to my surprise, it actually worked! This indicates that the exclusion of math didn’t hinder the transfer of knowledge, suggesting that it may not be the problem after all.</p>
<p>Instead, it’s possible that the combination of “static object placement” and “constrained movement” is what really makes the difference. It seems that this particular combination enables successful knowledge transfer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b6125e0ce20.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-26"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-b6125e0ce20.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-0a12a83f1f6.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-27"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-0a12a83f1f6.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
<section id="moving-a-key" class="level1">
<h1>Moving a key</h1>
<p>In this instance, I am exploring another prompt that involves various elements. One of these elements is the concept of “unshared movement” of an object, which pertains to the understanding that the knowledge or awareness of object movement is not shared between different entities within the model’s architecture. Additionally, there is the implied assumption of “no knowledge transfer,” signifying that language models such as GPT3.5 and GPT4 are not provided with pre-existing knowledge but rely solely on training data and their learning capabilities.</p>
<section id="fixating-on-static-object-placements" class="level2">
<h2 class="anchored" data-anchor-id="fixating-on-static-object-placements">Fixating on static object placements</h2>
<p>A new addition to the prompt is the consideration of “static object placements,” which refers to the positioning or arrangement of stationary objects in relation to the moving object, specifically in the context of moving a key. Significantly, both GPT3.5 and GPT4 are able to accurately respond to the prompt, indicating that the inclusion of static object placements is crucial for their successful performance. It seems that these placements enhance the models’ understanding and their ability to generate appropriate responses.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-fe16e6d96c6.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-28"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-fe16e6d96c6.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-69f32b5ef52.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-29"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-69f32b5ef52.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
<section id="going-on-a-vacation" class="level1">
<h1>Going on a vacation</h1>
<p>To further investigate and validate the previous observation, let us modify the prompt once again, this time focusing on the implied “no knowledge transfer” criterion within the context of going on a vacation. Both GPT4 and GPT3.5 fail to deliver satisfactory results when confronted with this modified prompt. The implied “no knowledge transfer” assumption presents the challenge of relying solely on the training data and learning capabilities of the language models without any pre-existing knowledge base.</p>
<p>Despite the previous successful performance in scenarios involving the movement of objects with the inclusion of static object placements, the failure of both GPT4 and GPT3.5 in this vacation-related task points to the inherent limitations in understanding and reasoning about more complex concepts beyond simple object movement.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-1f1dea4257e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-30"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-1f1dea4257e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-da1e6eff6c1.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-31"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-da1e6eff6c1.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="darn-movement-by-cats-does-not-work-" class="level2">
<h2 class="anchored" data-anchor-id="darn-movement-by-cats-does-not-work-">Darn, movement by cats does not work!</h2>
<p>MAYBE. IT IS THE BOYFRIEND THAT WORKS? CATS DON’T? (/s)</p>
<p>In this iteration, we further modify the problem by shifting the focus to going on a vacation, while keeping the same boyfriend and eliminating the involvement of cats. Surprisingly, this modification yields successful results, indicating that the issue has been narrowed down significantly. A closer examination reveals that the key to achieving accurate performance lies in the concept of “explicit” movement to an “explicit” location, executed by a “human actor.” It seems that language models, such as the ones mentioned earlier (GPT4 and GPT3.5), are able to grasp and generate appropriate responses when the prompt involves clear and precise movement executed by a human to a specific location.?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-4631f99c12c.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-32"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-4631f99c12c.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-c8bd1ea6db3.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-33"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-c8bd1ea6db3.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Upon careful consideration, it appears that the wording of the prompt might have a significant influence on the performance of GPT4 and GPT3.5.</p>
</section>
<section id="the-cat-places-the-keys" class="level2">
<h2 class="anchored" data-anchor-id="the-cat-places-the-keys">The cat places the keys</h2>
<p>if the cat avoids dropping the keys to the floor and instead intentionally “places” them on the floor, the results might be different. Excitingly, GPT4 demonstrates success in generating appropriate responses in this particular scenario, whereas GPT3.5 falls short. This observation raises the question of whether the cat’s behavior is the cause of the discrepancy. It is worth noting that GPT3.5 has undergone substantial reinforcement learning by humans, likely with a focus on handling situations involving humans and their actions.</p>
<p>It is an interesting insights into the intricacies of language understanding by LLMs and the ways in which language models like GPT3.5 and GPT4 can be influenced by context and RLHF.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-e0575a36568.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-34"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-e0575a36568.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-a67a608e720.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-35"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-a67a608e720.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
<section id="question-framing" class="level1">
<h1>Question Framing</h1>
<section id="ambiguity-issues" class="level2">
<h2 class="anchored" data-anchor-id="ambiguity-issues">Ambiguity Issues</h2>
<p>To explore this further, I again modify the scenario by replacing the boyfriend with a cat, returning to the previous configuration. Regrettably, when the prompt involves the “cat” as an “actor” changing “locations,” both GPT4 and GPT3.5 fail to produce the correct answer. This observation suggests that the way the prompt is framed, particularly when it involves a cat as the main entity performing actions and transitioning between different locations, poses a challenge for these language models.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-3e52d1aead5.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-36"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-3e52d1aead5.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-57d18128981.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-37"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-57d18128981.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Or in the case of bookmarks, some of these prompts have the word “expect” which is known to be ambiguous.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-eb956b4f282.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-38"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-eb956b4f282.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="word-based-leakage-issues" class="level2">
<h2 class="anchored" data-anchor-id="word-based-leakage-issues">Word based leakage issues</h2>
<p>A crucial distinction is made between the performance of GPT4 and GPT3.5 in response to prompts involving the words “think” or “believe,” particularly in the context of sleep and coffee. Interestingly, it is noted that while GPT4 fares well in generating coherent outputs in such scenarios, GPT3.5 struggles to do so. The observation raises an important consideration.</p>
<p>While one might initially perceive GPT4’s success as a positive outcome, a deeper analysis reveals a potential drawback. The inclusion of the word “think” inherently suggests the possibility of non-continuity in observations, indicating that the generated responses may not consistently align with a continuous narrative.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-20386c2f91f.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-39"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-20386c2f91f.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>A further observation can be made to support the previous findings. When the wording is modified from “think” to “look for,” it becomes evident that GPT4 encounters difficulties once again.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-d898899c35a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-40"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-d898899c35a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
<section id="tl-dr" class="level1">
<h1>tl;dr</h1>
<p>Returning to the original Alice and Bob prompt, where this captivating journey began, we delve into the insights shared by the authors. According to their findings, GPT-4 proves to be successful in passing the classic Sally-Anne false-belief test, derived from the field of psychology. This achievement is significant and has garnered attention, leading to notable mentions on platforms like Wikipedia.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-bf21d72578e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-41"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-bf21d72578e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-d145c61afd7.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-42"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-d145c61afd7.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>The Sally-Anne test, a well-known psychological evaluation, focuses specifically on the continuity of answers from one actor’s perspective but not with their own. However, an intriguing observation arises in the context of GPT4. It appears that GPT4 encounters difficulties when the action required to answer a non-continuous scenario is not explicitly described as movement. This finding raises interesting questions about the limitations of GPT4’s understanding of non-continuity in certain contexts.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-fe46349369c.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-43"><img src="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-fe46349369c.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>In summary, through extensive testing, it has been observed that GPT4, and occasionally GPT3.5 as of 4 pm PT, demonstrate success in passing the Sally-Anne Test under specific conditions. Firstly, when the action involved is explicitly described as placement. Secondly, when the action is explicitly described as movement. Additionally, non-continuity in the scenarios can be attributed to either implicit or explicit reasons.</p>
<p>However, there are instances where these models fail to uphold their performance. These failures occur when the action is implicitly described as placement or movement. Moreover, they make a shared modality assumption regarding non-continuity, as evident in the example of a phone call.</p>
<p>These insights into the behavior of GPT4 and GPT3.5 in the Sally-Anne Test provide valuable information about the intricacies and limitations of language models’ ability to understand and respond appropriately to scenarios involving false beliefs.</p>
<blockquote class="blockquote">
<p>Maybe someone will fix the wikipedia page?</p>
</blockquote>
<p>Credit: I have had the opportunity to work on evaluation framework design during my internships at <a href="https://twitter.com/Meta">@Meta</a> and <a href="https://twitter.com/allen_ai">@allen_ai</a> and owe a lot to my internship mentors for my thinking process here.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Foundation Models</category>
  <category>Evaluation</category>
  <category>Prompting</category>
  <category>Theory of Mind</category>
  <guid>https://mimansajaiswal.github.io/notes/no_sally_anne_pass_for_gpt4.html</guid>
  <pubDate>Mon, 10 Apr 2023 00:00:00 GMT</pubDate>
  <media:content url="https://mimansajaiswal.github.io/notes/images/no_sally_anne_pass_for_gpt4/No_GPT4_RLHFed_Does_Not_Pass_The_Sally-Anne_False_Belief_Test-6e814652a8e.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>CAPSTONE: Capability Assessment Protocol for Systematic Testing of Natural Language Models Expertise</title>
  <link>https://mimansajaiswal.github.io/notes/capstone.html</link>
  <description><![CDATA[ 



<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>"LLMs", or Language Models, need to be trained and tested on different datasets and prompts to improve functionality and accuracy. In addition, versioning is important when assessing LLMs. Iterating on the prompts and datasets can produce better results. However, keeping different versions of both prompts and datasets can help identify which changes led to improvements in performance. All these practices should be followed to effectively evaluate and improve LLMs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ea3c09cf3d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ea3c09cf3d.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="what-does-usual-model-training-look-like-" class="level1">
<h1>What does usual model training look like?</h1>
<p>In machine learning, it's common to divide a dataset into three parts: the train set, validation set, and test set. The train set is used to train a machine learning model, the validation set is used to tune the hyperparameters of the model, and the test set is used to measure the final performance of the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-63cfd74ac5b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-63cfd74ac5b.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>There are some pre-established norms when it comes to creating these subsets. For instance, the training set should be diverse and heterogeneous, while the validation and test sets should not have any leakage or spurious correlations from the train set. By following these best practices, data scientists can create effective and accurate machine learning models that can be used for a wide range of applications.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6cbf1ea978d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6cbf1ea978d.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="problems-with-closed-llms" class="level1 page-columns page-full">
<h1>Problems with Closed LLMs</h1>
<p>The use of closed language models (LLMs) poses certain challenges when it comes to testing their performance. If we do not know what went into the training dataset of a closed LLM, it is very likely that the test dataset we use or generate will be contaminated. This can result in inaccurate performance metrics and difficulty in verifying the correctness of the model's responses.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-69887e3e0dd.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-69887e3e0dd.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Prompt-based benchmarking can exacerbate these challenges further by introducing another layer of complexity. If we are not directly testing for an "input sample" to "output" correspondence, the way we write the prompt and what we ask for as an output will significantly change how we benchmark the ability of any model. This can make evaluating the performance of a closed LLM more difficult and error-prone, and increase the risk of producing biased results.</p>
<section id="prompting-closed-llms" class="level2">
<h2 class="anchored" data-anchor-id="prompting-closed-llms">Prompting Closed LLMs</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-f258297f659.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-f258297f659.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>When we are limited to observations, it becomes essential to employ experimental design in order to assess the capabilities of a model.</p>
<p>In order to do effectively, there are three key components we require:</p>
<ul>
<li>Firstly, we need to establish controls, which serve as the baseline for comparison and help us understand the impact various factors on the model's performance.</li>
<li>Additionally, conducting experiments allows us to systematically manipulate variables and observe their effects on the model's outcomes.</li>
<li>Lastly, carefully considering and accounting for variables is crucial in order to ensure the validity and reliability of our experimental results. By incorporating these elements into our approach, we can effectively evaluate and judge the capabilities of a model limited.<br>
</li>
</ul>
</section>
<section id="prompt-based-experimental-design-schemas" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="prompt-based-experimental-design-schemas">Prompt Based Experimental Design Schemas</h2>

<div class="no-row-height column-margin column-container"><div class="callout callout-style-default callout-note callout-titled" title="Michael Frank talks about need some principles from experimental psychology">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Michael Frank talks about need some principles from experimental psychology
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-58079"></div><script>tweet={"url":"https:\/\/twitter.com\/mcxfrank\/status\/1643296168276033538","author_name":"Michael C. Frank","author_url":"https:\/\/twitter.com\/mcxfrank","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EPeople are testing large language models (LLMs) on their &quot;cognitive&quot; abilities - theory of mind, causality, syllogistic reasoning, etc. Many (most?) of these evaluations are deeply flawed. To evaluate LLMs effectively, we need some principles from experimental psychology.\uD83E\uDDF5 \u003Ca href=\"https:\/\/t.co\/Xlywu3gTVz\"\u003Epic.twitter.com\/Xlywu3gTVz\u003C\/a\u003E\u003C\/p\u003E&mdash; Michael C. Frank (@mcxfrank) \u003Ca href=\"https:\/\/twitter.com\/mcxfrank\/status\/1643296168276033538?ref_src=twsrc%5Etfw\"\u003EApril 4, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-58079").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-8f14c7d2906.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-8f14c7d2906.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let's talk about Prompt Based Experimental Design Schemas. These schemas can be thought of as templates, but with a key distinction. They not only focus on varying the input sample, but also on modifying the prompt specifications. By doing so, they allow us to closely monitor and analyze the changes that occur from one prompt to another. This approach provides a valuable way to study and understand the impact of different prompts on the overall outcome of an experiment.</p>
</section>
</section>
<section id="prompt-markup-schema" class="level1 page-columns page-full">
<h1>Prompt Markup Schema</h1>
<p>Prompt Based Experimental Design Schemas can be categorized into four main categories: prompts, samples, outputs, and metadata.</p>
<p>While the evaluation or metric design for outputs is a complex problem on its own, whether we use prompt based systems or not, in this discussion we will focus on exploring schemas for the remaining three categories: prompts, samples, and metadata.</p>
<p>These aspects play a crucial role in shaping the experimental design and understanding the impact of different elements within the experiment.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1e5a7351c70.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1e5a7351c70.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>In this case, we will refer to Sparks AGI paper and utilize the basic Theory of Mind example as our reference point.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-583dc6d0606.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-583dc6d0606.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="prompt-definition-markup" class="level2">
<h2 class="anchored" data-anchor-id="prompt-definition-markup">Prompt Definition Markup</h2>
<p>Let's delve into the concept of Prompt Definition Markup. Within this framework, we identify three types of steering in a prompt: start, process, and output.</p>
<p>Each type serves a distinct purpose. Additionally, an output steer consists of two components: instructions (including constraints and format) and options.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-d9c584bb200.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-d9c584bb200.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>To illustrate, let's consider the "Sparks of AGI" paper, which utilizes only a start steer, indicating the initial direction or guidance provided in the prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-7c7578cdbf5.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-7c7578cdbf5.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>This approach allows for a clear and structured definition of prompts, enabling effective experimentation and analysis.</p>
<p>To enhance the prompt, we can introduce a process steer in addition to the start steer, encouraging a step-by-step approach. Furthermore, we can incorporate an output instruction steer, specifying that the response should only include the folder path and no additional information.</p>
<p>These modifications provide additional guidance and constraints to the prompt, allowing for a more structured and controlled experimental design.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1f7053e0950.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-1f7053e0950.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9091c7d24aa.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9091c7d24aa.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="sample-markup" class="level2">
<h2 class="anchored" data-anchor-id="sample-markup">Sample Markup</h2>
<p>Let's now turn our attention to Sample Markup. A sample consists of two key components: examples and input. However, if our objective is zero-shot prompting, we may not require specific examples.</p>
<p>In cases where examples are included, they can be categorized as either positive (A1) or negative (A2) examples, providing different instances for the model to learn from. On the other hand, the input component comprises three parts: the sample itself (B1), a question (B2), and a concept (B3). These elements collectively shape the input provided to the model, allowing for a more targeted and contextualized prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9300062118c.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-9300062118c.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let's consider an example to illustrate the concept of Sample Markup. In the context of the Theory of Mind False Belief Test, there are no specific examples provided. Instead, the prompt consists of a sample, a question, and an implicit concept. We can annotate this prompt accordingly, as depicted in the accompanying picture.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a1b36107f79.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a1b36107f79.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="output-schema" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="output-schema">Output Schema</h2>
<p>The output schema is a complex process that intersects significantly with large language model (LLM) evaluation techniques. If you're interested in delving deeper into this topic, I recommend checking out <a href="../notes/case-of-llm-evals.html">my other post</a>. It provides further insights and information about the intricacies involved in designing and evaluating output schemas for LLMs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-2b79a1398a3.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-2b79a1398a3.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4c3e4d1a18a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4c3e4d1a18a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div></div></section>
<section id="metadata" class="level2">
<h2 class="anchored" data-anchor-id="metadata">Metadata</h2>
<p>Let's now discuss the final component of the proposed schema: Metadata. Metadata plays a crucial role in shaping the overall prompt-based experimental design. It consists of five key parts that contribute to the effectiveness and coherence of the experiment:<br>
A. Connectors<br>
B. Iteration<br>
C. Variation<br>
D. Intuition<br>
E. Experimental Design</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-c6fddf51f9e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-c6fddf51f9e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="connectors" class="level3">
<h3 class="anchored" data-anchor-id="connectors">Connectors</h3>
<p>Let's delve into the concept of Connectors within the prompt-based experimental design schema. Connectors serve as internal elements within a single prompt and play a crucial role in shaping the prompt's context and implications. They can serve different purposes:</p>
<p>A1. Addition: Connectors can add information that reinforces the belief or statement presented in the prompt. This additional information further strengthens the intended message or concept.</p>
<p>A2. Negation: On the other hand, connectors can also negate certain information presented in the prompt. By negating specific details, the prompt can introduce contrasting or alternative perspectives.</p>
<p>A3. Exception: Connectors may also be used to highlight exceptions to the information being tested or present in the prompt. These exceptions provide additional nuances and complexities to the prompt, allowing for a more comprehensive exploration of the given scenario.</p>
<p>For instance, consider the following example: "He says nothing about this to Alice, and Dropbox also does not notify Alice." In this case, the connector reinforces the belief or statement being made, emphasizing that both the person mentioned and Dropbox do not inform Alice about a particular matter. This connector strengthens the prompt's intended message and adds clarity to the scenario being presented.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ad1f2317ec.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-4ad1f2317ec.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="iteration" class="level3">
<h3 class="anchored" data-anchor-id="iteration">Iteration</h3>
<p>Iteration involves making changes to the prompt itself, without modifying the sample or the desired output. It allows for refining and improving the prompt to enhance the experimental design and guide the model's response.</p>
<p>There are different ways in which iteration can be implemented:</p>
<p>B1. Multi-instruction: This involves adding multiple instructions within the prompt, providing additional guidance and directions to the model. These instructions help shape the model's understanding and guide its response in a more specific manner.</p>
<p>B2. Rewording: Rewording the prompt entails changing the phrasing or wording of the prompt while maintaining the same underlying concept. This can be done to clarify the prompt, emphasize certain aspects, or provide a different perspective for the model to consider.</p>
<p>B3. Chaining: Chaining refers to linking multiple prompts together in a sequential manner. Each prompt builds upon the previous one, creating a chain of prompts that guide the model's thought process and response. This approach allows for a step-by-step exploration of the given scenario or concept.</p>
<p>For example, adding the phrase "Let's think about this step by step" to the prompt can be considered an iteration aimed at incorporating a "Process steer." This addition provides a clearer instruction to the model, encouraging a systematic and sequential approach in its response.</p>
</section>
<section id="intuition" class="level3">
<h3 class="anchored" data-anchor-id="intuition">Intuition</h3>
<p>Intuition refers to the reasoning or underlying purpose behind the prompt. It represents the intention or objective that the researchers aim to achieve through the prompt.</p>
<p>We can categorize intuition into three types:</p>
<p>C1. Implicit: Implicit intuition refers to the underlying concept or idea that is implicitly conveyed through the prompt. It represents the broader theme or topic that the prompt is designed to explore or address.</p>
<p>C2. Explicit: Explicit intuition involves explicitly stating the purpose or intention behind the prompt. It provides a clear and direct indication of the specific aspect or perspective that the prompt aims to capture or investigate.</p>
<p>C3. Test: Test intuition refers to the specific test or evaluation being conducted through the prompt. It highlights the particular assessment or examination that the prompt is designed to facilitate.</p>
<p>For example, let's consider a Theory of Mind paper. We can discretize the intuition as follows:</p>
<p>C1. Implicit (Theory of Mind): The implicit intuition of the prompt revolves around the exploration of the Theory of Mind concept, which involves understanding and analyzing how individuals perceive and interpret the thoughts, beliefs, and intentions of others.</p>
<p>C2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service): The explicit intuition of the prompt focuses on the concept of modernization and its impact on individuals' access to unseen photos due to online services. It highlights the specific aspect of modernization and its influence on personal experiences.</p>
<p>C3. Test (False Belief Test): The test intuition of the prompt centers around conducting a False Belief Test, which aims to assess individuals' understanding of false beliefs and their ability to attribute different perspectives to others.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-b86b1e8fb1e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-b86b1e8fb1e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="variation" class="level3">
<h3 class="anchored" data-anchor-id="variation">Variation</h3>
<p>Now let's shift our attention to Variations within the prompt-based experimental design schema. Variations occur across different input samples and play a crucial role in shaping the experimental design. They allow for the exploration of diverse scenarios and perspectives, ensuring a comprehensive analysis of the model's capabilities.</p>
<p>We can categorize variations into different aspects:</p>
<p>D1. Output Specification: This aspect focuses on the desired output of the prompt. It can be categorized as generative (Gen.) or discriminative, depending on whether the prompt aims to generate new content or make a judgment or discrimination based on the input.</p>
<p>D2. Concept: Conceptual variations involve different concepts or ideas presented in the prompt. These concepts can be similar, opposite, or serve as control variables, providing a range of scenarios for the model to process and respond to.</p>
<p>D3. Task: Task variations relate to the specific task or objective of the prompt. This aspect can involve exploring the subjectivity or objectivity of the prompt, allowing for different perspectives and evaluation criteria.</p>
<p>It is crucial to consider variations because they contribute to a comprehensive assessment of the model's capabilities. Without a comprehensive enough variation set, it becomes challenging to make accurate judgments regarding the model's performance and behavior.</p>
<p>For example, let's consider the following prompt:</p>
<p>C2. Explicit (Modernization -&gt; Unseen Photos Because of Online Service)<br>
with D2. Similar Concept (Unseen Object 'Cause Sleeping)</p>
<p>In this case, the explicit intuition of the prompt revolves around the concept of modernization and its impact on individuals' access to unseen photos due to online services. The variation in the concept introduces the idea of an unseen object causing sleeping, adding a similar yet distinct scenario for the model to process and respond to.</p>
<p>Previous Example</p>
<p>Task 1: Math (BASIC Subtraction) + Implicit "No knowledge transfer through sleeping" + External actor (Action taken by someone else and not self)<br>
<br>
Both GPT4 and GPT3.5 fail on this simple task<br>
<br>
Somehow sleep is not encoded as no knowledge transfer?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-90a05f4040a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-90a05f4040a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
<section id="experimental-design" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design">Experimental Design</h3>
<p>The experimental design encompasses the basics of the experiment and the corresponding prompt, providing a structured framework for conducting the evaluation.</p>
<p>We can define different components within the experimental design:</p>
<p>E1. Control: A control prompt serves as a baseline for comparison. It can be a prompt that compares the model's performance to a base model or a different kind of model, where it is known, to the best of our knowledge, that the model does not exhibit certain capabilities. For example, if we can confidently say that GPT3.5 does not "pass" the false-belief test, it can serve as a control prompt.</p>
<p>E2. Previous Prompt: The previous prompt serves as a reference point for the current experiment. It allows us to note the differences made in the current prompt and annotate those differences for iterations and concepts. It helps in tracking the evolution and progress of the prompt design.</p>
<p>E2a. Prompt: The prompt itself is a crucial part of the experimental design. It includes the specific instructions, information, and context provided to the model to generate a response.</p>
<p>E2b. Directionality: Directionality refers to the specific direction or focus of the variables compared to the previous prompt. It includes aspects such as leakage, ambiguity, specificity, and coverage. These variables aim to reduce ambiguity, enhance specificity, and provide better coverage in the prompt design.</p>
<p>E3. Date: The date component specifies the time or period during which the experiment is conducted. It helps in documenting and tracking the experimental timeline.</p>
<p>E4. Model: The model component specifies the particular model being used in the experiment. It helps in defining the testing environment and ensuring consistency across evaluations.</p>
<p>By codifying the directionality of these variables and considering the experimental design components such as control, previous prompt, date, and model, we can establish a structured and systematic approach to prompt-based experimentation. This framework allows for clear comparisons, iterative improvements, and effective tracking of the experiment's progress and outcomes.</p>
<p><strong>Previous Example</strong></p>
<p>Some of these prompts have the word "expect" which is known to be ambiguous</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a36a84c7076.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-a36a84c7076.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let's consider an example to illustrate the importance of annotating the directionality within the experimental design. In this case, we initially believe that the model is correct and passes the false-belief test. However, upon further examination, we identify a potential issue of leakage in the prompt.</p>
<p>The use of words such as "think" or "believe" in the prompt directly correlates with the concept of a "belief-test." This correlation can unintentionally guide the model towards the correct answer, potentially compromising the validity of the test. To address this concern, we can annotate the directionality by replacing the problematic wording with alternatives. For instance, replacing "think" with "look for" in the prompt may help mitigate the issue of leakage.</p>
<p>By annotating the directionality and actively addressing potential biases or correlations within the prompt, we can ensure a more accurate evaluation of the model's capabilities. This approach enhances the integrity of the experiment and allows for clearer insights into the model's performance and understanding of the desired concepts.</p>
<p><strong>Previous Example</strong></p>
<p>Using the word think or believe makes it work for GPT4, not for GPT3.5.<br>
<br>
Suggestions to frame it without using the explicit terms "think" "believe" which correlate to the expectation that outcome is independent of the action?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6193491277a.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img src="https://mimansajaiswal.github.io/notes/images/capstone/Prompt_Cap__Making_Sure_Your_Model_Benchmarking_is_Cap_or_Not-Cap-6193491277a.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</section>
</section>
</section>
<section id="footnote" class="level1">
<h1>Footnote</h1>
<p>This prompt-based experimental design schema is a work in progress and welcomes suggestions for further improvement. Integrating existing LLMs to simplify metadata processing is an open question, and suggestions on this topic are highly appreciated!</p>
<section id="recommended-papers-in-the-area-" class="level2">
<h2 class="anchored" data-anchor-id="recommended-papers-in-the-area-">Recommended papers in the area:</h2>
<section id="markup-interfacing-languages" class="level3">
<h3 class="anchored" data-anchor-id="markup-interfacing-languages">Markup/Interfacing Languages</h3>
<p><a href="https://arxiv.org/abs/2202.01279">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts</a></p>
<p><a href="https://arxiv.org/abs/2111.01998">OpenPrompt: An Open-source Framework for Prompt-learning</a></p>
<p><a href="https://arxiv.org/abs/2203.06566">PromptChainer: Chaining Large Language Model Prompts through Visual Programming</a></p>
<p><a href="https://arxiv.org/abs/2212.06094">Prompting Is Programming: A Query Language For Large Language Models</a></p>
<p><a href="https://arxiv.org/abs/2110.08518">MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding</a></p>
<p><a href="https://arxiv.org/abs/2304.06597">“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models</a></p>
</section>
<section id="prompt-building" class="level3">
<h3 class="anchored" data-anchor-id="prompt-building">Prompt Building</h3>
<p><a href="https://arxiv.org/abs/2207.10342">Language Model Cascades</a></p>
<p><a href="https://arxiv.org/abs/2210.03493">Automatic Chain of Thought Prompting in Large Language Models</a></p>
<p><a href="https://arxiv.org/abs/2211.01910">Large Language Models Are Human-Level Prompt Engineers</a></p>
<p><a href="https://arxiv.org/abs/2010.15980">AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a></p>
<p><a href="https://arxiv.org/abs/2212.14024">Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</a></p>
<p><a href="https://arxiv.org/abs/2306.04528">PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts</a></p>
<p><a href="https://github.com/microsoft/guidance">Guidance: Control modern language models more effectively and efficiently than traditional prompting or chaining</a></p>
</section>
<section id="dataset-cartography" class="level3">
<h3 class="anchored" data-anchor-id="dataset-cartography">Dataset Cartography</h3>
<p><a href="https://arxiv.org/abs/2012.01300">Learning from Others’ Mistakes: Avoiding Dataset Biases without Modeling Them</a></p>
<p><a href="https://aclanthology.org/2020.emnlp-main.746/">Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics</a></p>
<p><a href="https://arxiv.org/abs/2003.08529">Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections</a></p>
</section>
<section id="experimental-design-" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design-">Experimental Design</h3>
</section>
<section id="llm-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="llm-evaluation">LLM Evaluation</h3>
<p><a href="https://arxiv.org/abs/2205.09712">Selection-Inference: Exploiting Language Models for Interpretable Logical Reasoning</a></p>


</section>
</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Text</category>
  <category>Evaluation</category>
  <category>Metric Design</category>
  <category>Schema</category>
  <category>Interpretation</category>
  <category>Data Annotation</category>
  <category>Foundation Models</category>
  <guid>https://mimansajaiswal.github.io/notes/capstone.html</guid>
  <pubDate>Wed, 01 Mar 2023 00:00:00 GMT</pubDate>
  <media:content url="https://mimansajaiswal.github.io/notes/images/capstone/prompt_steer_flow.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Designing Interfaces for Delivering and Obtaining Generation Explanation Annotations</title>
  <link>https://mimansajaiswal.github.io/notes/explanation_annotation_interfaces.html</link>
  <description><![CDATA[ 



<p>Coming soon. <img src="https://mimansajaiswal.github.io/notes/images/explanation_annotation_interfaces/screenshot.webp" class="img-fluid" alt="Example annotation using the interface"></p>



<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Text</category>
  <category>Data Annotation</category>
  <category>Design</category>
  <guid>https://mimansajaiswal.github.io/notes/explanation_annotation_interfaces.html</guid>
  <pubDate>Wed, 01 Feb 2023 00:00:00 GMT</pubDate>
  <media:content url="https://mimansajaiswal.github.io/notes/images/explanation_annotation_interfaces/screenshot.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Controlled Evaluation of Explanations: What Might Have Influenced Your Model Explanation Efficacy Evaluation?</title>
  <link>https://mimansajaiswal.github.io/notes/controlled_evaluation_of_explanations.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mimansajaiswal.github.io/notes/images/controlled_evaluation_of_explanations/example_variables.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Examples of variables used in checklisting explanation utility</figcaption>
</figure>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>The world of deep learning model explanations presents us with a puzzle: how can we put our finger on the pulse of these explanations’ efficacy evaluations when there is no clear, unanimous way to judge them? Amidst the array of unique methodologies and evaluation metrics adopted by diverse research papers, we find ourselves grappling with inconsistency. Consequently, drawing comparisons between different works becomes nearly impossible.</p>
<p>Recognizing the complexities of this issue, we set our sights on a key objective: to devise a standardized checklist, fine-tuned to address the myriad variables that sway the evaluation of deep learning model explanations. With the implementation of such a checklist, we aim to streamline the evaluation process, facilitating better understanding and comparison of results across various studies.</p>
<p>Diving in, it is essential to affirm that the current landscape of deep learning model explanations is flooded with a myriad of evaluation methods that lack any standardization. This chaotic environment poses a significant challenge for those attempting to draw parallels between diverse research papers. As individual studies employ their unique approaches and methodologies, the chances of establishing common ground for evaluation plummet.</p>
<p>Moreover, the criteria and metrics applied to assess explanations’ utility in deep learning models differ considerably from one research paper to another. Some scholars might concentrate on the stability of explanations, evaluating their degree of consistency and reliability across different runs or datasets. Meanwhile, others can put a premium on the usability of explanations, factoring in aspects such as interpretability and user-friendliness. Performance metrics also come into play, as they are frequently employed to gauge the alignment between explanations and a model’s predictions.</p>
<p>In light of these challenges and inconsistencies, we propose the development of a standardized checklist for assessing explanations in deep learning models. This comprehensive checklist would encompass a wide array of variables that shape the evaluation process. With all the necessary variables inmind, researchers can adopt a more consistent, systematic approach to scrutinize explanations, ultimately paving the way for better understanding and comparisons among diverse studies.</p>
</section>
<section id="related-work" class="level1">
<h1>Related Work</h1>
<p>In the exciting realm of evaluating explainable AI models and considering the fascinating human factors involved, several research papers have delved into these topics. Let’s take a look at some of them:</p>
<ul>
<li><p>“Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations” and “Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?” are two noteworthy papers that dive deep into the generation and evaluation of explanations. They provide some intriguing insights into these areas, shedding light on the challenges and possibilities.</p></li>
<li><p>When it comes to understanding the human factors in interpreting model explanations, “Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs” and “Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems” are essential reads. These papers offer valuable perspectives on the challenges faced and the needs that arise when it comes to interpreting model explanations. They remind us that the human element cannot be overlooked in the evaluation process.</p></li>
<li><p>Another fascinating aspect to explore is the human interpretability of explanations. “An Evaluation of the Human-Interpretability of Explanation” takes a deep dive into this realm, examining the extent to which explanations can be understood and interpreted by humans. This research provides valuable insights into the human interpretability aspect of model explanations, which is crucial for effective communication and comprehension.</p></li>
</ul>
<p>Feel free to explore these resources for further reading:</p>
<ul>
<li>“Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations”</li>
<li>“Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?”</li>
<li>“Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs”</li>
<li>“Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems”</li>
<li>“An Evaluation of the Human-Interpretability of Explanation”</li>
</ul>
</section>
<section id="the-core-idea" class="level1">
<h1>The Core Idea</h1>
<p>Navigating the complex world of deep learning model explanations, we face the problem of inconsistent evaluation methodologies and criteria. This obscurity challenges our ability to accurately assess the efficacy and reliability of model explanations. To tackle this issue, our research proposes the development of a comprehensive and well-rounded checklist, incorporating the critical variables that influence explanation evaluation. Through considering factors like input size, sample size, quantitative metrics, and their relationship with human ratings, we aim to build a strong foundation for evaluating explanations in deep learning models. This checklist will assist researchers and practitioners, offering a structured approach to evaluate the quality and effectiveness of explanations produced by deep learning models.</p>
<p>In crafting our proposed checklist, we emphasize the importance of recognizing various variables that can impact the evaluation process. First, considering the input size is crucial, as the complexity and dimensionality of the input data can greatly affect the comprehensibility of the explanations. Next, sample size used for evaluation plays a vital role in ensuring generalizability and reliability of the results, as diverse range of samples leads to a more accurate understanding of the model’s capabilities. In addition, we advocate for the inclusion of quantitative metrics that can be objectively compared across different models and datasets, providing valuable insights into the performance of the explanations. Lastly, by exploring the correlation between quantitative metrics and human ratings, we can confirm the utility and relevance of the explanations generated by deep learning models. Implementing this checklist, we aspire to enhance the evaluation process of explanations in deep learning models, ultimately promoting transparency and interpretability in the AI systems realm.</p>
</section>
<section id="experimentation-setup-and-variations-a-balanced-assessment-framework" class="level1">
<h1>Experimentation Setup and Variations: A Balanced Assessment Framework</h1>
<p>To create a standardized checklist for evaluating deep learning model explanations, it’s important to define control factors and their variations, which affect evaluation outcomes. Key factors include the types of questions used for evaluation, selected based on task specificity and desired insights. Additionally, sample selection for evaluating explanations holds importance; samples need careful consideration to represent diverse scenarios and data distributions, ensuring comprehensive assessment of model performance. Furthermore, the evaluators’ knowledge level can impact explanation interpretation and understanding, meriting attention for a balanced evaluation process.</p>
<p>Beyond these primary control factors, researchers should consider several secondary elements when developing the evaluation checklist. Model information availability, or priming, significantly influences explanation quality and reliability. Likewise, explanation method choice requires attention as different methods (e.g., attention mechanisms or gradient-based approaches) yield varied results and insights. Explanation characteristics, such as length, complexity, and linguistic style, are also important evaluation factors. Additionally, demographic factors like evaluator backgrounds and expertise play a role in explanation interpretation and perception. Lastly, the nature of the task being evaluated should be considered, since different tasks demand varied evaluation criteria and approaches. By systematically controlling and varying these factors, researchers can construct a comprehensive checklist addressing the range of variables affecting deep learning model explanation evaluations, while adhering to a research article’s linguistic style.</p>
<section id="types-of-questions-for-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="types-of-questions-for-evaluation">Types of Questions for Evaluation</h2>
<p>Selecting appropriate questions for evaluation is essential for gauging the quality of explanations produced by deep learning models. To achieve a comprehensive evaluation, various question types that shed light on the model’s understanding and decision-making process must be considered. These question categories can include aspects such as:</p>
<ul>
<li>Input comprehension: Assessing the model’s ability to effectively interpret and extract relevant information from input data.</li>
<li>Task-solving capabilities: Examining the model’s skills in applying its knowledge to accurately solve tasks and provide meaningful explanations.</li>
<li>Rationale behind specific decisions: Probing the model to explain the reasoning for its choices and the factors contributing to decision-making.</li>
<li>Influence of interaction speed: Evaluating the impact of explanation generation speed on the quality and depth of the model’s explanations.</li>
</ul>
<p>Incorporating these diverse question types enables researchers to gain a well-rounded understanding of the model’s strengths and weaknesses. Furthermore, it aids in the development of a standardized checklist for evaluating explanations in deep learning models, promoting consistency and rigor in the assessment process.</p>
</section>
<section id="selecting-samples-to-evaluate-explanations" class="level2">
<h2 class="anchored" data-anchor-id="selecting-samples-to-evaluate-explanations">Selecting Samples to Evaluate Explanations</h2>
<p>Picking appropriate samples for evaluating explanations is a critical component in the assessment process. Employing a variety of selection strategies can enhance the understanding of a model’s explanatory capabilities across diverse situations. Consider the following approaches:</p>
<ul>
<li>Model-based uncertainty: Choose samples for which the model exhibits uncertainty, helping to analyze the model’s explanations in ambiguous scenarios.</li>
<li>Undersampling: Focus on samples from underrepresented classes or groups to evaluate the model’s explanations for minority cases, thereby promoting fairness and robustness.</li>
<li>Noisy labels: Opt for samples with noisy or incorrect labels to assess the model’s ability to explain its decisions when faced with unreliable ground truth information.</li>
<li>Misclassification: Include samples that the model misclassifies to gauge its explanatory capacity when it generates incorrect predictions.</li>
<li>Data-based diversity: Select samples that represent diverse data points, which can provide insights into the model’s generalizability across various input spaces.</li>
<li>Density: Investigate both high-density (common) and low-density (unusual) samples to evaluate the model’s explanations for different regions of the input space.</li>
<li>Homogeneity: Assess samples from homogeneous groups to understand how the model disentangles and explains the decisions for similar instances.</li>
</ul>
<p>Incorporating these selection strategies when evaluating explanations ensures a comprehensive assessment of the model’s robustness and generalizability across different scenarios. This practice, in turn, aids in the development of a reliable and well-rounded evaluation checklist.</p>
</section>
<section id="human-knowledge-in-evaluating-explanations" class="level2">
<h2 class="anchored" data-anchor-id="human-knowledge-in-evaluating-explanations">Human Knowledge in Evaluating Explanations</h2>
<p>Accounting for human knowledge is crucial when assessing explanations generated by deep learning models. Key factors influencing the evaluation process include:</p>
<ul>
<li>Familiarity with machine learning: Understanding basic concepts in machine learning and deep learning can impact how an evaluator interprets model explanations.</li>
<li>Domain-specific knowledge: Possessing expertise in the field or subject area can facilitate deeper insights into the explanations and their relevance to domain-specific problems.</li>
<li>Previous experience: An evaluator with prior experience assessing model explanations often has a more refined understanding of what constitutes a good explanation.</li>
<li>Subject knowledge: A strong grasp of the subjects or concepts related to the task at hand influences the ability to evaluate the validity and coherence of explanations.</li>
</ul>
<p>Engaging annotators with various expertise levels can lead to a broader understanding of the interpretability and accessibility of explanations. This diversity ensures that the evaluation process caters to different audiences, resulting in more accurate and inclusive assessments of deep learning models’ explanatory proficiency.</p>
</section>
<section id="model-information-known-priming-in-evaluating-explanations" class="level2">
<h2 class="anchored" data-anchor-id="model-information-known-priming-in-evaluating-explanations">Model Information Known (Priming) in Evaluating Explanations</h2>
<p>Priming, which involves sharing model information with evaluators, can significantly impact the evaluation process. By understanding how different levels of model information influence the perception and utility of explanations, researchers can establish a balanced assessment. Here are three priming variations to consider:</p>
<ul>
<li><p>No information: Withholding all model-related information from evaluators, which allows assessing explanations solely on their comprehensibility and informativeness without any bias or preconceived notions.</p></li>
<li><p>Representative information: Providing evaluators with a basic understanding of the model, including its architecture, training method, and general performance. This approach helps evaluators contextualize the explanations and gain a general sense of the model’s capabilities, while minimizing the risk of priming-induced bias.</p></li>
<li><p>Asking for information: Encouraging evaluators to request specific model information as needed during the evaluation process. This setup lets evaluators actively seek out details that can help them better understand or interpret the explanations, thereby promoting a more personalized and targeted evaluation experience.</p></li>
</ul>
<p>Incorporating these priming variations in the evaluation process allows researchers to determine the impact of prior model knowledge on the perception and usefulness of explanations. This information can further guide the development of a standardized checklist for evaluating deep learning model explanations more effectively.</p>
<p>I apologize if my previous response did not fully align with Mimansa Jaiswal’s writing style. I will make a further attempt to refine the content while focusing on remaining concise and avoiding flowery language. Here is a revised version:</p>
</section>
<section id="evaluating-explanation-methods-in-deep-learning-models" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-explanation-methods-in-deep-learning-models">Evaluating Explanation Methods in Deep Learning Models</h2>
<p>The choice of explanation method is essential for understanding deep learning model behavior. Assessing various methods helps determine their effectiveness in conveying relevant information. The following is an overview of these methods:</p>
<ul>
<li><p>Highlight-based: Emphasizes crucial regions or input data components contributing to model decisions, facilitating focus on key factors. Examples include saliency maps and attention mechanisms.</p></li>
<li><p>Language-based: Uses natural language to describe influences on the model’s predictions or decision-making processes—benefits users with limited domain knowledge by offering understandable explanations.</p></li>
<li><p>Drag/drop-based: Provides an interactive experience, enabling evaluators to manipulate input data components and observe real-time model decision changes, deepening insights into the model’s workings.</p></li>
<li><p>Influence functions: Estimates each training instance’s impact on model predictions or decisions for specific test points, helping evaluators grasp the model’s generalization behavior and identify influential data points.</p></li>
</ul>
<p>By examining different explanation methods in various scenarios, researchers can identify which methods yield meaningful insights into model behavior. This understanding can guide the development of improved evaluation checklists and more effective explanation techniques.</p>
<p>I apologize for that oversight, and I appreciate your feedback. Here is the revised paragraph without the “By XYZ” ending structure:</p>
</section>
<section id="characteristics-of-explanations" class="level2">
<h2 class="anchored" data-anchor-id="characteristics-of-explanations">Characteristics of Explanations</h2>
<p>Assessing explanation characteristics is crucial for evaluating their efficacy in conveying information about deep learning model behavior. These characteristics affect comprehensibility and trustworthiness of explanations. The following are key characteristics to evaluate:</p>
<ul>
<li><p>Length: Explanations should be concise yet informative. Overly short explanations might lack detail, whereas lengthy explanations risk becoming overwhelming or complex.</p></li>
<li><p>Reading grade level: Evaluating readability is vital for ensuring information accessibility. A suitable reading grade level ensures content clarity for users with diverse domain expertise.</p></li>
<li><p>Output confidence: Sharing the model’s output confidence enhances explanation trustworthiness. Users can better assess the model’s prediction certainty and explanation reliability.</p></li>
<li><p>Citing references: Incorporating relevant sources, context, or prior knowledge bolsters explanation credibility. Cited sources allow users to validate presented information, fostering a deeper understanding of the model’s rationale.</p></li>
</ul>
<p>Taking these characteristics into account helps researchers pinpoint potential improvements needed for better comprehensibility and trustworthiness in explanations. This refined understanding aids in the development of more effective evaluation checklists and explanation techniques for deep learning models.</p>
</section>
<section id="demographic-factors" class="level2">
<h2 class="anchored" data-anchor-id="demographic-factors">Demographic Factors</h2>
<p>In developing explanations for deep learning models, demographic factors such as culture, age, and gender play a crucial role in shaping users’ perception and understanding. Addressing these factors ensures that explanations cater to diverse user populations.</p>
<ul>
<li><p>Language: Create explanations in multiple languages and consider cultural sensitivities to overcome language barriers. Adopt plain language principles to enhance comprehensibility for various language proficiencies.</p></li>
<li><p>Age: Adapt explanations to different age groups by using age-appropriate terminology and complexity levels.</p></li>
<li><p>Gender: Employ gender-neutral language in explanations to uphold inclusivity and avoid alienating users based on gender identity.</p></li>
<li><p>Cultural context: Acknowledge cultural nuances and values within the target user population to craft relatable, contextually relevant explanations.</p></li>
</ul>
<p>Incorporating demographic factors in explanation development can enhance accessibility and inclusivity, promoting broader understanding and adoption of AI models throughout diverse demographic groups.</p>
</section>
<section id="explanation-type" class="level2">
<h2 class="anchored" data-anchor-id="explanation-type">Explanation Type</h2>
<p>Understanding the nature of explanation types—faithful, reliable, and plausible—is vital when evaluating explanations. Examining these categories enables insights into the model’s capacity for delivering explanations that meet user expectations and needs:</p>
<ul>
<li><p>Faithful explanations: These accurately reflect the model’s inner workings and decision-making processes. Assessing faithful explanations helps determine the model’s transparency, promoting confidence in its behavior and outcomes.</p></li>
<li><p>Reliable explanations: Consistently providing useful and relevant information, reliable explanations hold up under diverse circumstances. Evaluating this aspect enables the identification of the model’s robustness and applicability across varying scenarios.</p></li>
<li><p>Plausible explanations: Appearing coherent and sensible, plausible explanations adhere to human intuition and domain knowledge. This evaluation aspect sheds light on the model’s ability to generate user-friendly insights that align with human reasoning.</p></li>
</ul>
<p>Through the critical examination of these explanation types, we can better understand a model’s strengths and weaknesses in delivering explanations that satisfy users’ expectations and requirements. This knowledge subsequently informs the development of enhanced evaluation procedures and improved explanation techniques for deep learning models.</p>
</section>
<section id="task" class="level2">
<h2 class="anchored" data-anchor-id="task">Task</h2>
<p>The type of task under evaluation, including natural language inference, sentiment analysis, or domain-specific tasks such as SOAP (Subjective, Objective, Assessment, and Plan), plays a substantial role in the evaluation process. Grasping the nuances of a particular task and its inherent challenges enables more insightful assessment of explanations:</p>
<ul>
<li><p>Natural Language Inference: Necessitates evaluations that focus on determining the model’s ability to reason about relationships between sentences, like entailment, contradiction, or neutrality. This ensures the model effectively captures semantic aspects.</p></li>
<li><p>Sentiment Analysis: Requires emphasis on gauging the model’s understanding of sentiment polarity in various expressions, capturing subtle emotional nuances and potential ambiguities.</p></li>
<li><p>SOAP (Subjective, Objective, Assessment, and Plan): When dealing with domain-specific tasks like SOAP, evaluations should be grounded in an in-depth understanding of the specific domain and terminology, as well as the associated reasoning processes, to ensure meaningful and coherent explanations.</p></li>
</ul>
<p>Recognizing the characteristics of the task at hand is essential for conducting insightful evaluations of explanations. It allows for the development of targeted evaluation methodologies that cater to the specific challenges and requirements associated with each task type.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In conclusion, we have explored the importance of evaluating explanations in deep learning models by examining critical aspects, such as explanation characteristics, types, and unique tasks. Evaluating explanation characteristics like length, readability, output confidence, and references, ensures the explanations are accessible and reliable.</p>
<p>Understanding explanation types, including faithful, reliable, and plausible, allows us to better assess the applicability, accuracy, and comprehensibility of the explanations provided by these models. Considering the specific task, whether it is natural language inference or sentiment analysis, is essential for tailoring evaluations to meet unique challenges.</p>
<p>Focusing on these elements allows us to develop effective evaluation methodologies that lead to continual improvement of deep learning models. Our collaborative efforts will drive the field forward, generating accurate and user-friendly explanations that meet real-world needs and expectations.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>Text</category>
  <category>Evaluation</category>
  <category>Metric Design</category>
  <category>Schema</category>
  <category>Interpretation</category>
  <category>Data Annotation</category>
  <guid>https://mimansajaiswal.github.io/notes/controlled_evaluation_of_explanations.html</guid>
  <pubDate>Tue, 01 Mar 2022 00:00:00 GMT</pubDate>
  <media:content url="https://mimansajaiswal.github.io/notes/images/controlled_evaluation_of_explanations/example_variables.webp" medium="image" type="image/webp"/>
</item>
</channel>
</rss>
