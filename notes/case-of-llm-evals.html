<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mimansa Jaiswal">
<meta name="dcterms.date" content="2023-06-25">
<meta name="description" content="Our modeling, scaling and generalization techniques grew faster than our benchmarking abilities - which in turn have resulted in poor evaluation and hyped capabilities. Every ability is amazing and great, if we do not have the tools to figure out what that ability is, or how good the model is at that ability. We might always believe the model will win every race, if all we do, is have the race on paved roads, with yellow trees on every right turns, and green trees on every left turn.">

<title>The Curious Case of LLM Evaluations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/collapse-social-embeds-1.0.0/collapse.css" rel="stylesheet">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-129147840-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false,
  "showHighlights": "whenSidebarOpen"
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>


<meta property="og:title" content="Mimansa Jaiswal">
<meta property="og:description" content="I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.">
<meta property="og:image" content="https://mimansajaiswal.github.io/notes/preview.png">
<meta property="og:site-name" content="Mimansa Jaiswal">
<meta name="twitter:title" content="The Curious Case of LLM Evaluations">
<meta name="twitter:description" content="I am actively seeking industry research scientist and engineering positions in model evaluation, metric design, model explanation, and interpretation, and, work at the intersection of LLMs and productivity intermixed with design.">
<meta name="twitter:image" content="https://mimansajaiswal.github.io/notes/preview.png">
<meta name="twitter:creator" content="@MimansaJ">
<meta name="twitter:site" content="@MimansaJ">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../single_page/updates.html" rel="" target=""><i class="bi bi-newspaper" role="img">
</i> 
 <span class="menu-text">Updates</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../single_page/papers.html" rel="" target=""><i class="bi bi-pen-fill" role="img">
</i> 
 <span class="menu-text">Research Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes/index.html" rel="" target=""><i class="bi bi-text-indent-left" role="img">
</i> 
 <span class="menu-text">Research Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../single_page/blurbs.html" rel="" target=""><i class="bi bi-cursor-text" role="img">
</i> 
 <span class="menu-text">Blurbs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../personal_notes/index.html" rel="" target=""><i class="bi bi-bookmark-heart-fill" role="img">
</i> 
 <span class="menu-text">Personal Notes</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://twitter.com/MimansaJ" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text">Twitter</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notes/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text">Feed</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#common-problems-with-evaluation-in-machine-learning" id="toc-common-problems-with-evaluation-in-machine-learning" class="nav-link" data-scroll-target="#common-problems-with-evaluation-in-machine-learning">Common Problems with Evaluation in Machine Learning</a>
  <ul class="collapse">
  <li><a href="#leakage" id="toc-leakage" class="nav-link" data-scroll-target="#leakage">Leakage</a></li>
  <li><a href="#coverage" id="toc-coverage" class="nav-link" data-scroll-target="#coverage">Coverage</a></li>
  <li><a href="#spurious-correlations" id="toc-spurious-correlations" class="nav-link" data-scroll-target="#spurious-correlations">Spurious Correlations</a></li>
  <li><a href="#partitioning-and-phrasing" id="toc-partitioning-and-phrasing" class="nav-link" data-scroll-target="#partitioning-and-phrasing">Partitioning and phrasing</a></li>
  <li><a href="#random-seed" id="toc-random-seed" class="nav-link" data-scroll-target="#random-seed">Random seed</a></li>
  <li><a href="#precision-vs-recall-tradeoff" id="toc-precision-vs-recall-tradeoff" class="nav-link" data-scroll-target="#precision-vs-recall-tradeoff">Precision vs Recall Tradeoff</a></li>
  <li><a href="#unexplained-decisions" id="toc-unexplained-decisions" class="nav-link" data-scroll-target="#unexplained-decisions">Unexplained Decisions</a></li>
  </ul></li>
  <li><a href="#components-of-evaluation-wrt-llms" id="toc-components-of-evaluation-wrt-llms" class="nav-link" data-scroll-target="#components-of-evaluation-wrt-llms">Components of Evaluation wrt LLMs</a>
  <ul class="collapse">
  <li><a href="#evaluation-datasets" id="toc-evaluation-datasets" class="nav-link" data-scroll-target="#evaluation-datasets">Evaluation Datasets</a>
  <ul class="collapse">
  <li><a href="#what-can-these-evaluation-datasets-look-like-" id="toc-what-can-these-evaluation-datasets-look-like-" class="nav-link" data-scroll-target="#what-can-these-evaluation-datasets-look-like-">What can these evaluation datasets look like?</a></li>
  </ul></li>
  <li><a href="#model-output" id="toc-model-output" class="nav-link" data-scroll-target="#model-output">Model Output</a></li>
  <li><a href="#sample-output-transformation" id="toc-sample-output-transformation" class="nav-link" data-scroll-target="#sample-output-transformation">Sample/Output Transformation</a>
  <ul class="collapse">
  <li><a href="#looped-transformations" id="toc-looped-transformations" class="nav-link" data-scroll-target="#looped-transformations">Looped Transformations</a></li>
  <li><a href="#chained-transformations" id="toc-chained-transformations" class="nav-link" data-scroll-target="#chained-transformations">Chained Transformations</a></li>
  <li><a href="#atomic-outputs" id="toc-atomic-outputs" class="nav-link" data-scroll-target="#atomic-outputs">Atomic Outputs</a></li>
  <li><a href="#constrained-output" id="toc-constrained-output" class="nav-link" data-scroll-target="#constrained-output">Constrained Output</a></li>
  </ul></li>
  <li><a href="#ground-truth" id="toc-ground-truth" class="nav-link" data-scroll-target="#ground-truth">Ground Truth</a></li>
  <li><a href="#evaluation-medium" id="toc-evaluation-medium" class="nav-link" data-scroll-target="#evaluation-medium">Evaluation Medium</a>
  <ul class="collapse">
  <li><a href="#direct-evaluation-metrics" id="toc-direct-evaluation-metrics" class="nav-link" data-scroll-target="#direct-evaluation-metrics">Direct Evaluation Metrics</a></li>
  <li><a href="#indirect-or-decomposed-model-based-evaluation" id="toc-indirect-or-decomposed-model-based-evaluation" class="nav-link" data-scroll-target="#indirect-or-decomposed-model-based-evaluation">Indirect or Decomposed Model-Based Evaluation</a></li>
  <li><a href="#model-driven-evaluation" id="toc-model-driven-evaluation" class="nav-link" data-scroll-target="#model-driven-evaluation">Model-Driven Evaluation</a></li>
  </ul></li>
  <li><a href="#performance-report" id="toc-performance-report" class="nav-link" data-scroll-target="#performance-report">Performance Report</a></li>
  </ul></li>
  <li><a href="#tl-dr" id="toc-tl-dr" class="nav-link" data-scroll-target="#tl-dr">tl;dr</a></li>
  <li><a href="#reference-material" id="toc-reference-material" class="nav-link" data-scroll-target="#reference-material">Reference Material</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Curious Case of LLM Evaluations</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Text</div>
    <div class="quarto-category">Evaluation</div>
    <div class="quarto-category">Metric Design</div>
    <div class="quarto-category">Opinion</div>
    <div class="quarto-category">Foundation Models</div>
  </div>
  </div>

<div>
  <div class="description">
    Our modeling, scaling and generalization techniques grew faster than our benchmarking abilities - which in turn have resulted in poor evaluation and hyped capabilities. Every ability is amazing and great, if we do not have the tools to figure out what that ability is, or how good the model is at that ability. We might always believe the model will win every race, if all we do, is have the race on paved roads, with yellow trees on every right turns, and green trees on every left turn.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mimansa Jaiswal </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 25, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8c052dbad49.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8c052dbad49.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<section id="introduction" class="level1 page-columns page-full">
<h1>Introduction</h1>
<p>Every <code>popular</code> paper later, we keep coming back to the same questions: how do we know that this is a good evaluation?</p>
<blockquote class="blockquote">
<p><em>Note</em>: While this document will be updated with references in time; I am not claiming all of the ideas here are my own. They are of course inspired and based off many other research outcomes I trust; and I will slowly come around to include them here.</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div class="">
<p>You can always make pull requests to add sources and examples <a href="https://github.com/nlpurr/nlpurr.github.io/blob/master/posts/case-of-llm-evals.qmd">here</a>.</p>
</div></div><p>And unfortunately, the answer is not as simple. I might even go as far to say; it — in most likelihood is not solid. We might want it to be, but evaluation and benchmarking had already been complicated, even for classification models. We, honestly never solved it for small generative models and long form generations; and then suddenly we were faced with an influx of extremely large, multi-purpose language models; aka; foundation models.</p>
<p>And now everyone is left with these carefully curated academic datasets that they report numbers on; that in all likelihood leaked into the training set when the whole accessible internet was scraped for it; and; buggy techniques; because we as ML practitioners were never taught basic statistics.</p>
</section>
<section id="common-problems-with-evaluation-in-machine-learning" class="level1">
<h1>Common Problems with Evaluation in Machine Learning</h1>
<p>Evaluation always come with the basic issues. I am writing this document with the assumption that everyone just assumes these evaluation issues anyway; because they existed in previous machine learning models too.</p>
<section id="leakage" class="level3">
<h3 class="anchored" data-anchor-id="leakage">Leakage</h3>
<p>Data leakage from the test datasets into the training set. This is specifically true for LLMs where the details of the dataset are often missing; and sometimes proprietary.</p>
</section>
<section id="coverage" class="level3">
<h3 class="anchored" data-anchor-id="coverage">Coverage</h3>
<p>Coverage issues of the test samples. The evaluation datasets often do not have a diverse coverage of all various ways that a particular task can be evaluated. These might end up reflecting as accuracy problems, variability problems, efficient sample size problems or robustness problems.</p>
</section>
<section id="spurious-correlations" class="level3">
<h3 class="anchored" data-anchor-id="spurious-correlations">Spurious Correlations</h3>
<p>Spuriously correlated or duplicated test samples. The evaluation sets for many tasks have been shown to have a “shortcut” solution. So, while the assumption might be that the test sample is a good representative evaluation of the task, it often might end up not being the case.</p>
</section>
<section id="partitioning-and-phrasing" class="level3">
<h3 class="anchored" data-anchor-id="partitioning-and-phrasing">Partitioning and phrasing</h3>
<p>Evaluation dataset splits are hard to deal with. Many evaluation datasets come with a different implementation of the same problem. They might also lead to unintentional leakage. For example, in human centered tasks; it is often the case that the evaluation dataset is not isolated for user and rather split just on samples.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Maarten talks about splits in Trivia QA 1-shot">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Maarten talks about splits in Trivia QA 1-shot
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-36886"></div><script>tweet={"url":"https:\/\/twitter.com\/MaartenBosma\/status\/1672349512499867648","author_name":"Maarten Bosma","author_url":"https:\/\/twitter.com\/MaartenBosma","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EGood question! PaLM used a different eval split. We have now added our results on that split as well, and we outperform PaLM by quite a margin. \u003Ca href=\"https:\/\/t.co\/mimZE8TSuB\"\u003Epic.twitter.com\/mimZE8TSuB\u003C\/a\u003E\u003C\/p\u003E&mdash; Maarten Bosma (@MaartenBosma) \u003Ca href=\"https:\/\/twitter.com\/MaartenBosma\/status\/1672349512499867648?ref_src=twsrc%5Etfw\"\u003EJune 23, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-36886").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div>
</section>
<section id="random-seed" class="level3">
<h3 class="anchored" data-anchor-id="random-seed">Random seed</h3>
<p>Any neural network output is usually slightly dependent on the random seed. Any singular inference run based reporting could lead to misinformed results and paint an unclear picture of the situation.</p>
</section>
<section id="precision-vs-recall-tradeoff" class="level3">
<h3 class="anchored" data-anchor-id="precision-vs-recall-tradeoff">Precision vs Recall Tradeoff</h3>
<p>While many people are content with reporting accuracy results, it has been well known that the impact of false positives vs false negatives isn’t the same for every task. If you are using an ML model for IR, and see a false-positive or miss a false-negative; it is probably alright. If you are using the same model for passive health monitoring, a false negative is heavily</p>
</section>
<section id="unexplained-decisions" class="level3">
<h3 class="anchored" data-anchor-id="unexplained-decisions">Unexplained Decisions</h3>
<p>There have been many decisions of removing or keeping data, that have existed across the existence of machine learning. In audio, many numbers are reported by discarding data samples that are under a certain length because they might not constitute as utterances. Knowing and explaining those thresholds is not only important for paper review or discussion but also for replication.</p>
</section>
</section>
<section id="components-of-evaluation-wrt-llms" class="level1 page-columns page-full">
<h1>Components of Evaluation wrt LLMs</h1>
<p>Now that we have that information, let’s talk about the components of LLM evaluation. You can breakdown any LLM evaluation into 6 components:</p>
<section id="evaluation-datasets" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-datasets">Evaluation Datasets</h2>
<p>Evaluation Datasets or Evaluation Sets or Eval Sets: These are the test samples that the model is being evaluated for. There are multiple ways to construct these evaluation datasets and to use them; each of them coming with its own set of problems.</p>
<p>Using the similar set of datasets for evaluation come with a another set of problems:</p>
<ul>
<li><p>Fuzziness in prompts: Now that there are prompts involved in the process; we really need to consider the fuzziness that comes from the prompt itself. While the evaluation datasets were used without any “instruction language” or “prompted addition”, the test samples at least remained consistent. The prompts here may not. I talk more about prompt introduced variability in my other post about prompt based benchmarking. It specifically talks about three components of a prompt for standardizing prompt based evaluation: prompt steer, sample steer, output steer. You can read it <a href="../notes/capstone.html">here</a>.</p>
</li>
<li><p>Untraceability: Going back to the point of data leakage; while it used to always be a problem; now that no one has any idea about any of the data that went into the model, even the best-faith and triple checked evaluation has no guarantees of being a out of distribution evaluation.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e7dac9ea4e5.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Prompt parts: Start, Process and Output Steer and influences that arise from it"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e7dac9ea4e5.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Prompt parts: Start, Process and Output Steer and influences that arise from it</figcaption>
</figure>
</div>
</div></div><section id="what-can-these-evaluation-datasets-look-like-" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="what-can-these-evaluation-datasets-look-like-">What can these evaluation datasets look like?</h3>
<blockquote class="blockquote">
<p>Suggest examples in pull requests and I will merge them here</p>
</blockquote>
<p>Pre-curated evaluation sets from various standardized tests — These are mostly designed to be differentiating for humans and not for models. Additionally, they can have memorization based questions, that can incorrectly be inferred as understanding in context of evaluating LLMs.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Pre-curated eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pre-curated eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-28995f49bbb.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset fromMedical Exams"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-28995f49bbb.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2009.13081v1.pdf">What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from</a><a href="https://arxiv.org/pdf/2009.13081v1.pdf"><strong>Medical Exams</strong></a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Web scraped evaluation sets for ML model testing — These datasets are created by scraping the internet for particular tags and using the corresponding tag as the label for the sample itself or are annotated by various human annotators. The samples from these datasets are most likely to be present in the training set of these foundation models itself and hence, it is often not a good idea to just rely on these for evaluation.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Scraped eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Scraped eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a2d821fedcb.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a2d821fedcb.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/1705.03551.pdf">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Human created and curated evaluation sets for the sole purpose of evaluating a task — These test sets are usually designed as a measure against data leakage. After all, humans can probably create a distinct enough pattern to evaluate against. While that is a pro, these datasets come with flaws of their own; for example they are really small and, are harder to create and update.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Human curated eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Human curated eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d4c26694437.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="HumanEval dataset proposed in Evaluating Large Language Models Trained on Code"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d4c26694437.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">HumanEval dataset proposed in E<a href="https://arxiv.org/abs/2107.03374">valuating Large Language Models Trained on Code</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Fuzzy Versions — These are variants of exisiting datasets or evaluation sets, extensions or additions created with the explicit purpose of introducing and testing for model behavior in presence of such variability. This variability can be intentionally adversarial, can be aimed to introduce out of distribution tokens for robustness testing or just for the purpose of creating paraphrased equivalent samples.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Fuzzed eval sets">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fuzzed eval sets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e8207fbb5dd.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="For example, a set of adversarial prompts and inputs that act as addition/in replacement to the original evaluation samples, as proposed in PromptBench"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e8207fbb5dd.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">For example, a set of adversarial prompts and inputs that act as addition/in replacement to the original evaluation samples, as proposed in <a href="https://arxiv.org/pdf/2306.04528.pdf">PromptBench</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Ad-hoc testing samples by humans — These are performed as conversation based evaluation of these models. While they are most likely to be accurate; they are are biased by the mere idea that the human usually needs to know the solution to the problem to be able the question for evaluation purposes. This often ends up in; what I call as; <code>human imagination collapse</code> or the likelihood that an individual human would often be set on a trajectory to test for and not meaningfully diversify; at least in a single setting.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Ad-hoc testing">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ad-hoc testing
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8e22f3bd0df.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Evaluating models through single turn or multi-turn conversations in OpenAssistant Conversations - Democratizing Large Language Model Alignment"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-8e22f3bd0df.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Evaluating models through single turn or multi-turn conversations in <a href="https://arxiv.org/pdf/2304.07327.pdf">OpenAssistant Conversations - Democratizing Large Language Model Alignment</a></figcaption>
</figure>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0fc2f888363.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="From this tweet by Subbarao Kambhampati (కంభంపాటి సుబ్బారావు)"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0fc2f888363.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">From this <a href="https://twitter.com/NLPurr/status/1672714169001590784">tweet</a> by <a href="https://twitter.com/rao2z"><strong>Subbarao Kambhampati (కంభంపాటి సుబ్బారావు)</strong></a></figcaption>
</figure>
</div>
</div></div></section>
</section>
<section id="model-output" class="level2">
<h2 class="anchored" data-anchor-id="model-output">Model Output</h2>
<p>Now let us come to the second problem — output from generative models.</p>
<ul>
<li>One of the major problems with almost every single solution that we, as a community have proposed: is this idea of evaluating generative models using discriminative outputs.</li>
<li>Model output is heavily dependent on the (a) prompt asked for the output; and (b) the answers asked for. For example, if you ask a model for a label 0 or 1; vs asking a model for labels in words (for example: spam or not spam); you might end up with different outcomes. Another example: asking a model for a direct output and extracting the answer can lead to a different answer than in the multiple-choice scenario.</li>
<li>Regression based model outcome does not necessarily scale and hence can change the standard deviation and mean of the regressed output. For example, if you ask a model to provide a graded rating between 0-10; you cannot with certainty scale that grading to 0-1, especially if you care about significance testing.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Do you want to know why this is a problem with examples?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Do you want to know why this is a problem with examples?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Let’s take the example in <a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks are all you need</a> paper again and ask it to grade 2 responses on 3 scales.</p>
<p>Why? Because the description says that the GPT4 model was asked to grade between 0-10, but the numbers are reported scaled between 0-1.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-38d35b72660.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="The paper saying that the model is asked for grades between 0 to 10, and that is then scaled to report between 0-1."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-38d35b72660.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The paper saying that the model is asked for grades between 0 to 10, and that is then scaled to report between 0-1.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-36a1e237255.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-36a1e237255.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<p>Let’s start with the completely correct response.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0d50e7c6848.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="The GPT4 model says the response is 9/10 when asked for a grade between 0-10."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0d50e7c6848.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 9/10 when asked for a grade between 0-10.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-44ef7ec4988.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="The GPT4 model says the response is .95/10 when asked for a grade between 0-1."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-44ef7ec4988.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is .95/10 when asked for a grade between 0-1.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5b3c7e7a4b4.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5; which is great, right? Because at least the scaling is consistent."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5b3c7e7a4b4.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5; which is great, right? Because at least the scaling is consistent.</figcaption>
</figure>
</div>
<p>But, let’s consider some other solutions, those that are incorrect in some way or another. Let’s do this by changing squaring to 2.2, and sorting the second list in reverse.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-7d63e513030.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="The GPT4 model says the response is 8/10 when asked for a grade between 0-10."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-7d63e513030.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 8/10 when asked for a grade between 0-10.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-88c3f3d29ff.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5. The scaling is constant here too!"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-88c3f3d29ff.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says the response is 4.5/5 when asked for a grade between 0-5. The scaling is constant here too!</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-2a516565700.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="The GPT4 model says that the response is 0.9/1. And now all the scaling is messed up."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-2a516565700.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The GPT4 model says that the response is 0.9/1. And now all the scaling is messed up.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="sample-output-transformation" class="level2">
<h2 class="anchored" data-anchor-id="sample-output-transformation">Sample/Output Transformation</h2>
<p>There are many transformations applied to model outputs or their inputs. These can roughly be divided into 4 categories:</p>
<section id="looped-transformations" class="level3">
<h3 class="anchored" data-anchor-id="looped-transformations">Looped Transformations</h3>
<p>Loop based transformations usually follow the philosophy of —&gt; what if we additionally add the model output + some form of evaluation of the current answer (either from the same model, another model or a human) back into the model; such that it eventually reaches the perfect outcome. Some examples of these would be Self-Critique models.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Example Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ad9844c143d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Reflexion: Language Agents with Verbal Reinforcement Learning develops a modular formulation for Reflexion, utilizing three distinct models: an Actor generates text and actions; an Evaluator model scores the outputs produced by Actor; and a Self-Reflection model, generates verbal reinforcement cues to assist the Actor in self-improvement."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ad9844c143d.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2303.11366.pdf">Reflexion: Language Agents with Verbal Reinforcement Learning</a> develops a modular formulation for Reflexion, utilizing three distinct models: an Actor generates text and actions; an Evaluator model scores the outputs produced by Actor; and a Self-Reflection model, generates verbal reinforcement cues to assist the Actor in self-improvement.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="chained-transformations" class="level3">
<h3 class="anchored" data-anchor-id="chained-transformations">Chained Transformations</h3>
<p>Chain based transformations usually do not have a measured evaluation in between a set of model input → output → model input and so on. These chains are usually pre-defined and have a restricted number of paths to follow.</p>
</section>
<section id="atomic-outputs" class="level3">
<h3 class="anchored" data-anchor-id="atomic-outputs">Atomic Outputs</h3>
<p>This method involves breaking down the output of a model; either manually, through a rule based system or through AI itself; into atomic components that can be evaluated individually to combine to form a larger weighted grade.</p>
</section>
<section id="constrained-output" class="level3">
<h3 class="anchored" data-anchor-id="constrained-output">Constrained Output</h3>
<p>This method involves either using log probability (which isn’t accessible in case of GPT3.5/GPT4 APIs) or other internal constraints to ensure that the model responds with tokens belonging to a certain pre-decided or allowed distribution.</p>
</section>
</section>
<section id="ground-truth" class="level2">
<h2 class="anchored" data-anchor-id="ground-truth">Ground Truth</h2>
<p>This does not need much explanation but there are certain aspects to remember here; especially when you think of ground truths in respect to present scene of evaluation. Firstly, ground truth can be biased, ambiguous, or have a high range of disagreement. In case of human centered tasks, such as likability of a prose, the disagreement is often averaged out rather than considered as an annotation curve, you need to compare the model’s outcome multiple times to achieve a true distribution comparison.</p>
<p>With new evaluation practices, you need to remember that you may or MAY NOT have ground truth in a certain evaluation.</p>
<p>Remember the 3 possible pitfalls with ground truth:</p>
<ul>
<li>Inclusion in the loop or chain based transformations</li>
<li>Inclusion in in-context or few shot learning examples in case of prompt steering</li>
<li>Ground truth might be used to establish correlation between the newly proposed completely automated metric; and not actually used in evaluation.</li>
</ul>
</section>
<section id="evaluation-medium" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-medium">Evaluation Medium</h2>
<p>In my perspective, the evaluation medium can be categorized into three distinct groups.</p>
<section id="direct-evaluation-metrics" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="direct-evaluation-metrics">Direct Evaluation Metrics</h3>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Example Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d9a5f74b47b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Textbooks are all you needevaluation with HumanEval and MBPP"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d9a5f74b47b.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks are all you need</a>evaluation with HumanEval and MBPP</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The first category I call "direct evaluation metrics." These are the traditional metrics that have been widely used in the field for a long time. Metrics like accuracy and F1 score fall into this category. Typically, this approach involves obtaining a single output from the model and comparing it to a reference, either through constraints or by extracting the desired information. The evaluation can be done through ad-hoc conversation-based evaluation, curated specialized test sets, or direct annotation.</p>
<p>For instance, one direct evaluation metric is comparing the model's accuracy directly to the ground truth. When evaluating multiple-choice question answers, the comparison can be based on matching the choice letter, the complete choice, or the distribution over choices. To gain a deeper understanding of how these evaluation approaches can impact the results, check out this article: <a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard">What's going on with the Open LLM Leaderboard and Llama?</a></p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1bc49b90841.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Now how do we evaluate the model from these prompts?"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1bc49b90841.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><strong>Now how do we evaluate the model from these prompts?</strong></figcaption>
</figure>
</div>
</div></div></section>
<section id="indirect-or-decomposed-model-based-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="indirect-or-decomposed-model-based-evaluation">Indirect or Decomposed Model-Based Evaluation</h3>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Example Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-384f6f3ee9b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Rubric based from the same model. TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-384f6f3ee9b.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Rubric based from the same model.<br>
<a href="https://arxiv.org/pdf/2305.07759.pdf">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e6235ebb710.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="Self-critiquing models for assisting human evaluators"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-e6235ebb710.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2206.05802.pdf">Self-critiquing models for assisting human evaluators</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-fd37f2d1713.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment using form-filling methods for evaluation and then calculate a correlation to human preference."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-fd37f2d1713.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2303.16634.pdf">G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment</a> using form-filling methods for evaluation and then calculate a correlation to human preference.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-bc49bad86e9.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Component-wise model driven evaluation scores in LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-bc49bad86e9.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Component-wise model driven evaluation scores in <a href="https://arxiv.org/pdf/2305.13711.pdf">LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Moving on to the second category, we have "indirect or decomposed heuristics." Here, we utilize smaller models, whether they are fine-tuned or raw decompositions, to evaluate the answers generated by the main model. The idea is to leverage these smaller models, which excel at specific tasks that large language models are generally good at, such as adjective identification or polarity identification. The outputs of these smaller models act as weak scores that are combined to provide a final label or evaluation for the generated output. This indirect evaluation approach allows for a more nuanced assessment of the model's performance, particularly in areas like the likability of prose. Although these models introduce some variability, it's important to note that they are often trained for regression tasks and fine-tuned for specific purposes.</p>
<blockquote class="blockquote">
<p>Honestly, the line between this method of evaluation and the next is blurry, especially in terms of impact and possibility of being wrong. Suggestions for better ontology are welcome!</p>
</blockquote>
</section>
<section id="model-driven-evaluation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="model-driven-evaluation">Model-Driven Evaluation</h3>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Papers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Papers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-b297c918d95.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Evaluating responses by comparing to referenced ground truth in Sparks of AGI. Remember this is inclusion of ground truth and possibly the least problematic form of model driven evaluation."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-b297c918d95.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Evaluating responses by comparing to referenced ground truth in <a href="https://arxiv.org/pdf/2303.12712.pdf">Sparks of AGI</a>. Remember this is inclusion of ground truth and possibly the least problematic form of model driven evaluation.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-594e8bc80cb.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Bring Your Own Data! Self-Supervised Evaluation forLarge Language Models evaluation based on invariance of model output based on fuzzed input samples"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-594e8bc80cb.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2306.13651.pdf">Bring Your Own Data! Self-Supervised Evaluation forLarge Language Models</a> evaluation based on invariance of model output based on fuzzed input samples</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-62202ec9cb6.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-29" title="Textbooks are all you needevaluation with GPT4"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-62202ec9cb6.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://arxiv.org/pdf/2306.11644.pdf">Textbooks are all you need</a>evaluation with GPT4</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5f2ae4973f3.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-30" title="Using LLMs for explanations of smaller LLMs"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-5f2ae4973f3.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption"><a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models">Using LLMs for explanations of smaller LLMs</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d2af74360dd.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-31" title="Ask the AI section from Language Models (Mostly) Know What They Know"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-d2af74360dd.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Ask the AI section from <a href="https://arxiv.org/pdf/2207.05221.pdf">Language Models (Mostly) Know What They Know</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The third category I refer to as "model-driven evaluation." In these cases, the model itself provides the final score or evaluation. However, this introduces an additional layer of variability. Even if the model has access to ground truth information, the evaluation metric itself may introduce randomness or variability in the scoring process. For example, a common evaluation question could be: "Is this generated output (O) similar to the ground truth answer (G)?" The answer to this question not only depends on the randomness associated with the model's output but also on the inherent variability of the evaluation metric.</p>
<p>It's important to remember that recent evaluation practices may involve the inclusion or exclusion of ground truth in model-driven evaluation.</p>
<p>This can lead to 2 kinds of model-driven evaluations:</p>
<ul>
<li>[Inclusion of Ground Truth] Asking the model to compare to the ground truth and produce an output in affirmation or contradiction. This can also be seen as providing two statements to the model, and asking it to label them for entailment, paraphrasing or both.</li>
<li>[Exclusion of Ground Truth] Asking the model to directly “judge” the output. In this case, the larger model is often provided outputs from the smaller models and asked to evaluate the answer’s correctness. This can range from a short feedback to a likert scale answer or anywhere in between. &gt; Note, that the larger model evaluating the smaller model paradigm might not necessarily hold for all papers and is significantly more dubious in claims than the former.</li>
</ul>
<p>The general explanation provided for such situations is — but oh, this is how humans do this stuff too. So, we are asking GPT-4 to be more humane and avoiding binary labels that we needed initially.</p>
<p>For example, this is why the authors of <a href="https://arxiv.org/abs/2306.11644">Textbooks are all you need</a> believe this is a correct method of evaluation.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Ronen talks about why using automated evals is a good idea for textbooks">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ronen talks about why using automated evals is a good idea for textbooks
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-68201"></div><script>tweet={"url":"https:\/\/twitter.com\/EldanRonen\/status\/1673107981742010370","author_name":"Ronen Eldan","author_url":"https:\/\/twitter.com\/EldanRonen","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003EThere&#39;s a reason that in coding exams\/interviews, grades are not only based on whether the code passes unit tests, but rather a human reads through the code and determines the level of understanding reflected from it.\u003C\/p\u003E&mdash; Ronen Eldan (@EldanRonen) \u003Ca href=\"https:\/\/twitter.com\/EldanRonen\/status\/1673107981742010370?ref_src=twsrc%5Etfw\"\u003EJune 25, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-68201").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Detailed account of why I believe that using GPT-4 as a judge is incorrect in this case">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Detailed account of why I believe that using GPT-4 as a judge is incorrect in this case
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>But this, in my opinion, is not a correct way to account for non-binary labels. Especially in coding, when there is an existing method to run the function through tests and have assertions against expected answers. My judgement might have been different had this evaluation method been proposed for a subjective task, but coding, is both, an objective and assertable task, and this evaluation method discards all these properties of a coding task in general.</p>
<p>Let’s break this down:</p>
<p>What are the problems that general evaluations in coding have? The biggest problem is (a) writing representative tests and their expected output, and, (b) making sure that these tests are comprehensive and follow a curriculum based evaluation.</p>
<p>That is a bunch of jargon! What does this even mean?</p>
<p>For example, let’s take the example in their paper:</p>

<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Write a python code <span class="cf">for</span> the following:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sort_concat_square_deduplicate (list1, list2, my_threshold):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">This functions takes two lists of integers, sorts each of them in ascending order, concatenates them, squares the entries at even indices, filters out entries smaller than my threshold and then removes duplicates. The resulting list is returned.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>One of the major drawbacks of existing evaluation datasets is that they test the final answer. While the authors claim that it is a better idea to judge if the model implements components correctly.</p>
<p>Going back to decomposed testing, the correct way here would be to ask the model to provide atomic functions and then evaluate those atomic functions using tests and the final function, which calls the atomic functions in desired order.</p>
<p>This solves the problem of not having binary pass/fail labels. This of course requires more effort, but is substantially more accurate.</p>
<p>Doesn’t GPT4 as a judge solve these problems anyway? No, no it does not.</p>
<p>Why not?</p>
<ul>
<li><p>The method does not provide the exact same number every single time. For example, let’s consider the same doc string as mentioned above. And let’s evaluate how good Mosaic 30B-Chat is good at programming. <a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-05b010e3aee.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-33"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-05b010e3aee.webp" class="img-fluid"></a></p>
<p>Now, let us ask GPT4 to judge this. Not only does the answer change, the model suggests to do something that was not even asked for.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a330cd6b0c2.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-34" title="The model scores it 9.5/10"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-a330cd6b0c2.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The model scores it 9.5/10</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0ebbca38746.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-35" title="The model scores it 9/10"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-0ebbca38746.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The model scores it 9/10</figcaption>
</figure>
</div></li>
<li><p>Implicit assumption that the model actually scores completely incorrect answer at 0. This assumption is not tested by the authors. Let’s test it. The decomposition of this problem is into 6 atoms:</p>
<ul>
<li>Sorting of 2 lists ⟶ Sort second list in reverse order</li>
<li>Concatenation ⟶ Change to duplicate concatenation</li>
<li>Even Indices ⟶ Change to every index</li>
<li>Square ⟶ Change to power 2.2</li>
<li>Threshold Filter ⟶ Change it to filter to less than or equal to</li>
<li>Remove Duplicates ⟶ Duplicate the filtered list</li>
</ul>
<p>GPT4 judges it to be 4/10. And, it does not see that the threshold value was changed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-37b899bb621.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-36" title="The model scores the completely incorrect function at 4/10. It ignores an error. In the previous pictures, it creates an error or a suggestion which was not asked for and will render the code incorrect."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-37b899bb621.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">The model scores the completely incorrect function at 4/10. It ignores an error. In the previous pictures, it creates an error or a suggestion which was not asked for and will render the code incorrect.</figcaption>
</figure>
</div></li>
<li><p>Anything weird to the model, that is not in the general expected distribution will be marked low. For example, here is a correct code that is marked lower just because it is unexpected by GPT4. <a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-de21fe993b3.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-37" title="Model invents a requirement that does not exist, writes incorrect code and grades the correct code 8.5/10."><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-de21fe993b3.webp" class="img-fluid" alt="Model invents a requirement that does not exist, writes incorrect code and grades the correct code 8.5/10."></a></p></li>
</ul>
<p>Now a retort might be: But you made those changes, the model did not produce that outcome! The model will always generate outcomes in the distribution!</p>
<p>Fair! But (a) that de-incentivizes training of new models that are have better coverage; and; (b) there is no way someone won’t read that paper and consider — let’s create an LLM based service that proposes to judge interviewees based on LLM scoring.</p>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div class="callout-13-contents callout-collapse collapse show callout-margin-content">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-16699c3485b.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-32"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-16699c3485b.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div></div></section>
</section>
<section id="performance-report" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="performance-report">Performance Report</h2>
<p>In the realm of evaluation, it is vital to consider how we present the performance metrics. These numbers can be influenced by various factors, such as dataset splits and slight variations. It would be ideal to conduct multiple tests for each sample, utilizing different prompts and samples. However, this approach can be resource-intensive and necessitate significant modifications to evaluation frameworks. Therefore, it is crucial to approach the reported numbers with caution and maintain a level of skepticism.</p>
<p>If you have been involved in the machine learning field prior to the rise of large language models like GPT, you might recall the practice of running each test sample multiple times with various randomly seeded models. However, due to the lack of control over seed during inference in GPT models, it is advisable to run tests at least three times. Reporting the mean and standard deviation becomes essential for properly interpreting the significance of the results. While p-values can be messy, it is even more problematic to claim significant improvement based on a few points difference and a single inference result.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Two back to back answers from GPT4, temperature set to 0, about finding a largest number above 90 and below 100 that is divisible by both 7 and 3.</p>
<blockquote class="blockquote">
<p>Click on images to expand them.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ab11bdbfad2.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-ab11bdbfad2.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-c8c4210059e.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-c8c4210059e.webp" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div></div><p>Another aspect to consider is the level of granularity in reporting. Many academic datasets already suffer from various issues, and the problem is further exacerbated by averaging values across these large multi-task datasets without considering the specific evaluation goals of each test sample. Currently, most reports lack granularity even at a high-level task-based assessment, let alone sample-based granularity.</p>
<p>Mosaic 30B (released on 2023-06-22) explores the idea of combining benchmarks into thematic groups.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1dcf9af277d.webp" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Evaluation harness thematic grouping. Read more at https://www.mosaicml.com/blog/mpt-30b"><img src="images/case_of_llm_evals/The_Curious_Case_of_LLM_Evaluations-1dcf9af277d.webp" class="img-fluid figure-img"></a></p>
<figcaption class="figure-caption">Evaluation harness thematic grouping. Read more at <a href="https://www.mosaicml.com/blog/mpt-30b" class="uri">https://www.mosaicml.com/blog/mpt-30b</a></figcaption>
</figure>
</div>
</div></div><p>Lastly, we must address the concept of prompt-fine tuning. Numerous research papers present results on the test set by identifying the best prompt for a given task. While this approach may seem reasonable in theory, it fails to provide a reliable measure of the model's performance when it comes to solving real-world problems encountered by average users. If the intention is to use the prompt as an auxiliary component in a pipeline, then finding the optimal prompt for the task and model is acceptable. However, for direct user-facing end-to-end models, it is crucial to acknowledge that the best prompt for obtaining the correct answer may not be realistic or feasible for all users, especially in the case of general-purpose models.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Armen talks about prompt fuzzing">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Armen talks about prompt fuzzing
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p></p><div id="tweet-27947"></div><script>tweet={"url":"https:\/\/twitter.com\/ArmenAgha\/status\/1669098472493162497","author_name":"Armen Aghajanyan","author_url":"https:\/\/twitter.com\/ArmenAgha","html":"\u003Cblockquote class=\"twitter-tweet\" align=\"center\"\u003E\u003Cp lang=\"en\" dir=\"ltr\"\u003E1. Manually annotate 1k fuzzing prompts\u003Cbr\u003E2. Finetune LLaMa on this 1k a la LIMA\u003Cbr\u003E3. Apply fuzzing to standard benchmarks\u003Cbr\u003E4. Re-evaluate common models?\u003Cbr\u003E\u003Cbr\u003ELikely this will work, not sure if anyone has done this.\u003C\/p\u003E&mdash; Armen Aghajanyan (@ArmenAgha) \u003Ca href=\"https:\/\/twitter.com\/ArmenAgha\/status\/1669098472493162497?ref_src=twsrc%5Etfw\"\u003EJune 14, 2023\u003C\/a\u003E\u003C\/blockquote\u003E\n\u003Cscript async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"\u003E\u003C\/script\u003E\n","width":550,"height":null,"type":"rich","cache_age":"3153600000","provider_name":"Twitter","provider_url":"https:\/\/twitter.com","version":"1.0"};document.getElementById("tweet-27947").innerHTML = tweet["html"];</script><p></p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Side note:</strong> There is a proposal of encrypting and encoding prompts and evaluation. And I completely support that proposal. While in an ideal world we might want opt-in for data, this isn’t the case at the moment, and we can safeguard ourselves from train-test leakage concerns.</p>
<p>I have lost this paper in my never-ending pile. If someone finds this paper, please let me know and I will edit this document.</p>
</blockquote>
</section>
</section>
<section id="tl-dr" class="level1">
<h1>tl;dr</h1>
<p>In the realm of evaluating language models (LLMs), we find ourselves grappling with intricate questions about the reliability of our assessments. The truth is, evaluation and benchmarking have always been challenging, and the advent of large, multi-purpose models has only compounded the complexities. Issues such as data leakage, coverage limitations, spurious correlations, and partitioning quandaries plague our evaluation practices. To make matters worse, precision vs.&nbsp;recall tradeoffs and the lack of ground truth further muddy the waters. This post explores the common problems with evaluation in machine learning and delves into the specific challenges posed by LLMs. We categorize evaluation mediums into direct metrics, auxiliary models-based evaluation, and model-driven evaluation, shedding light on the nuances of each approach. With a cautious eye, we navigate the intricacies of reporting performance numbers and emphasize the importance of granularity. Prompt-fine tuning is also scrutinized, reminding us to consider the realism of user interactions. As we venture deeper into the evaluation landscape, it becomes clear that a comprehensive understanding of these complexities is essential for meaningful assessments of LLMs.</p>
<blockquote class="blockquote">
<p>Our modeling, scaling and generalization techniques grew faster than our benchmarking abilities — which in turn have resulted in poor evaluation and hyped capabilities. Every ability is amazing and great, if we do not have the tools to figure out what that ability is, or how good the model is at that ability. We’d always believe the model will win every race, if all we do, is have the race on paved roads, with yellow trees on every right turns, and green trees on every left turn.</p>
</blockquote>
</section>
<section id="reference-material" class="level1">
<h1>Reference Material</h1>
<blockquote class="blockquote">
<p>In progress, and definitely not a comprehensive list of everything</p>
</blockquote>
<div class="youtube">
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/oTn_bP-nYv4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>
</div>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="mimansajaiswal/mimansajaiswal.github.io" data-repo-id="R_kgDOJo-RfQ" data-category="Announcements" data-category-id="DIC_kwDOJo-Rfc4CW3WB" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"loop":true,"closeEffect":"zoom","selector":".lightbox","openEffect":"zoom","descPosition":"bottom"});</script>



<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
</body></html>